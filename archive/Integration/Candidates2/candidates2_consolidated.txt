CONSOLIDATED FILES FROM /home/seanpatten/projects/AIOS/Integration/Candidates2
================================================================================


================================================================================
FILE: chatgpt2.py
================================================================================

#!/usr/bin/env python3
"""
aiose â€” All-in-One Scheduler & Executor  (<500 lines)

Highlights (taken & improved):
- SQLite WAL + tuned pragmas (claude/chatgpt/gemini*)
- One table for everything; JSON deps/properties; metrics (chatgpt/claude)
- Atomic pop with json_each dep check + RETURNING fallback (chatgpt)
- Local subprocess runner + optional in-process Python function jobs ("py:module.func")
  (claudeâ€™s registry idea, simplified)
- systemd-run transient units with resource controls, OnCalendar or on-active delay, Persistent=true
  (aios_systemd/deepseek/grok/gemini*)
- Batch worker w/ reclaim of stalled, exponential backoff, stdout/stderr capture (chatgpt/glm)
- Efficient reconciliation of MANY units at once (geminiDeep)
- CLI: add | worker | start | stop | status | list | stats | cleanup | reconcile | install
"""
from __future__ import annotations
import argparse, json, os, shlex, signal, sqlite3, subprocess, sys, time, threading, importlib
from pathlib import Path
from typing import Optional, Dict, Any, List

# ---------- constants ----------
DB_PATH = Path.home() / ".aiose.db"
UNIT_PREFIX = "aiose-"
USE_USER = (os.geteuid() != 0)
SYSTEMCTL = ["systemctl", "--user"] if USE_USER else ["systemctl"]
SYSDRUN   = ["systemd-run", "--user", "--collect", "--quiet"] if USE_USER else ["systemd-run","--collect","--quiet"]
DEFAULT_TIMEOUT = 300
DEFAULT_RETRIES = 3

PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

# ---------- helpers ----------
def now_ms() -> int: return int(time.time()*1000)
def unit_name(ident: str|int) -> str:
    s = "".join(c if str(c).isalnum() or c in "._-:" else "_" for c in str(ident))
    return f"{UNIT_PREFIX}{s}.service"
def run(cmd: List[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

# ---------- storage ----------
class Store:
    """SQLite store + queue operations (thread-safe)."""
    def __init__(self, path=DB_PATH):
        path.parent.mkdir(parents=True, exist_ok=True)
        self.c = sqlite3.connect(path, isolation_level=None, check_same_thread=False)
        self.c.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        for p in PRAGMAS: self.c.execute(p)
        self.c.executescript("""
        CREATE TABLE IF NOT EXISTS tasks(
          id INTEGER PRIMARY KEY,
          name TEXT UNIQUE,                  -- optional human handle
          cmd  TEXT NOT NULL,                -- 'shell string' OR 'py:module.func'
          mode TEXT DEFAULT 'local',         -- 'local' | 'systemd' | 'py'
          args TEXT,                         -- JSON array of args (local/systemd join into shell)
          env  TEXT,                         -- JSON map
          cwd  TEXT,                         -- working dir
          p INT DEFAULT 0,                   -- priority
          s TEXT DEFAULT 'q',                -- q=queued r=running d=done f=failed sched=startable/start
          at INT DEFAULT 0,                  -- scheduled_at (ms) for local/py
          schedule TEXT,                     -- systemd OnCalendar (cron-like)
          w  TEXT,                           -- worker_id (local/py)
          r  INT DEFAULT 0,                  -- retry_count
          maxr INT DEFAULT 3,                -- max_retries
          timeout INT DEFAULT 300,           -- seconds (local/py)
          e TEXT,                            -- error message
          res TEXT,                          -- result (stdout/stderr json)
          ct INT DEFAULT (strftime('%s','now')*1000),
          st INT, et INT,                    -- start/end times
          dep TEXT,                          -- JSON array of dependency IDs
          -- Systemd props
          unit TEXT, rtprio INT, nice INT, slice TEXT,
          cpu_weight INT, mem_max_mb INT
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(s,p DESC,at,id) WHERE s IN ('q','r');
        CREATE TABLE IF NOT EXISTS metrics(task_id INTEGER PRIMARY KEY, qt REAL, et REAL,
          FOREIGN KEY(task_id) REFERENCES tasks(id) ON DELETE CASCADE);
        """)

    # CRUD
    def add(self, **kw) -> int:
        with self.lock:
            cols=",".join(kw.keys()); qs=",".join("?" for _ in kw)
            return self.c.execute(f"INSERT INTO tasks({cols}) VALUES({qs})", tuple(kw.values())).lastrowid
    def get_by_id(self, tid:int) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE id=?", (tid,)).fetchone()
    def get_by_name(self, name:str) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE name=?", (name,)).fetchone()
    def list(self) -> List[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks ORDER BY ct DESC").fetchall()
    def update(self, tid:int, **kw):
        if not kw: return
        with self.lock:
            sets=",".join(f"{k}=?" for k in kw)
            self.c.execute(f"UPDATE tasks SET {sets} WHERE id=?", (*kw.values(), tid))

    # Queue ops
    def pop(self, worker_id:str) -> Optional[Dict[str,Any]]:
        """Pop next eligible local/py task with dep check."""
        with self.lock:
            row = self.c.execute("""
              SELECT id, cmd, mode, args, env, cwd, timeout FROM tasks
              WHERE s='q' AND at<=? AND mode IN ('local','py')
                AND (dep IS NULL OR NOT EXISTS(
                  SELECT 1 FROM json_each(tasks.dep) AS d
                  JOIN tasks t2 ON t2.id=d.value WHERE t2.s!='d'))
              ORDER BY p DESC, at, id LIMIT 1
            """, (now_ms(),)).fetchone()
            if not row: return None
            try:
                claimed = self.c.execute(
                    "UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q' RETURNING id, cmd, mode, args, env, cwd, timeout",
                    (worker_id, now_ms(), row["id"])
                ).fetchone()
                if claimed: return dict(claimed)
            except sqlite3.OperationalError:
                self.c.execute("BEGIN IMMEDIATE")
                n = self.c.execute("UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q'",
                                   (worker_id, now_ms(), row["id"])).rowcount
                self.c.execute("COMMIT")
                if n:
                    return dict(row)
        return None

    def done(self, tid:int, ok:bool, result:Any=None, error:str=None, worker_id:str=""):
        with self.lock:
            t = self.c.execute("SELECT ct,st,r,maxr FROM tasks WHERE id=? AND (w=? OR w IS NULL)", (tid, worker_id)).fetchone()
            if not t: return
            tnow = now_ms()
            if ok:
                self.c.execute("UPDATE tasks SET s='d', et=?, res=?, w=NULL WHERE id=?",
                               (tnow, json.dumps(result)[:2000] if result else None, tid))
                if t["st"]:
                    qt = (t["st"]-t["ct"])/1000.0; et=(tnow - t["st"])/1000.0
                    self.c.execute("INSERT OR REPLACE INTO metrics(task_id,qt,et) VALUES(?,?,?)", (tid,qt,et))
            else:
                rc = t["r"] or 0; mr = t["maxr"] or DEFAULT_RETRIES
                if rc < mr:
                    delay = 1000*(2**rc)
                    self.c.execute("UPDATE tasks SET s='q', at=?, r=r+1, e=?, w=NULL WHERE id=?",
                                   (tnow+delay, (error or "")[:500], tid))
                else:
                    self.c.execute("UPDATE tasks SET s='f', et=?, e=?, w=NULL WHERE id=?", (tnow, (error or "")[:500], tid))

    def reclaim(self, timeout_ms:int=300000) -> int:
        with self.lock:
            cutoff = now_ms() - timeout_ms
            return self.c.execute("UPDATE tasks SET s='q', w=NULL, r=r+1 WHERE s='r' AND st<?", (cutoff,)).rowcount

    def running_units(self) -> Dict[str,int]:
        with self.lock:
            rows = self.c.execute("SELECT id,unit FROM tasks WHERE s IN ('start','sched','r') AND unit IS NOT NULL").fetchall()
            return {row["unit"]: row["id"] for row in rows}

    def stats(self) -> Dict[str,Any]:
        with self.lock:
            counts = {r["s"]: r["c"] for r in self.c.execute("SELECT s,COUNT(*) c FROM tasks GROUP BY s")}
            perf = self.c.execute("SELECT AVG(qt) avg_qt, AVG(et) avg_et, MAX(qt) max_qt, MAX(et) max_et FROM metrics").fetchone()
            return {"tasks": counts, "perf": dict(perf) if perf else {}}

    def cleanup(self, days:int=7) -> int:
        cutoff = now_ms() - days*86400000
        with self.lock:
            n = self.c.execute("DELETE FROM tasks WHERE s IN ('d','f') AND et<?", (cutoff,)).rowcount
            pc = self.c.execute("PRAGMA page_count").fetchone()[0]
            fr = self.c.execute("PRAGMA freelist_count").fetchone()[0]
            if fr > pc*0.3: self.c.execute("VACUUM")
            return n

# ---------- systemd orchestration ----------
def systemd_start(db:Store, task:sqlite3.Row) -> tuple[bool,str,str]:
    """Start via systemd-run; support OnCalendar & on-active delay; set Persistent=true."""
    unit = unit_name(task["name"] or task["id"])
    props = ["--property=StandardOutput=journal",
             "--property=StandardError=journal",
             "--property=KillMode=control-group",
             "--property=TimeoutSec={}".format(task["timeout"] or DEFAULT_TIMEOUT),
             "--property=Persistent=true"]
    if task["cwd"]:  props += [f"--property=WorkingDirectory={task['cwd']}"]
    if task["nice"] is not None: props += [f"--property=Nice={int(task['nice'])}"]
    if task["rtprio"] is not None:
        props += ["--property=CPUSchedulingPolicy=rr",
                  f"--property=CPUSchedulingPriority={int(task['rtprio'])}"]
    if task["slice"]: props += [f"--slice={task['slice']}"]
    if task["cpu_weight"]: props += [f"--property=CPUWeight={int(task['cpu_weight'])}"]
    if task["mem_max_mb"]: props += [f"--property=MemoryMax={int(task['mem_max_mb'])}M"]

    env=[]
    if task["env"]:
        for k,v in json.loads(task["env"]).items():
            env += ["--setenv", f"{k}={v}"]

    when=[]
    if task["schedule"]:
        when += ["--on-calendar", task["schedule"]]
    else:
        # if future 'at', translate to on-active delay
        delay_ms = max(0, (task["at"] or now_ms()) - now_ms())
        if delay_ms>0: when += ["--on-active", f"{int((delay_ms+999)/1000)}s"]

    # Build shell command: join cmd + args (if any)
    args = json.loads(task["args"] or "[]")
    shell = task["cmd"] if not args else task["cmd"] + " " + " ".join(shlex.quote(a) for a in args)
    cp = run([*SYSDRUN, "--unit", unit, *props, *env, *when, "--", "/bin/sh", "-lc", shell])
    db.update(task["id"], unit=unit, s=("sched" if when else "start"))
    msg = cp.stderr.strip() or cp.stdout.strip()
    return (cp.returncode==0, unit, msg)

def systemd_stop(db:Store, name_or_id:str):
    unit = unit_name(name_or_id)
    run(SYSTEMCTL + ["stop", unit])
    run(SYSTEMCTL + ["stop", unit.replace(".service",".timer")])
    # status reconciled later

def systemd_status(name_or_id:str) -> Dict[str,str]:
    unit = unit_name(name_or_id)
    cp = run(SYSTEMCTL + ["show", unit, "--property=ActiveState", "--property=Result", "--property=MainPID"])
    if cp.returncode!=0: return {"unit": unit, "active":"unknown"}
    m = dict(line.split("=",1) for line in cp.stdout.splitlines() if "=" in line)
    return {"unit": unit, "active": m.get("ActiveState"), "result": m.get("Result"), "pid": m.get("MainPID")}

def reconcile_systemd(db:Store) -> int:
    """Batch-check many units efficiently; mark tasks done/failed."""
    umap = db.running_units()
    if not umap: return 0
    cp = run([*SYSTEMCTL, "show", *umap.keys(), "--property=Id,ActiveState,Result"])
    if cp.returncode!=0: return 0
    count=0
    for block in cp.stdout.strip().split("\n\n"):
        props = dict(line.split("=",1) for line in block.splitlines() if "=" in line)
        unit = props.get("Id"); tid = umap.get(unit); 
        if not tid: continue
        active = props.get("ActiveState"); result = props.get("Result")
        if active in ("inactive","failed"):
            db.done(tid, ok=(result=="success"))
            count += 1
        elif active=="active":
            # ensure state marked running
            db.update(tid, s='r')
    return count

# ---------- local/py worker ----------
class Worker:
    def __init__(self, store:Store, wid:Optional[str]=None):
        self.db=store
        self.wid=wid or f"w{os.getpid()}"
        self.run_flag=True
        signal.signal(signal.SIGTERM, self._stop)
        signal.signal(signal.SIGINT,  self._stop)
    def _stop(self,*_): self.run_flag=False

    def _exec_py(self, cmd:str, args:List[str], timeout:int) -> tuple[bool,Dict[str,str]|None,str|None]:
        """Execute 'py:module.func' in-process (simple & fast)."""
        try:
            assert cmd.startswith("py:"), "not py job"
            mod_fn = cmd[3:]
            mod, fn = mod_fn.rsplit(".",1)
            module = importlib.import_module(mod)
            func = getattr(module, fn)
            # crude timeout via alarm (POSIX); fallback to no-timeout if not available
            ok=True; out=None; err=None
            if hasattr(signal, "SIGALRM"):
                def _raise(*_): raise TimeoutError("PY_TIMEOUT")
                signal.signal(signal.SIGALRM, _raise); signal.alarm(max(1, int(timeout)))
                result = func(*args) if args else func()
                signal.alarm(0)
            else:
                result = func(*args) if args else func()
            out = {"stdout": str(result)[:1000], "stderr": ""}
            return True, out, None
        except TimeoutError:
            return False, None, "TIMEOUT"
        except Exception as e:
            return False, None, repr(e)

    def loop(self, batch:int=1, idle_ms:int=50):
        print(f"Worker {self.wid} running (batch={batch})")
        tick=0
        while self.run_flag:
            tick += 1
            if tick%100==0:
                r=self.db.reclaim()
                if r: print(f"reclaimed {r} stalled")
                # also sync with systemd
                try: 
                    n=reconcile_systemd(self.db)
                    if n: print(f"reconciled {n} systemd tasks")
                except Exception: pass
            got=[]
            for _ in range(batch):
                item=self.db.pop(self.wid)
                if item: got.append(item)
            if not got:
                time.sleep(idle_ms/1000.0); continue
            for t in got:
                if not self.run_flag: break
                mode=t["mode"]; args=json.loads(t["args"] or "[]"); env=json.loads(t["env"] or "{}")
                try:
                    if mode=="py" and t["cmd"].startswith("py:"):
                        ok,res,err=self._exec_py(t["cmd"], args, int(t["timeout"] or DEFAULT_TIMEOUT))
                        self.db.done(t["id"], ok, res, err, self.wid); continue
                    # local subprocess
                    shell = t["cmd"] if not args else t["cmd"] + " " + " ".join(shlex.quote(a) for a in args)
                    proc=subprocess.run(shell, shell=True, capture_output=True, text=True,
                                        timeout=int(t["timeout"] or DEFAULT_TIMEOUT),
                                        cwd=(t["cwd"] or None),
                                        env={**os.environ, **env})
                    self.db.done(t["id"], proc.returncode==0,
                                 {"stdout": proc.stdout[:1000], "stderr": proc.stderr[:1000]},
                                 proc.stderr if proc.returncode!=0 else None, self.wid)
                except subprocess.TimeoutExpired:
                    self.db.done(t["id"], False, error="TIMEOUT", worker_id=self.wid)
                except Exception as e:
                    self.db.done(t["id"], False, error=str(e), worker_id=self.wid)

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(description="aiose â€” All-in-One Scheduler & Executor")
    sub = ap.add_subparsers(dest="cmd", required=True)

    # add
    pa = sub.add_parser("add", help="add task")
    pa.add_argument("cmd", help="shell string OR py:module.func")
    pa.add_argument("args", nargs="*", help="args (joined for shell; passed to py func)")
    pa.add_argument("--name")
    pa.add_argument("--mode", choices=["local","systemd","py"], default="local")
    pa.add_argument("--priority", type=int, default=0)
    pa.add_argument("--delay-ms", type=int, default=0)
    pa.add_argument("--deps", help="JSON list of task IDs")
    pa.add_argument("--env", action="append", default=[], help="KEY=VAL")
    pa.add_argument("--cwd")
    pa.add_argument("--on-calendar", help="systemd OnCalendar expression")
    pa.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    pa.add_argument("--retries", type=int, default=DEFAULT_RETRIES)
    # systemd props
    pa.add_argument("--rtprio", type=int); pa.add_argument("--nice", type=int)
    pa.add_argument("--slice"); pa.add_argument("--cpu-weight", type=int); pa.add_argument("--mem-max-mb", type=int)
    pa.add_argument("--start", action="store_true", help="start immediately (systemd)")

    # worker
    pw = sub.add_parser("worker", help="run local/py worker")
    pw.add_argument("--batch", type=int, default=1); pw.add_argument("--idle-ms", type=int, default=50)

    # systemd management
    ps = sub.add_parser("start", help="start a systemd task by id/name")
    ps.add_argument("name_or_id")
    pk = sub.add_parser("stop", help="stop systemd task by id/name"); pk.add_argument("name_or_id")
    pz = sub.add_parser("status", help="status for id/name");        pz.add_argument("name_or_id")

    # ops
    sub.add_parser("list", help="list tasks")
    sub.add_parser("stats", help="show stats")
    pc = sub.add_parser("cleanup", help="delete old done/failed"); pc.add_argument("--days", type=int, default=7)
    sub.add_parser("reconcile", help="reconcile systemd units")
    sub.add_parser("install", help="print a systemd user service for aiose worker")

    args = ap.parse_args()
    db = Store()

    if args.cmd=="add":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        at  = now_ms() + max(0, args.delay_ms)
        dep = json.dumps(json.loads(args.deps)) if args.deps else None
        tid = db.add(
            name=args.name, cmd=args.cmd, mode=args.mode, args=json.dumps(args.args) if args.args else None,
            env=(json.dumps(env) if env else None), cwd=args.cwd, p=args.priority, s="q",
            at=at, schedule=args.on_calendar, dep=dep, timeout=args.timeout, maxr=args.retries,
            rtprio=args.rtprio, nice=args.nice, slice=args.slice, cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb
        )
        if args.mode=="systemd" and (args.start or args.on_calendar or args.delay_ms>0):
            row = db.get_by_id(tid)
            ok, unit, msg = systemd_start(db, row)
            print(unit if ok else f"ERROR: {msg}")
        else:
            print(f"queued task {tid}")

    elif args.cmd=="worker":
        Worker(db).loop(batch=args.batch, idle_ms=args.idle_ms)

    elif args.cmd=="start":
        key = args.name_or_id
        row = db.get_by_id(int(key)) if key.isdigit() else db.get_by_name(key)
        if not row: sys.exit("unknown task")
        if row["mode"]!="systemd": sys.exit("task is not systemd mode")
        ok, unit, msg = systemd_start(db, row)
        print(unit if ok else f"ERROR: {msg}")

    elif args.cmd=="stop":
        systemd_stop(db, args.name_or_id); print("stopped")

    elif args.cmd=="status":
        print(json.dumps(systemd_status(args.name_or_id), indent=2))

    elif args.cmd=="list":
        try: reconcile_systemd(db)
        except Exception: pass
        for r in db.list():
            ident=r["name"] or r["id"]; sched=r["schedule"] or "-"
            unit=r["unit"] or "-"; mode=r["mode"]; s=r["s"]
            print(f"{ident:20} id={r['id']:4} mode={mode:7} s={s:5} p={r['p']:2} at={r['at']} unit={unit} sched={sched}")

    elif args.cmd=="stats":
        print(json.dumps(db.stats(), indent=2))

    elif args.cmd=="cleanup":
        print(f"deleted {db.cleanup(args.days)}")

    elif args.cmd=="reconcile":
        n=reconcile_systemd(db); print(f"ok (updated {n})")

    elif args.cmd=="install":
        exe = sys.executable; here=os.path.abspath(__file__)
        print(f"""[Unit]
Description=aiose worker
After=default.target

[Service]
Type=simple
ExecStart={exe} {here} worker
Restart=always
RestartSec=3
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=default.target
""")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: claude2.py
================================================================================

#!/usr/bin/env python3
"""
Pure Python Job Orchestrator - Zero subprocess calls
Combines the best patterns from all implementations with multiprocessing execution
"""
import sqlite3, json, time, sys, os, signal, threading, traceback, pickle
from multiprocessing import Process, Queue, Manager, cpu_count
from typing import Optional, Dict, Any, List, Callable
from pathlib import Path
import importlib.util

# Configuration
DB_PATH = Path.home() / ".pyorch.db"

# Optimized SQLite pragmas (best from all implementations)
PRAGMAS = """
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA cache_size=-8000;
PRAGMA temp_store=MEMORY;
PRAGMA mmap_size=268435456;
PRAGMA busy_timeout=5000;
PRAGMA wal_autocheckpoint=1000;
"""

class JobRegistry:
    """Registry for Python callable jobs"""
    _jobs = {}
    
    @classmethod
    def register(cls, name: str, func: Callable, module: str = None):
        """Register a Python function as a job"""
        cls._jobs[name] = {'func': func, 'module': module}
    
    @classmethod
    def get(cls, name: str) -> Optional[Callable]:
        """Get a registered function"""
        job = cls._jobs.get(name)
        return job['func'] if job else None
    
    @classmethod
    def load_module(cls, path: str):
        """Load jobs from a Python module"""
        spec = importlib.util.spec_from_file_location("jobs", path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        for name in dir(module):
            obj = getattr(module, name)
            if callable(obj) and not name.startswith('_'):
                cls.register(name, obj, path)
        return list(cls._jobs.keys())

class Orchestrator:
    """High-performance job orchestrator with multiprocessing"""
    
    def __init__(self, db_path: Path = DB_PATH, max_workers: int = None):
        self.db_path = db_path
        self.max_workers = max_workers or cpu_count()
        self.processes = {}
        self.manager = Manager()
        self.result_queue = self.manager.Queue()
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Initialize database with best schema patterns"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            conn.executescript(PRAGMAS + """
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE,
                    func TEXT NOT NULL,
                    args TEXT,
                    kwargs TEXT,
                    
                    -- Scheduling
                    priority INT DEFAULT 0,
                    scheduled_at INT DEFAULT 0,
                    schedule TEXT,  -- cron-like pattern
                    
                    -- State
                    status TEXT DEFAULT 'pending' CHECK(status IN 
                        ('pending','running','completed','failed','cancelled')),
                    pid INT,
                    worker TEXT,
                    
                    -- Execution
                    retry_count INT DEFAULT 0,
                    max_retries INT DEFAULT 3,
                    timeout_sec INT DEFAULT 300,
                    
                    -- Results
                    error TEXT,
                    result TEXT,
                    traceback TEXT,
                    
                    -- Dependencies
                    depends_on TEXT,  -- JSON array of job IDs
                    
                    -- Resource limits
                    nice INT,
                    cpu_affinity TEXT,  -- JSON array of CPU indices
                    memory_limit_mb INT,
                    
                    -- Timestamps
                    created_at INT DEFAULT (strftime('%s','now')*1000),
                    started_at INT,
                    completed_at INT
                );
                
                -- Optimized composite index
                CREATE INDEX IF NOT EXISTS idx_queue ON jobs(
                    status, priority DESC, scheduled_at, id
                ) WHERE status IN ('pending','running');
                
                CREATE INDEX IF NOT EXISTS idx_deps ON jobs(depends_on) 
                    WHERE depends_on IS NOT NULL;
                
                -- Metrics table
                CREATE TABLE IF NOT EXISTS metrics (
                    job_id INTEGER PRIMARY KEY,
                    queue_time REAL,
                    exec_time REAL,
                    cpu_time REAL,
                    memory_peak_mb REAL,
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
                
                -- Artifacts table for job outputs
                CREATE TABLE IF NOT EXISTS artifacts (
                    id INTEGER PRIMARY KEY,
                    job_id INTEGER,
                    key TEXT,
                    value BLOB,
                    created_at INT DEFAULT (strftime('%s','now')*1000),
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
            """)
    
    def add(self, name: str, func: str, args: tuple = None, kwargs: dict = None,
            priority: int = 0, scheduled_at: int = None, depends_on: List[int] = None,
            max_retries: int = 3, timeout_sec: int = 300, schedule: str = None,
            nice: int = None, cpu_affinity: List[int] = None, 
            memory_limit_mb: int = None) -> int:
        """Add a job with comprehensive options"""
        now = int(time.time() * 1000)
        scheduled_at = scheduled_at or now
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            cursor = conn.execute("""
                INSERT OR REPLACE INTO jobs 
                (name, func, args, kwargs, priority, scheduled_at, schedule,
                 depends_on, max_retries, timeout_sec, nice, cpu_affinity, memory_limit_mb)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                name, func,
                json.dumps(args) if args else None,
                json.dumps(kwargs) if kwargs else None,
                priority, scheduled_at, schedule,
                json.dumps(depends_on) if depends_on else None,
                max_retries, timeout_sec, nice,
                json.dumps(cpu_affinity) if cpu_affinity else None,
                memory_limit_mb
            ))
            return cursor.lastrowid
    
    def get_next(self) -> Optional[Dict[str, Any]]:
        """Atomically get next eligible job with dependency checking"""
        now = int(time.time() * 1000)
        
        with self.lock:
            conn = sqlite3.connect(self.db_path, isolation_level=None)
            conn.row_factory = sqlite3.Row
            
            try:
                # Atomic claim with RETURNING (most efficient)
                result = conn.execute("""
                    UPDATE jobs SET status='running', started_at=?, pid=?
                    WHERE id = (
                        SELECT id FROM jobs
                        WHERE status='pending' AND scheduled_at <= ?
                        AND (depends_on IS NULL OR NOT EXISTS (
                            SELECT 1 FROM json_each(jobs.depends_on) AS d
                            JOIN jobs AS dj ON dj.id = d.value
                            WHERE dj.status != 'completed'
                        ))
                        ORDER BY priority DESC, scheduled_at, id
                        LIMIT 1
                    )
                    RETURNING *
                """, (now, os.getpid(), now)).fetchone()
                
                if result:
                    return dict(result)
                    
            except sqlite3.OperationalError:
                # Fallback for older SQLite
                row = conn.execute("""
                    SELECT id FROM jobs
                    WHERE status='pending' AND scheduled_at <= ?
                    AND (depends_on IS NULL OR NOT EXISTS (
                        SELECT 1 FROM json_each(jobs.depends_on) AS d
                        JOIN jobs AS dj ON dj.id = d.value
                        WHERE dj.status != 'completed'
                    ))
                    ORDER BY priority DESC, scheduled_at, id
                    LIMIT 1
                """, (now,)).fetchone()
                
                if row:
                    conn.execute(
                        "UPDATE jobs SET status='running', started_at=?, pid=? WHERE id=? AND status='pending'",
                        (now, os.getpid(), row['id'])
                    )
                    return dict(conn.execute("SELECT * FROM jobs WHERE id=?", (row['id'],)).fetchone())
            finally:
                conn.close()
        
        return None
    
    def execute(self, job: Dict) -> bool:
        """Execute job in separate process with resource limits"""
        job_id = job['id']
        
        # Parse job data
        args = json.loads(job['args']) if job['args'] else ()
        kwargs = json.loads(job['kwargs']) if job['kwargs'] else {}
        
        # Create worker process
        p = Process(
            target=self._worker_process,
            args=(job_id, job['func'], args, kwargs, 
                  self.result_queue, job.get('timeout_sec', 300),
                  job.get('nice'), job.get('cpu_affinity'))
        )
        
        # Start process
        p.start()
        self.processes[job_id] = p
        
        # Update job with process ID
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute("UPDATE jobs SET pid=? WHERE id=?", (p.pid, job_id))
        
        return True
    
    def _worker_process(self, job_id: int, func_name: str, args: tuple,
                       kwargs: dict, result_queue: Queue, timeout: int,
                       nice: Optional[int], cpu_affinity: Optional[str]):
        """Worker process that executes a job"""
        try:
            # Set process nice value
            if nice is not None:
                os.nice(nice)
            
            # Set CPU affinity if specified
            if cpu_affinity:
                try:
                    import psutil
                    p = psutil.Process()
                    p.cpu_affinity(json.loads(cpu_affinity))
                except ImportError:
                    pass  # psutil not available
            
            # Setup timeout
            def timeout_handler(signum, frame):
                raise TimeoutError(f"Job exceeded {timeout}s timeout")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout)
            
            # Get and execute function
            func = JobRegistry.get(func_name)
            if not func:
                # Try to import dynamically
                if '.' in func_name:
                    module_name, func_name = func_name.rsplit('.', 1)
                    module = importlib.import_module(module_name)
                    func = getattr(module, func_name)
                else:
                    raise ValueError(f"Function {func_name} not found")
            
            # Execute
            result = func(*args, **kwargs) if args or kwargs else func()
            
            signal.alarm(0)  # Cancel timeout
            result_queue.put((job_id, True, result, None))
            
        except Exception as e:
            signal.alarm(0)
            result_queue.put((job_id, False, None, {
                'error': str(e),
                'traceback': traceback.format_exc()
            }))
    
    def complete(self, job_id: int, success: bool, result: Any = None,
                error_info: Dict = None):
        """Complete job with retry logic and metrics"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            if success:
                # Success - mark completed and record metrics
                conn.execute("""
                    UPDATE jobs 
                    SET status='completed', completed_at=?, result=?, pid=NULL
                    WHERE id=?
                """, (now, json.dumps(result) if result else None, job_id))
                
                # Calculate and store metrics
                job = conn.execute(
                    "SELECT created_at, started_at FROM jobs WHERE id=?", 
                    (job_id,)
                ).fetchone()
                
                if job and job['started_at']:
                    queue_time = (job['started_at'] - job['created_at']) / 1000.0
                    exec_time = (now - job['started_at']) / 1000.0
                    
                    conn.execute("""
                        INSERT OR REPLACE INTO metrics 
                        (job_id, queue_time, exec_time)
                        VALUES (?, ?, ?)
                    """, (job_id, queue_time, exec_time))
            else:
                # Failure - check retry logic
                job = conn.execute(
                    "SELECT retry_count, max_retries FROM jobs WHERE id=?",
                    (job_id,)
                ).fetchone()
                
                if job and job['retry_count'] < job['max_retries']:
                    # Exponential backoff: 1s, 2s, 4s, 8s...
                    delay = 1000 * (2 ** job['retry_count'])
                    conn.execute("""
                        UPDATE jobs 
                        SET status='pending', scheduled_at=?, retry_count=retry_count+1,
                            error=?, traceback=?, pid=NULL
                        WHERE id=?
                    """, (now + delay, error_info.get('error'),
                         error_info.get('traceback'), job_id))
                else:
                    # Final failure
                    conn.execute("""
                        UPDATE jobs 
                        SET status='failed', completed_at=?, error=?, traceback=?, pid=NULL
                        WHERE id=?
                    """, (now, error_info.get('error'),
                         error_info.get('traceback'), job_id))
        
        # Clean up process reference
        if job_id in self.processes:
            del self.processes[job_id]
    
    def process_results(self):
        """Process completed job results from queue"""
        processed = 0
        while not self.result_queue.empty():
            try:
                job_id, success, result, error = self.result_queue.get_nowait()
                self.complete(job_id, success, result, error)
                processed += 1
            except:
                break
        return processed
    
    def reclaim_stalled(self, timeout_ms: int = 300000) -> int:
        """Reclaim stalled jobs and terminate stuck processes"""
        cutoff = int(time.time() * 1000) - timeout_ms
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            stalled = conn.execute("""
                SELECT id, pid FROM jobs 
                WHERE status='running' AND started_at < ?
            """, (cutoff,)).fetchall()
            
            for job in stalled:
                # Terminate process if still running
                if job['id'] in self.processes:
                    self.processes[job['id']].terminate()
                    del self.processes[job['id']]
                
                # Reset job for retry
                conn.execute("""
                    UPDATE jobs 
                    SET status='pending', pid=NULL, retry_count=retry_count+1
                    WHERE id=?
                """, (job['id'],))
        
        return len(stalled)
    
    def stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            # Job counts by status
            counts = {row['status']: row['c'] for row in conn.execute(
                "SELECT status, COUNT(*) c FROM jobs GROUP BY status"
            )}
            
            # Performance metrics
            perf = conn.execute("""
                SELECT AVG(queue_time) avg_qt, AVG(exec_time) avg_et,
                       MAX(queue_time) max_qt, MAX(exec_time) max_et,
                       MIN(queue_time) min_qt, MIN(exec_time) min_et,
                       COUNT(*) total
                FROM metrics
            """).fetchone()
            
            # Recent failures
            failures = conn.execute("""
                SELECT name, error FROM jobs 
                WHERE status='failed' 
                ORDER BY completed_at DESC LIMIT 5
            """).fetchall()
            
            return {
                'jobs': counts,
                'performance': dict(perf) if perf else {},
                'active_processes': len(self.processes),
                'max_workers': self.max_workers,
                'recent_failures': [dict(f) for f in failures]
            }
    
    def save_artifact(self, job_id: int, key: str, value: Any):
        """Save job output artifact"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute(
                "INSERT INTO artifacts (job_id, key, value) VALUES (?, ?, ?)",
                (job_id, key, pickle.dumps(value))
            )
    
    def get_artifact(self, job_id: int, key: str) -> Any:
        """Retrieve job output artifact"""
        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                "SELECT value FROM artifacts WHERE job_id=? AND key=?",
                (job_id, key)
            ).fetchone()
            return pickle.loads(row['value']) if row else None
    
    def cleanup(self, days: int = 7) -> int:
        """Clean old completed jobs and vacuum database"""
        cutoff = int(time.time() * 1000) - (days * 86400000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            deleted = conn.execute("""
                DELETE FROM jobs 
                WHERE status IN ('completed', 'failed') AND completed_at < ?
            """, (cutoff,)).rowcount
            
            # Vacuum if significantly fragmented
            page_count = conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = conn.execute("PRAGMA freelist_count").fetchone()[0]
            
            if freelist > page_count * 0.3:
                conn.execute("VACUUM")
        
        return deleted

class Scheduler:
    """Main scheduler with worker pool management"""
    
    def __init__(self, orchestrator: Orchestrator, batch_size: int = 1):
        self.orch = orchestrator
        self.batch_size = batch_size
        self.running = True
        
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)
    
    def _shutdown(self, *_):
        self.running = False
        print("\nScheduler shutting down...")
        
        # Terminate all processes
        for p in self.orch.processes.values():
            p.terminate()
            p.join(timeout=1)
    
    def run(self):
        """Main scheduler loop with intelligent job management"""
        print(f"Scheduler started (workers={self.orch.max_workers}, batch={self.batch_size})")
        maintenance_counter = 0
        
        while self.running:
            maintenance_counter += 1
            
            # Process completed jobs
            processed = self.orch.process_results()
            if processed:
                print(f"Processed {processed} job results")
            
            # Periodic maintenance
            if maintenance_counter % 50 == 0:
                reclaimed = self.orch.reclaim_stalled()
                if reclaimed:
                    print(f"Reclaimed {reclaimed} stalled jobs")
            
            # Clean up finished processes
            for job_id, process in list(self.orch.processes.items()):
                if not process.is_alive():
                    process.join(timeout=0.1)
                    if job_id in self.orch.processes:
                        del self.orch.processes[job_id]
            
            # Start new jobs if capacity available
            while len(self.orch.processes) < self.orch.max_workers:
                job = self.orch.get_next()
                
                if not job:
                    break
                
                print(f"Starting job {job['id']}: {job['name']} ({job['func']})")
                self.orch.execute(job)
            
            time.sleep(0.1)
        
        print("Scheduler stopped")

# Built-in job functions
def example_add(x: int, y: int) -> int:
    """Example job that adds two numbers"""
    time.sleep(1)
    return x + y

def example_fail():
    """Example job that always fails"""
    raise ValueError("This job always fails")

def example_long(duration: int = 10) -> str:
    """Example long-running job"""
    time.sleep(duration)
    return f"Completed after {duration} seconds"

def example_data_processor(input_file: str, output_file: str) -> Dict:
    """Example data processing job"""
    # Simulate processing
    time.sleep(2)
    return {"processed": True, "records": 1000, "output": output_file}

# Register built-in jobs
JobRegistry.register("add", example_add)
JobRegistry.register("fail", example_fail)
JobRegistry.register("long", example_long)
JobRegistry.register("process", example_data_processor)

def main():
    """CLI interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pure Python Job Orchestrator")
    sub = parser.add_subparsers(dest="cmd", required=True)
    
    # Add command
    add_p = sub.add_parser("add", help="Add a job")
    add_p.add_argument("name", help="Job name")
    add_p.add_argument("func", help="Function name")
    add_p.add_argument("--args", nargs="+", help="Arguments")
    add_p.add_argument("--kwargs", type=json.loads, help="Keyword arguments (JSON)")
    add_p.add_argument("--priority", type=int, default=0)
    add_p.add_argument("--depends", type=int, nargs="+", help="Dependency job IDs")
    add_p.add_argument("--retries", type=int, default=3, help="Max retries")
    add_p.add_argument("--timeout", type=int, default=300, help="Timeout seconds")
    add_p.add_argument("--nice", type=int, help="Process nice value")
    add_p.add_argument("--delay", type=int, default=0, help="Delay seconds")
    
    # Scheduler command
    sched_p = sub.add_parser("scheduler", help="Run scheduler")
    sched_p.add_argument("--workers", type=int, help="Max workers")
    sched_p.add_argument("--batch", type=int, default=1, help="Batch size")
    
    # List command
    list_p = sub.add_parser("list", help="List jobs")
    list_p.add_argument("--status", help="Filter by status")
    list_p.add_argument("--limit", type=int, default=50, help="Limit results")
    
    # Stats command
    sub.add_parser("stats", help="Show statistics")
    
    # Cleanup command
    cleanup_p = sub.add_parser("cleanup", help="Clean old jobs")
    cleanup_p.add_argument("--days", type=int, default=7)
    
    # Load module command
    load_p = sub.add_parser("load", help="Load job module")
    load_p.add_argument("module", help="Python module path")
    
    # Get artifact command
    art_p = sub.add_parser("artifact", help="Get job artifact")
    art_p.add_argument("job_id", type=int)
    art_p.add_argument("key")
    
    args = parser.parse_args()
    orch = Orchestrator()
    
    if args.cmd == 'add':
        # Parse arguments
        parsed_args = []
        if args.args:
            for arg in args.args:
                # Try to parse as number
                try:
                    parsed_args.append(int(arg))
                except ValueError:
                    try:
                        parsed_args.append(float(arg))
                    except ValueError:
                        parsed_args.append(arg)
        
        scheduled_at = None
        if args.delay:
            scheduled_at = int(time.time() * 1000) + (args.delay * 1000)
        
        job_id = orch.add(
            name=args.name,
            func=args.func,
            args=tuple(parsed_args) if parsed_args else None,
            kwargs=args.kwargs,
            priority=args.priority,
            depends_on=args.depends,
            max_retries=args.retries,
            timeout_sec=args.timeout,
            nice=args.nice,
            scheduled_at=scheduled_at
        )
        print(f"Added job {job_id}: {args.name}")
    
    elif args.cmd == 'scheduler':
        if args.workers:
            orch.max_workers = args.workers
        scheduler = Scheduler(orch, args.batch)
        scheduler.run()
    
    elif args.cmd == 'list':
        with sqlite3.connect(orch.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            query = "SELECT * FROM jobs"
            params = []
            
            if args.status:
                query += " WHERE status=?"
                params.append(args.status)
            
            query += " ORDER BY created_at DESC LIMIT ?"
            params.append(args.limit)
            
            jobs = conn.execute(query, params).fetchall()
            
            for job in jobs:
                created = time.strftime('%Y-%m-%d %H:%M:%S',
                                       time.localtime(job['created_at'] / 1000))
                print(f"[{job['id']}] {job['name']} ({job['func']}): "
                      f"{job['status']} (pri={job['priority']}, created={created})")
    
    elif args.cmd == 'stats':
        stats = orch.stats()
        print(json.dumps(stats, indent=2))
    
    elif args.cmd == 'cleanup':
        deleted = orch.cleanup(args.days)
        print(f"Deleted {deleted} old jobs")
    
    elif args.cmd == 'load':
        jobs = JobRegistry.load_module(args.module)
        print(f"Loaded module {args.module}")
        print(f"Available jobs: {jobs}")
    
    elif args.cmd == 'artifact':
        value = orch.get_artifact(args.job_id, args.key)
        if value:
            print(json.dumps(value) if not isinstance(value, bytes) else value)
        else:
            print(f"No artifact '{args.key}' for job {args.job_id}")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: consolidate_intcandidates.py
================================================================================

#!/usr/bin/env python3
"""
Script to consolidate all files from Integration/IntCandidates directory
into a single text file with filename headers and code content.
"""

import os
from pathlib import Path

def consolidate_files():
    # Define the directory path
    source_dir = Path("Integration/IntCandidates")
    output_file = "intcandidates_consolidated.txt"

    # Check if directory exists
    if not source_dir.exists():
        print(f"Error: Directory {source_dir} does not exist!")
        return

    # Get all Python files in the directory
    py_files = sorted(source_dir.glob("*.py"))

    if not py_files:
        print(f"No Python files found in {source_dir}")
        return

    # Create consolidated output
    with open(output_file, 'w', encoding='utf-8') as outfile:
        outfile.write(f"CONSOLIDATED FILES FROM {source_dir}\n")
        outfile.write("=" * 80 + "\n\n")

        for i, filepath in enumerate(py_files, 1):
            print(f"Processing {i}/{len(py_files)}: {filepath.name}")

            # Write file header
            outfile.write(f"\n{'='*80}\n")
            outfile.write(f"FILE: {filepath.name}\n")
            outfile.write(f"{'='*80}\n\n")

            # Write file content
            try:
                with open(filepath, 'r', encoding='utf-8') as infile:
                    content = infile.read()
                    outfile.write(content)
                    if not content.endswith('\n'):
                        outfile.write('\n')
            except Exception as e:
                outfile.write(f"ERROR READING FILE: {e}\n")

            outfile.write(f"\n{'='*80}\n")

    # Get file size
    file_size = os.path.getsize(output_file)
    file_size_kb = file_size / 1024

    print(f"\nâœ… Successfully consolidated {len(py_files)} files")
    print(f"ðŸ“„ Output file: {output_file}")
    print(f"ðŸ“Š Total size: {file_size_kb:.2f} KB ({file_size:,} bytes)")
    print(f"ðŸ“ Files included:")
    for f in py_files:
        print(f"   - {f.name}")

if __name__ == "__main__":
    consolidate_files()

================================================================================

================================================================================
FILE: deepseek2.py
================================================================================

#!/usr/bin/env python3
"""
Ultimate Job Orchestrator - Best-of-Breed Synthesis
Combines optimal performance, robustness, and simplicity in under 300 lines
"""
import sqlite3, subprocess, json, time, sys, os, signal, threading, argparse
from pathlib import Path
from typing import Optional, Dict, Any, List

# Global configuration
DB_PATH = Path.home() / ".ultimate_orchestrator.db"
UNIT_PREFIX = "ultorch-"
PRAGMAS = ["PRAGMA journal_mode=WAL", "PRAGMA synchronous=NORMAL", "PRAGMA busy_timeout=5000"]

class UltimateOrchestrator:
    """Unified orchestrator with hybrid execution modes"""
    
    def __init__(self, db_path: str = DB_PATH):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Minimal but complete schema"""
        for pragma in PRAGMAS:
            self.conn.execute(pragma)
            
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                command TEXT NOT NULL,
                mode TEXT DEFAULT 'auto',  -- auto, direct, systemd
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'queued', -- queued, running, completed, failed
                scheduled_at INTEGER DEFAULT (strftime('%s','now')),
                worker_id TEXT,
                retries INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                error_msg TEXT,
                result TEXT,
                created_at INTEGER DEFAULT (strftime('%s','now')),
                started_at INTEGER,
                completed_at INTEGER,
                dependencies TEXT,  -- JSON array
                resource_limits TEXT  -- JSON: {mem_mb, cpu_weight, nice, ...}
            );
            
            CREATE INDEX IF NOT EXISTS idx_ready ON jobs(status, priority DESC, scheduled_at) 
            WHERE status = 'queued';
        """)
    
    def add_job(self, name: str, command: str, mode: str = "auto", 
                priority: int = 0, delay: int = 0, dependencies: List[str] = None,
                max_retries: int = 3, **resource_limits) -> int:
        """Add job with smart defaults"""
        scheduled_at = int(time.time()) + delay
        deps_json = json.dumps(dependencies) if dependencies else None
        resources_json = json.dumps(resource_limits) if resource_limits else None
        
        with self.lock:
            cursor = self.conn.execute("""
                INSERT INTO jobs (name, command, mode, priority, scheduled_at, 
                                dependencies, max_retries, resource_limits)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (name, command, mode, priority, scheduled_at, deps_json, max_retries, resources_json))
            return cursor.lastrowid
    
    def _get_execution_mode(self, job: Dict) -> str:
        """Smart mode selection: systemd for long-running, direct for quick tasks"""
        if job['mode'] != 'auto':
            return job['mode']
        
        # Heuristic: commands with nohup, sleep, or server indicators use systemd
        systemd_indicators = ['nohup', 'server', 'service', 'daemon', 'sleep']
        if any(indicator in job['command'].lower() for indicator in systemd_indicators):
            return 'systemd'
        return 'direct'
    
    def _check_dependencies(self, job_id: int) -> bool:
        """Verify all dependencies are completed"""
        job = self.conn.execute("SELECT dependencies FROM jobs WHERE id = ?", (job_id,)).fetchone()
        if not job or not job['dependencies']:
            return True
            
        deps = json.loads(job['dependencies'])
        placeholders = ','.join('?' * len(deps))
        result = self.conn.execute(f"""
            SELECT COUNT(*) as incomplete FROM jobs 
            WHERE name IN ({placeholders}) AND status != 'completed'
        """, deps).fetchone()
        
        return result['incomplete'] == 0
    
    def get_ready_jobs(self, worker_id: str, limit: int = 5) -> List[Dict]:
        """Get jobs ready for execution with dependency resolution"""
        with self.lock:
            now = int(time.time())
            ready = []
            
            # Find eligible jobs
            candidates = self.conn.execute("""
                SELECT * FROM jobs 
                WHERE status = 'queued' AND scheduled_at <= ?
                ORDER BY priority DESC, scheduled_at, id
                LIMIT ?
            """, (now, limit * 2)).fetchall()  # Get extra to account for dependencies
            
            for job in candidates:
                if len(ready) >= limit:
                    break
                    
                if self._check_dependencies(job['id']):
                    # Atomically claim the job
                    updated = self.conn.execute("""
                        UPDATE jobs SET status = 'running', worker_id = ?, started_at = ?
                        WHERE id = ? AND status = 'queued'
                    """, (worker_id, now, job['id'])).rowcount
                    
                    if updated:
                        ready.append(dict(job))
            
            return ready
    
    def _execute_direct(self, job: Dict) -> tuple[bool, str, Any]:
        """Execute job directly with resource limits"""
        try:
            # Basic resource limits using ulimit if available
            resources = json.loads(job.get('resource_limits', '{}'))
            cmd = job['command']
            
            if resources.get('mem_mb'):
                cmd = f"ulimit -v {resources['mem_mb'] * 1024}; {cmd}"
            
            result = subprocess.run(
                cmd, shell=True, timeout=300, capture_output=True, text=True
            )
            
            success = result.returncode == 0
            output = {
                'stdout': result.stdout[:1000],
                'stderr': result.stderr[:1000],
                'returncode': result.returncode
            }
            return success, result.stderr if not success else None, output
            
        except subprocess.TimeoutExpired:
            return False, "Timeout after 300 seconds", None
        except Exception as e:
            return False, str(e), None
    
    def _execute_systemd(self, job: Dict) -> tuple[bool, str]:
        """Execute job as systemd transient unit"""
        try:
            unit_name = f"{UNIT_PREFIX}{job['name']}.service"
            resources = json.loads(job.get('resource_limits', '{}'))
            
            cmd = [
                'systemd-run', '--user', '--collect', '--quiet',
                '--unit', unit_name,
                '--property=StandardOutput=journal',
                '--property=StandardError=journal',
            ]
            
            # Apply resource limits
            if resources.get('nice'):
                cmd.extend(['--nice', str(resources['nice'])])
            if resources.get('mem_mb'):
                cmd.extend([f'--property=MemoryMax={resources["mem_mb"]}M'])
            
            cmd.extend(['--', 'sh', '-c', job['command']])
            
            result = subprocess.run(cmd, timeout=10, capture_output=True, text=True)
            
            if result.returncode == 0:
                return True, unit_name
            return False, result.stderr
            
        except Exception as e:
            return False, str(e)
    
    def complete_job(self, job_id: int, success: bool, error: str = None, result: Any = None):
        """Mark job completion with retry logic"""
        with self.lock:
            job = self.conn.execute(
                "SELECT retries, max_retries FROM jobs WHERE id = ?", (job_id,)
            ).fetchone()
            
            now = int(time.time())
            
            if success:
                self.conn.execute("""
                    UPDATE jobs SET status = 'completed', completed_at = ?, result = ?
                    WHERE id = ?
                """, (now, json.dumps(result) if result else None, job_id))
            else:
                if job and job['retries'] < job['max_retries']:
                    # Exponential backoff
                    delay = 60 * (2 ** job['retries'])
                    self.conn.execute("""
                        UPDATE jobs SET status = 'queued', retries = retries + 1,
                        scheduled_at = ?, error_msg = ?
                        WHERE id = ?
                    """, (now + delay, error, job_id))
                else:
                    self.conn.execute("""
                        UPDATE jobs SET status = 'failed', completed_at = ?, error_msg = ?
                        WHERE id = ?
                    """, (now, error, job_id))
    
    def reconcile_systemd_jobs(self):
        """Update status of systemd-managed jobs"""
        with self.lock:
            systemd_jobs = self.conn.execute("""
                SELECT id, name FROM jobs 
                WHERE status = 'running' AND mode IN ('systemd', 'auto')
            """).fetchall()
            
            for job in systemd_jobs:
                unit_name = f"{UNIT_PREFIX}{job['name']}.service"
                try:
                    result = subprocess.run([
                        'systemctl', '--user', 'show', unit_name,
                        '--property=ActiveState,Result'
                    ], capture_output=True, text=True, timeout=5)
                    
                    if result.returncode == 0:
                        info = dict(line.split('=', 1) for line in result.stdout.splitlines() if '=')
                        if info.get('ActiveState') in ('inactive', 'failed'):
                            success = info.get('Result') == 'success'
                            self.complete_job(job['id'], success, 
                                            error=None if success else f"Systemd: {info.get('Result')}")
                except:
                    pass  # Unit might not exist yet
    
    def cleanup_old_jobs(self, days: int = 7) -> int:
        """Remove old completed jobs"""
        cutoff = int(time.time()) - (days * 86400)
        with self.lock:
            deleted = self.conn.execute("""
                DELETE FROM jobs 
                WHERE status IN ('completed', 'failed') AND completed_at < ?
            """, (cutoff,)).rowcount
            return deleted
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        with self.lock:
            counts = dict(self.conn.execute(
                "SELECT status, COUNT(*) FROM jobs GROUP BY status"
            ).fetchall())
            
            performance = self.conn.execute("""
                SELECT 
                    AVG(started_at - created_at) as avg_queue_time,
                    AVG(completed_at - started_at) as avg_exec_time,
                    COUNT(*) as total_completed
                FROM jobs WHERE status = 'completed'
            """).fetchone()
            
            return {
                'job_counts': counts,
                'performance': dict(performance) if performance else {},
                'total_jobs': sum(counts.values())
            }

class SmartWorker:
    """Adaptive worker that chooses optimal execution strategy"""
    
    def __init__(self, orchestrator: UltimateOrchestrator, worker_id: str = None):
        self.orch = orchestrator
        self.worker_id = worker_id or f"worker-{os.getpid()}-{int(time.time())}"
        self.running = True
        self._setup_signals()
    
    def _setup_signals(self):
        """Graceful shutdown handling"""
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)
    
    def _shutdown(self, signum, frame):
        """Handle shutdown gracefully"""
        self.running = False
        print(f"\nWorker {self.worker_id} shutting down...")
    
    def run(self, batch_size: int = 3):
        """Main worker loop with adaptive execution"""
        print(f"SmartWorker {self.worker_id} started (batch_size={batch_size})")
        
        while self.running:
            try:
                # Periodic maintenance
                self.orch.reconcile_systemd_jobs()
                
                # Get batch of ready jobs
                jobs = self.orch.get_ready_jobs(self.worker_id, batch_size)
                
                if not jobs:
                    time.sleep(1)
                    continue
                
                for job in jobs:
                    if not self.running:
                        break
                    
                    execution_mode = self.orch._get_execution_mode(job)
                    
                    if execution_mode == 'direct':
                        success, error, result = self.orch._execute_direct(job)
                        self.orch.complete_job(job['id'], success, error, result)
                    else:  # systemd
                        success, unit_info = self.orch._execute_systemd(job)
                        if success:
                            print(f"Started systemd unit: {unit_info}")
                            # Job completion will be handled by reconcile
                        else:
                            self.orch.complete_job(job['id'], False, error=unit_info)
                            
            except Exception as e:
                print(f"Worker error: {e}")
                time.sleep(5)

def main():
    """Clean CLI interface"""
    parser = argparse.ArgumentParser(description="Ultimate Job Orchestrator")
    subparsers = parser.add_subparsers(dest='command', required=True)
    
    # Add job command
    add_parser = subparsers.add_parser('add', help='Add a new job')
    add_parser.add_argument('name', help='Job name (unique)')
    add_parser.add_argument('command', help='Command to execute')
    add_parser.add_argument('--mode', choices=['auto', 'direct', 'systemd'], default='auto',
                          help='Execution mode (default: auto)')
    add_parser.add_argument('--priority', type=int, default=0, help='Job priority')
    add_parser.add_argument('--delay', type=int, default=0, help='Delay execution by N seconds')
    add_parser.add_argument('--max-retries', type=int, default=3, help='Maximum retry attempts')
    add_parser.add_argument('--depends-on', nargs='*', help='Job dependencies by name')
    add_parser.add_argument('--mem-mb', type=int, help='Memory limit in MB')
    add_parser.add_argument('--nice', type=int, help='Nice level (-20 to 19)')
    add_parser.add_argument('--cpu-weight', type=int, help='CPU weight (1-10000)')
    
    # Worker command
    worker_parser = subparsers.add_parser('worker', help='Start smart worker')
    worker_parser.add_argument('--batch-size', type=int, default=3, help='Jobs to process simultaneously')
    worker_parser.add_argument('--worker-id', help='Worker identifier')
    
    # Management commands
    subparsers.add_parser('stats', help='Show system statistics')
    subparsers.add_parser('reconcile', help='Reconcile systemd jobs')
    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup old jobs')
    cleanup_parser.add_argument('--days', type=int, default=7, help='Remove jobs older than N days')
    
    args = parser.parse_args()
    orch = UltimateOrchestrator()
    
    if args.command == 'add':
        resource_limits = {}
        if args.mem_mb: resource_limits['mem_mb'] = args.mem_mb
        if args.nice: resource_limits['nice'] = args.nice
        if args.cpu_weight: resource_limits['cpu_weight'] = args.cpu_weight
        
        job_id = orch.add_job(
            name=args.name,
            command=args.command,
            mode=args.mode,
            priority=args.priority,
            delay=args.delay,
            dependencies=args.depends_on,
            max_retries=args.max_retries,
            **resource_limits
        )
        print(f"Added job {job_id}: {args.name}")
        
    elif args.command == 'worker':
        worker = SmartWorker(orch, args.worker_id)
        worker.run(args.batch_size)
        
    elif args.command == 'stats':
        stats = orch.get_stats()
        print(json.dumps(stats, indent=2))
        
    elif args.command == 'reconcile':
        orch.reconcile_systemd_jobs()
        print("Systemd jobs reconciled")
        
    elif args.command == 'cleanup':
        deleted = orch.cleanup_old_jobs(args.days)
        print(f"Cleaned up {deleted} old jobs")

if __name__ == '__main__':
    main()

================================================================================

================================================================================
FILE: geminiStudio2.py
================================================================================

#!/usr/bin/env python3
"""
job_orchestrator.py: A unified job orchestration and scheduling system.
Integrates direct process management with optional systemd execution for robustness.
"""
import argparse
import json
import os
import shlex
import sqlite3
import subprocess
import sys
import time
from typing import Any, Dict, List, Optional

# --- Database Setup and Management ---

def get_db_connection(db_path: str = "orchestrator.db") -> sqlite3.Connection:
    """Establishes and configures the SQLite database connection."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    # Optimizations from claudeCodeD
    conn.executescript("""
        PRAGMA journal_mode=WAL;
        PRAGMA synchronous=NORMAL;
        PRAGMA cache_size=-8000;
        PRAGMA temp_store=MEMORY;
        PRAGMA busy_timeout=5000;
    """)
    return conn

def initialize_schema(conn: sqlite3.Connection):
    """Initializes the database schema."""
    conn.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE NOT NULL,
            command TEXT NOT NULL,
            args TEXT DEFAULT '[]',
            status TEXT DEFAULT 'queued',
            priority INTEGER DEFAULT 0,
            scheduled_at INTEGER,
            worker_id TEXT,
            retry_count INTEGER DEFAULT 0,
            error_message TEXT,
            result TEXT,
            created_at INTEGER NOT NULL,
            started_at INTEGER,
            ended_at INTEGER,
            use_systemd BOOLEAN DEFAULT FALSE
        )
    """)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_status_priority ON jobs(status, priority DESC, scheduled_at)")

# --- Core Orchestrator Class ---

class JobOrchestrator:
    """Manages job lifecycle, from creation to execution and completion."""

    def __init__(self, db_path: str = "orchestrator.db"):
        self.conn = get_db_connection(db_path)
        initialize_schema(self.conn)

    def add_job(
        self,
        name: str,
        command: str,
        args: Optional[List[str]] = None,
        priority: int = 0,
        delay_ms: int = 0,
        use_systemd: bool = False,
    ) -> int:
        """Adds a new job to the database."""
        current_time = int(time.time() * 1000)
        scheduled_at = current_time + delay_ms if delay_ms > 0 else current_time
        args_json = json.dumps(args or [])

        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO jobs (name, command, args, priority, scheduled_at, use_systemd, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
            (name, command, args_json, priority, scheduled_at, use_systemd, current_time),
        )
        self.conn.commit()
        return cursor.lastrowid

    def list_jobs(self):
        """Lists all jobs in the database."""
        cursor = self.conn.execute("SELECT id, name, command, status, use_systemd FROM jobs ORDER BY created_at DESC")
        return cursor.fetchall()

    def get_next_job(self, worker_id: str) -> Optional[Dict[str, Any]]:
        """Atomically retrieves and locks the next available job."""
        now = int(time.time() * 1000)
        cursor = self.conn.cursor()

        # Find a suitable job
        cursor.execute(
            """
            SELECT id FROM jobs
            WHERE status = 'queued' AND scheduled_at <= ?
            ORDER BY priority DESC, scheduled_at
            LIMIT 1
            """,
            (now,),
        )
        job_row = cursor.fetchone()
        if not job_row:
            return None

        job_id = job_row['id']

        # Atomically claim it
        cursor.execute(
            """
            UPDATE jobs
            SET status = 'running', worker_id = ?, started_at = ?
            WHERE id = ? AND status = 'queued'
            """,
            (worker_id, now, job_id),
        )
        self.conn.commit()

        if cursor.rowcount > 0:
            job = self.conn.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            return dict(job)
        return None

    def finalize_job(
        self,
        job_id: int,
        success: bool,
        result: Optional[str] = None,
        error: Optional[str] = None,
    ):
        """Marks a job as done or failed."""
        status = 'done' if success else 'failed'
        ended_at = int(time.time() * 1000)
        self.conn.execute(
            "UPDATE jobs SET status = ?, result = ?, error_message = ?, ended_at = ?, worker_id = NULL WHERE id = ?",
            (status, result, error, ended_at, job_id),
        )
        self.conn.commit()

# --- Worker and Execution Logic ---

class Worker:
    """A worker that executes jobs."""

    def __init__(self, orchestrator: JobOrchestrator, worker_id: str):
        self.orchestrator = orchestrator
        self.worker_id = worker_id

    def run_once(self):
        """Fetches and runs a single job."""
        job = self.orchestrator.get_next_job(self.worker_id)
        if not job:
            print("No available jobs.")
            return

        print(f"Executing job {job['name']} (ID: {job['id']})...")
        command = [job['command']] + json.loads(job['args'])

        try:
            if job['use_systemd']:
                self._run_with_systemd(job, command)
            else:
                self._run_with_subprocess(job, command)
        except Exception as e:
            self.orchestrator.finalize_job(job['id'], success=False, error=str(e))
            print(f"An unexpected error occurred while running job {job['name']}: {e}")

    def _run_with_systemd(self, job: Dict[str, Any], command: List[str]):
        """Executes a job using systemd-run."""
        unit_name = f"orchestrator-{job['name']}.service"
        systemd_command = [
            "systemd-run",
            "--user",
            "--collect",
            "--quiet",
            f"--unit={unit_name}",
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
        ] + command

        result = subprocess.run(systemd_command, capture_output=True, text=True)

        if result.returncode == 0:
            self.orchestrator.finalize_job(job['id'], success=True, result=f"Systemd unit '{unit_name}' started.")
        else:
            self.orchestrator.finalize_job(job['id'], success=False, error=result.stderr)

    def _run_with_subprocess(self, job: Dict[str, Any], command: List[str]):
        """Executes a job using a direct subprocess call."""
        try:
            process = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=300,  # 5-minute timeout
                check=True,
            )
            output = {
                "stdout": process.stdout[:1000],
                "stderr": process.stderr[:1000],
            }
            self.orchestrator.finalize_job(job['id'], success=True, result=json.dumps(output))
        except subprocess.CalledProcessError as e:
            error_output = {
                "stdout": e.stdout[:1000],
                "stderr": e.stderr[:1000],
                "returncode": e.returncode,
            }
            self.orchestrator.finalize_job(job['id'], success=False, error=json.dumps(error_output))
        except subprocess.TimeoutExpired:
            self.orchestrator.finalize_job(job['id'], success=False, error="Job timed out after 300 seconds.")

# --- Command-Line Interface ---

def main():
    """Main function to handle command-line arguments."""
    parser = argparse.ArgumentParser(description="A unified job orchestrator.")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # 'add' command
    add_parser = subparsers.add_parser("add", help="Add a new job.")
    add_parser.add_argument("name", help="A unique name for the job.")
    add_parser.add_argument("cmd", help="The command to execute.")
    add_parser.add_argument("args", nargs="*", help="Arguments for the command.")
    add_parser.add_argument("--priority", type=int, default=0, help="Job priority (higher is sooner).")
    add_parser.add_argument("--delay", type=int, default=0, help="Delay execution by N milliseconds.")
    add_parser.add_argument("--systemd", action="store_true", help="Execute the job via systemd-run.")

    # 'list' command
    subparsers.add_parser("list", help="List all jobs.")

    # 'worker' command
    worker_parser = subparsers.add_parser("worker", help="Run a worker to execute a single job.")
    worker_parser.add_argument("--id", default=f"worker-{os.getpid()}", help="A unique ID for the worker.")

    args = parser.parse_args()
    orchestrator = JobOrchestrator()

    if args.command == "add":
        try:
            job_id = orchestrator.add_job(
                name=args.name,
                command=args.cmd,
                args=args.args,
                priority=args.priority,
                delay_ms=args.delay,
                use_systemd=args.systemd,
            )
            print(f"Job '{args.name}' added with ID: {job_id}")
        except sqlite3.IntegrityError:
            print(f"Error: A job with the name '{args.name}' already exists.", file=sys.stderr)
            sys.exit(1)

    elif args.command == "list":
        jobs = orchestrator.list_jobs()
        if not jobs:
            print("No jobs found.")
            return

        print(f"{'ID':<5} {'Name':<20} {'Status':<10} {'Systemd':<8} {'Command'}")
        print("-" * 70)
        for job in jobs:
            print(f"{job['id']:<5} {job['name']:<20} {job['status']:<10} {'Yes' if job['use_systemd'] else 'No':<8} {job['command']}")

    elif args.command == "worker":
        worker = Worker(orchestrator, args.id)
        worker.run_once()

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: geminiWeb.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Orchestrator: A unified job orchestrator and scheduler.

This script combines a high-performance SQLite task queue with robust systemd-based
process execution. It manages the entire lifecycle of tasks, including dependencies,
retries, scheduling, and resource management, by delegating process control to
systemd's transient unit capabilities.
"""
import argparse
import json
import os
import shlex
import signal
import sqlite3
import subprocess
import sys
import threading
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

# --- Configuration ---
DB_PATH = Path.home() / ".aios_orchestrator.db"
UNIT_PREFIX = "aios-"

# --- Systemd Command Helpers ---
# Use user scope if not root for broader compatibility without sudo.
USE_USER_SCOPE = os.geteuid() != 0
SYSTEMCTL = ["systemctl", "--user"] if USE_USER_SCOPE else ["systemctl"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"] if USE_USER_SCOPE else ["systemd-run", "--collect", "--quiet"]

# --- High-Performance SQLite Settings ---
PRAGMAS = """
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA cache_size=-8000;
PRAGMA temp_store=MEMORY;
PRAGMA busy_timeout=5000;
PRAGMA wal_autocheckpoint=1000;
"""

class Orchestrator:
    """Manages the task lifecycle, from database state to systemd execution."""

    def __init__(self, db_path=DB_PATH):
        """Initializes the database connection and schema."""
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.executescript(PRAGMAS + """
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                cmd TEXT NOT NULL,
                status TEXT DEFAULT 'pending',
                priority INT DEFAULT 0,
                retries INT DEFAULT 0,
                unit_name TEXT,
                props TEXT,   -- JSON for systemd properties (rtprio, mem_mb, etc.)
                deps TEXT,    -- JSON array of dependency IDs
                created_at INT,
                started_at INT,
                completed_at INT
            );
            CREATE INDEX IF NOT EXISTS idx_status_prio ON tasks(status, priority DESC, created_at);
        """)

    def _sh(self, cmd: List[str]) -> subprocess.CompletedProcess:
        """Helper for running shell commands."""
        return subprocess.run(cmd, text=True, capture_output=True, check=False)

    def add(self, name: str, cmd: str, **kwargs) -> Optional[int]:
        """Adds a task to the database."""
        props = {k: v for k, v in kwargs.items() if v is not None and k in
                 {'rtprio', 'mem_mb', 'cpu_w', 'schedule', 'nice', 'slice', 'cwd'}}
        try:
            cursor = self.conn.execute(
                "INSERT INTO tasks (name, cmd, priority, deps, props, created_at) VALUES (?, ?, ?, ?, ?, ?)",
                (name, cmd, kwargs.get("prio", 0), json.dumps(kwargs.get("deps") or []),
                 json.dumps(props), int(time.time() * 1000))
            )
            return cursor.lastrowid
        except sqlite3.IntegrityError:
            print(f"Error: Task name '{name}' already exists.", file=sys.stderr)
            return None

    def _get_runnable_tasks(self) -> List[sqlite3.Row]:
        """Finds pending tasks whose dependencies are met."""
        return self.conn.execute("""
            SELECT * FROM tasks
            WHERE status='pending' AND (deps IS NULL OR deps = '[]' OR NOT EXISTS (
                SELECT 1 FROM json_each(tasks.deps) AS d
                JOIN tasks AS dt ON dt.id = d.value
                WHERE dt.status != 'completed'
            )) ORDER BY priority DESC, created_at ASC
        """).fetchall()

    def _finalize_task(self, task_id: int, success: bool):
        """Marks a task as complete or requeues it for retry."""
        task = self.conn.execute("SELECT retries FROM tasks WHERE id=?", (task_id,)).fetchone()
        if not task: return

        if success:
            self.conn.execute("UPDATE tasks SET status='completed', completed_at=? WHERE id=?", (int(time.time() * 1000), task_id))
        elif task['retries'] < 3:
            delay_ms = 1000 * (2 ** task['retries'])
            new_at = int(time.time() * 1000) + delay_ms
            self.conn.execute("UPDATE tasks SET status='pending', retries=retries+1, at=? WHERE id=?", (new_at, task_id))
        else:
            self.conn.execute("UPDATE tasks SET status='failed', completed_at=? WHERE id=?", (int(time.time() * 1000), task_id))

    def _launch_task_via_systemd(self, task: sqlite3.Row):
        """Constructs and executes a systemd-run command for a given task."""
        unit_name = f"{UNIT_PREFIX}{task['name']}-{task['id']}.service"
        props = json.loads(task['props'] or '{}')
        cmd = [*SYSDRUN, "--unit", unit_name]
        
        # Apply properties
        prop_map = {
            "rtprio": ("CPUSchedulingPolicy=rr", f"CPUSchedulingPriority={props.get('rtprio')}"),
            "mem_mb": (f"MemoryMax={props.get('mem_mb')}M",),
            "cpu_w": (f"CPUWeight={props.get('cpu_w')}",),
            "nice": (f"Nice={props.get('nice')}",),
            "slice": (f"--slice={props.get('slice')}",), # Slice is a direct argument
            "cwd": (f"WorkingDirectory={props.get('cwd')}",)
        }
        for key, values in prop_map.items():
            if props.get(key) is not None:
                if key == 'slice':
                    cmd.append(values)
                else:
                    for value in values:
                        cmd.append(f"--property={value}")

        cmd.extend(["--property=StandardOutput=journal", "--property=StandardError=journal"])
        if props.get('schedule'):
            cmd.append(f"--on-calendar={props['schedule']}")
        
        cmd.extend(["--", "/bin/sh", "-c", task['cmd']])
        
        result = self._sh(cmd)
        if result.returncode == 0:
            status = 'scheduled' if props.get('schedule') else 'running'
            self.conn.execute("UPDATE tasks SET status=?, unit_name=?, started_at=? WHERE id=?",
                              (status, unit_name, int(time.time() * 1000), task['id']))
        else:
            print(f"Error launching systemd unit for task {task['id']}: {result.stderr.strip()}", file=sys.stderr)
            self._finalize_task(task['id'], success=False)

    def reconcile(self, running_tasks: Dict[int, str]):
        """Checks the status of systemd units and updates the database."""
        if not running_tasks: return
        
        res = self._sh([*SYSTEMCTL, "show", *running_tasks.values(), "--property=ActiveState,Result"])
        status_blocks = res.stdout.strip().split('\n\n')
        
        for i, (task_id, unit_name) in enumerate(running_tasks.items()):
            props = dict(line.split("=", 1) for line in status_blocks[i].splitlines() if "=" in line)
            if props.get("ActiveState") in ("inactive", "failed"):
                success = props.get("Result") == "success"
                print(f"Task {task_id} ('{unit_name}') finished. Success: {success}")
                self._finalize_task(task_id, success)

    def run_worker(self, max_concurrent: int = os.cpu_count()):
        """Main worker loop to manage the task lifecycle."""
        print(f"Worker started (PID: {os.getpid()}). Max concurrent jobs: {max_concurrent}")
        shutdown = threading.Event()
        signal.signal(signal.SIGINT, lambda s, f: shutdown.set())
        signal.signal(signal.SIGTERM, lambda s, f: shutdown.set())

        while not shutdown.is_set():
            with self.conn:
                running_tasks = {r['id']: r['unit_name'] for r in
                                 self.conn.execute("SELECT id, unit_name FROM tasks WHERE status='running' AND unit_name IS NOT NULL")}
            
            # 1. Reconcile current state
            self.reconcile(running_tasks)
            
            # 2. Launch new tasks if there's capacity
            num_to_launch = max_concurrent - len(running_tasks)
            if num_to_launch > 0:
                for task in self._get_runnable_tasks()[:num_to_launch]:
                    self._launch_task_via_systemd(dict(task))
            
            time.sleep(2) # Poll interval
        print("\nWorker shutdown complete.")

def main():
    """Defines and handles the command-line interface."""
    parser = argparse.ArgumentParser(description="AIOS Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)

    add_p = subparsers.add_parser("add", help="Add a new task.")
    add_p.add_argument("name", help="Unique name for the task.")
    add_p.add_argument("command", help="The shell command to execute.")
    add_p.add_argument("--prio", type=int, default=0, help="Priority (higher is sooner).")
    add_p.add_argument("--deps", type=json.loads, help="JSON list of task IDs, e.g., '[1,2]'.")
    add_p.add_argument("--schedule", help="systemd OnCalendar string for scheduling.")
    add_p.add_argument("--rtprio", type=int, help="Real-time priority (1-99).")
    add_p.add_argument("--mem_mb", type=int, help="Memory limit in MB.")
    add_p.add_argument("--cpu_w", type=int, help="CPU weight (1-10000).")

    worker_p = subparsers.add_parser("worker", help="Run a worker process.")
    worker_p.add_argument("concurrency", type=int, nargs="?", default=os.cpu_count(), help="Max concurrent jobs.")

    subparsers.add_parser("stats", help="Show queue statistics.")
    subparsers.add_parser("install", help="Generate a systemd unit file for the worker.")

    args = parser.parse_args()
    orch = Orchestrator()

    if args.command == "worker":
        orch.run_worker(max_concurrent=args.concurrency)
    elif args.command == "add":
        task_id = orch.add(args.name, args.command, prio=args.prio, deps=args.deps,
                           schedule=args.schedule, rtprio=args.rtprio,
                           mem_mb=args.mem_mb, cpu_w=args.cpu_w)
        if task_id:
            print(f"Task '{args.name}' added with ID {task_id}.")
    elif args.command == "stats":
        print(json.dumps(orch.get_stats(), indent=2))
    elif args.command == "install":
        print(generate_systemd_unit())

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: glm2.py
================================================================================



```python
#!/usr/bin/env python3
"""
Unified Job Orchestration System
Combines the best features from multiple implementations into a single, cohesive system
"""
import argparse, json, os, shlex, sqlite3, subprocess, sys, time, signal, threading
from pathlib import Path
from typing import Optional, Dict, Any, List, Union

# Configuration
DB_PATH = Path.home() / ".orchestrator.db"
UNIT_PREFIX = "orch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

# Optimized SQLite pragmas
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA mmap_size=268435456",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

class Orchestrator:
    """Unified job orchestration system with dual execution modes"""
    
    def __init__(self, db_path: str = DB_PATH):
        self.db_path = db_path
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Initialize database with optimized schema"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            # Apply performance pragmas
            for pragma in PRAGMAS:
                conn.execute(pragma)
            
            # Unified schema combining best features
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE,
                    cmd TEXT NOT NULL,
                    args TEXT,
                    env TEXT,
                    cwd TEXT,
                    mode TEXT DEFAULT 'local',  -- 'local' or 'systemd'
                    priority INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'queued',  -- queued, running, completed, failed, stopped
                    created_at INTEGER DEFAULT (strftime('%s','now')*1000),
                    scheduled_at INTEGER DEFAULT (strftime('%s','now')*1000),
                    started_at INTEGER,
                    ended_at INTEGER,
                    worker_id TEXT,
                    retry_count INTEGER DEFAULT 0,
                    max_retries INTEGER DEFAULT 3,
                    error_message TEXT,
                    result TEXT,
                    dependencies TEXT,  -- JSON array of job IDs
                    schedule TEXT,  -- systemd calendar format
                    unit TEXT,  -- systemd unit name
                    -- Resource controls
                    nice INTEGER,
                    rtprio INTEGER,
                    slice TEXT,
                    cpu_weight INTEGER,
                    mem_max_mb INTEGER
                );
                
                CREATE INDEX IF NOT EXISTS idx_jobs_queue 
                ON jobs(status, priority DESC, scheduled_at, id) 
                WHERE status IN ('queued', 'running');
                
                CREATE TABLE IF NOT EXISTS metrics (
                    job_id INTEGER PRIMARY KEY,
                    queue_time REAL,
                    exec_time REAL,
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
            """)
    
    def add_job(self, name: str, cmd: str, args: List[str] = None, 
                env: Dict[str, str] = None, cwd: str = None, 
                mode: str = 'local', priority: int = 0,
                scheduled_at: int = None, dependencies: List[int] = None,
                schedule: str = None, max_retries: int = 3,
                nice: int = None, rtprio: int = None, slice: str = None,
                cpu_weight: int = None, mem_max_mb: int = None) -> int:
        """Add a new job to the system"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            return conn.execute("""
                INSERT INTO jobs (
                    name, cmd, args, env, cwd, mode, priority, scheduled_at,
                    dependencies, schedule, max_retries, nice, rtprio, slice, 
                    cpu_weight, mem_max_mb
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                name, cmd, json.dumps(args or []), json.dumps(env or {}), 
                cwd, mode, priority, scheduled_at or int(time.time() * 1000),
                json.dumps(dependencies or []), schedule, max_retries,
                nice, rtprio, slice, cpu_weight, mem_max_mb
            )).lastrowid
    
    def get_job(self, job_id: int = None, name: str = None) -> Optional[Dict]:
        """Get job by ID or name"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            if job_id:
                row = conn.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            elif name:
                row = conn.execute("SELECT * FROM jobs WHERE name = ?", (name,)).fetchone()
            else:
                return None
                
            if row:
                job = dict(row)
                # Convert JSON fields
                if job['args']:
                    job['args'] = json.loads(job['args'])
                if job['env']:
                    job['env'] = json.loads(job['env'])
                if job['dependencies']:
                    job['dependencies'] = json.loads(job['dependencies'])
                return job
            return None
    
    def _check_dependencies(self, conn: sqlite3.Connection, job_id: int) -> bool:
        """Check if all dependencies are completed"""
        row = conn.execute(
            "SELECT dependencies FROM jobs WHERE id=?", (job_id,)
        ).fetchone()
        
        if not row or not row['dependencies']:
            return True
            
        deps = json.loads(row['dependencies'])
        incomplete = conn.execute("""
            SELECT COUNT(*) as c FROM jobs 
            WHERE id IN ({}) AND status != 'completed'
        """.format(','.join('?' * len(deps))), deps).fetchone()
        
        return incomplete['c'] == 0
    
    def pop_job(self, worker_id: str) -> Optional[Dict]:
        """Get next available job for worker execution"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            # Find next eligible job
            row = conn.execute("""
                SELECT id, cmd, args, env, cwd, mode FROM jobs
                WHERE status = 'queued' AND scheduled_at <= ? AND mode = 'local'
                ORDER BY priority DESC, scheduled_at, id
                LIMIT 1
            """, (now,)).fetchone()
            
            if not row:
                return None
            
            # Check dependencies
            if not self._check_dependencies(conn, row['id']):
                return None
            
            # Try to claim the job
            try:
                result = conn.execute("""
                    UPDATE jobs 
                    SET status = 'running', worker_id = ?, started_at = ?
                    WHERE id = ? AND status = 'queued'
                    RETURNING id, cmd, args, env, cwd, mode
                """, (worker_id, now, row['id'])).fetchone()
                
                if result:
                    job = dict(result)
                    # Convert JSON fields
                    if job['args']:
                        job['args'] = json.loads(job['args'])
                    if job['env']:
                        job['env'] = json.loads(job['env'])
                    return job
                return None
            except sqlite3.OperationalError:
                # Fallback for older SQLite
                conn.execute("BEGIN IMMEDIATE")
                updated = conn.execute(
                    "UPDATE jobs SET status = 'running', worker_id = ?, started_at = ? "
                    "WHERE id = ? AND status = 'queued'",
                    (worker_id, now, row['id'])
                ).rowcount
                conn.execute("COMMIT")
                
                if updated:
                    job = dict(row)
                    # Convert JSON fields
                    if job['args']:
                        job['args'] = json.loads(job['args'])
                    if job['env']:
                        job['env'] = json.loads(job['env'])
                    return job
                return None
    
    def start_job_systemd(self, job_id: int) -> bool:
        """Start a job using systemd"""
        job = self.get_job(job_id)
        if not job or job['mode'] != 'systemd':
            return False
        
        unit = f"{UNIT_PREFIX}{job['name'] or job['id']}.service"
        props = [
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",
        ]
        
        # Add resource control properties
        if job.get('rtprio'):
            props.extend([
                "--property=CPUSchedulingPolicy=rr",
                f"--property=CPUSchedulingPriority={job['rtprio']}"
            ])
        if job.get('nice') is not None:
            props.append(f"--property=Nice={job['nice']}")
        if job.get('slice'):
            props.append(f"--slice={job['slice']}")
        if job.get('cpu_weight'):
            props.append(f"--property=CPUWeight={job['cpu_weight']}")
        if job.get('mem_max_mb'):
            props.append(f"--property=MemoryMax={job['mem_max_mb']}M")
        
        # Add environment variables
        env = []
        if job.get('env'):
            for k, v in job['env'].items():
                env.extend(["--setenv", f"{k}={v}"])
        
        # Add schedule if provided
        schedule = []
        if job.get('schedule'):
            schedule.extend(["--on-calendar", job['schedule']])
        
        # Add working directory
        if job.get('cwd'):
            props.append(f"--property=WorkingDirectory={job['cwd']}")
        
        # Build command
        cmd = [
            *SYSDRUN, "--unit", unit, *props, *env, *schedule,
            "--", job['cmd'], *job.get('args', [])
        ]
        
        # Execute systemd-run
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                # Update job status
                with sqlite3.connect(self.db_path, isolation_level=None) as conn:
                    conn.execute(
                        "UPDATE jobs SET unit=?, status=? WHERE id=?",
                        (unit, "running", job_id)
                    )
                return True
            return False
        except Exception:
            return False
    
    def complete_job(self, job_id: int, success: bool = True, 
                     result: Any = None, error: str = None) -> bool:
        """Mark a job as completed"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            # Get job details
            job = conn.execute(
                "SELECT * FROM jobs WHERE id = ?", (job_id,)
            ).fetchone()
            
            if not job:
                return False
            
            if success:
                # Update job status
                conn.execute("""
                    UPDATE jobs 
                    SET status = 'completed', ended_at = ?, result = ?, worker_id = NULL
                    WHERE id = ?
                """, (now, json.dumps(result) if result else None, job_id))
                
                # Record metrics
                if job['started_at']:
                    queue_time = (job['started_at'] - job['created_at']) / 1000.0
                    exec_time = (now - job['started_at']) / 1000.0
                    conn.execute(
                        "INSERT OR REPLACE INTO metrics(job_id, queue_time, exec_time) VALUES (?, ?, ?)",
                        (job_id, queue_time, exec_time)
                    )
                return True
            else:
                # Handle failure with retry logic
                if job['retry_count'] < job['max_retries']:
                    # Exponential backoff: 1s, 2s, 4s
                    delay = 1000 * (2 ** job['retry_count'])
                    conn.execute("""
                        UPDATE jobs 
                        SET status = 'queued', retry_count = retry_count + 1, 
                            error_message = ?, worker_id = NULL, scheduled_at = ?
                        WHERE id = ?
                    """, (error, now + delay, job_id))
                else:
                    # Final failure
                    conn.execute("""
                        UPDATE jobs 
                        SET status = 'failed', ended_at = ?, error_message = ?, worker_id = NULL
                        WHERE id = ?
                    """, (now, error, job_id))
                return True
    
    def stop_job(self, job_id: int = None, name: str = None) -> bool:
        """Stop a running job"""
        job = self.get_job(job_id, name)
        if not job:
            return False
        
        if job.get('unit'):
            # Stop systemd unit
            try:
                subprocess.run(
                    SYSTEMCTL + ["stop", job['unit']], 
                    capture_output=True, check=True
                )
                # Also stop timer if it exists
                subprocess.run(
                    SYSTEMCTL + ["stop", job['unit'].replace('.service', '.timer')], 
                    capture_output=True, check=False
                )
            except subprocess.CalledProcessError:
                pass
        
        # Update status
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute(
                "UPDATE jobs SET status = 'stopped' WHERE id = ?",
                (job['id'],)
            )
        return True
    
    def reconcile_systemd_jobs(self):
        """Update status of systemd-managed jobs"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            running_jobs = conn.execute(
                "SELECT id, name, unit FROM jobs WHERE status = 'running' AND mode = 'systemd' AND unit IS NOT NULL"
            ).fetchall()
            
            for job in running_jobs:
                # Check systemd unit status
                try:
                    result = subprocess.run([
                        'systemctl', '--user', 'show', job['unit'],
                        '--property=ActiveState,Result,MainPID'
                    ], capture_output=True, text=True, check=True)
                    
                    if result.returncode == 0:
                        props = dict(line.split('=', 1) for line in result.stdout.strip().split('\n') if '=' in line)
                        active = props.get('ActiveState')
                        
                        if active in ('inactive', 'failed'):
                            success = props.get('Result') == 'success'
                            self.complete_job(job['id'], success)
                except subprocess.CalledProcessError:
                    pass
    
    def list_jobs(self, status: str = None) -> List[Dict]:
        """List jobs, optionally filtered by status"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            if status:
                rows = conn.execute(
                    "SELECT * FROM jobs WHERE status = ? ORDER BY created_at DESC",
                    (status,)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT * FROM jobs ORDER BY created_at DESC"
                ).fetchall()
            
            jobs = []
            for row in rows:
                job = dict(row)
                # Convert JSON fields
                if job['args']:
                    job['args'] = json.loads(job['args'])
                if job['env']:
                    job['env'] = json.loads(job['env'])
                if job['dependencies']:
                    job['dependencies'] = json.loads(job['dependencies'])
                jobs.append(job)
            
            return jobs
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            # Job counts by status
            counts = {row['status']: row['count'] for row in conn.execute(
                "SELECT status, COUNT(*) as count FROM jobs GROUP BY status"
            )}
            
            # Performance metrics
            perf = conn.execute("""
                SELECT AVG(queue_time) as avg_queue_time, 
                       AVG(exec_time) as avg_exec_time,
                       MAX(queue_time) as max_queue_time,
                       MAX(exec_time) as max_exec_time
                FROM metrics
            """).fetchone()
            
            return {
                'job_counts': counts,
                'performance': dict(perf) if perf else {}
            }
    
    def cleanup(self, days: int = 7) -> int:
        """Clean up old completed/failed jobs"""
        cutoff = int(time.time() * 1000) - (days * 86400000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            deleted = conn.execute("""
                DELETE FROM jobs 
                WHERE status IN ('completed', 'failed') AND ended_at < ?
            """, (cutoff,)).rowcount
            
            # Vacuum if needed
            page_count = conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = conn.execute("PRAGMA freelist_count").fetchone()[0]
            
            if freelist > page_count * 0.3:
                conn.execute("VACUUM")
            
            return deleted

class Worker:
    """Worker process for local job execution"""
    
    def __init__(self, orchestrator: Orchestrator, worker_id: str = None):
        self.orchestrator = orchestrator
        self.worker_id = worker_id or f"worker-{os.getpid()}"
        self.running = True
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)
    
    def _shutdown(self, *_):
        self.running = False
    
    def run(self, batch_size: int = 1):
        """Main worker loop"""
        print(f"Worker {self.worker_id} started (batch_size={batch_size})")
        
        while self.running:
            # Reconcile systemd jobs periodically
            if int(time.time()) % 10 == 0:
                self.orchestrator.reconcile_systemd_jobs()
            
            # Process batch of jobs
            jobs = []
            for _ in range(batch_size):
                job = self.orchestrator.pop_job(self.worker_id)
                if job:
                    jobs.append(job)
            
            if not jobs:
                time.sleep(0.05)
                continue
            
            for job in jobs:
                if not self.running:
                    break
                
                try:
                    # Execute command
                    env = os.environ.copy()
                    env.update(job.get('env', {}))
                    
                    result = subprocess.run(
                        [job['cmd']] + job.get('args', []),
                        env=env,
                        cwd=job.get('cwd'),
                        capture_output=True,
                        text=True,
                        timeout=300
                    )
                    
                    self.orchestrator.complete_job(
                        job['id'],
                        result.returncode == 0,
                        {
                            'stdout': result.stdout[:1000],
                            'stderr': result.stderr[:1000],
                            'returncode': result.returncode
                        },
                        result.stderr if result.returncode != 0 else None
                    )
                except subprocess.TimeoutExpired:
                    self.orchestrator.complete_job(
                        job['id'], False, error="TIMEOUT"
                    )
                except Exception as e:
                    self.orchestrator.complete_job(
                        job['id'], False, error=str(e)
                    )

def main():
    """Command-line interface"""
    parser = argparse.ArgumentParser(description="Job Orchestration System")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # Add command
    add_parser = subparsers.add_parser("add", help="Add a new job")
    add_parser.add_argument("name", help="Job name")
    add_parser.add_argument("command", help="Command to execute")
    add_parser.add_argument("args", nargs="*", help="Command arguments")
    add_parser.add_argument("--env", action="append", help="Environment variables (KEY=VAL)")
    add_parser.add_argument("--cwd", help="Working directory")
    add_parser.add_argument("--mode", choices=["local", "systemd"], default="local", help="Execution mode")
    add_parser.add_argument("--priority", type=int, default=0, help="Job priority")
    add_parser.add_argument("--schedule", help="Systemd calendar schedule")
    add_parser.add_argument("--delay", type=int, default=0, help="Delay execution by N seconds")
    add_parser.add_argument("--dependencies", nargs="*", type=int, help="Dependency job IDs")
    add_parser.add_argument("--max-retries", type=int, default=3, help="Maximum retry attempts")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority")
    add_parser.add_argument("--nice", type=int, help="Nice value")
    add_parser.add_argument("--slice", help="Systemd slice")
    add_parser.add_argument("--cpu-weight", type=int, help="CPU weight")
    add_parser.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add_parser.add_argument("--start", action="store_true", help="Start job immediately")
    
    # List command
    list_parser = subparsers.add_parser("list", help="List jobs")
    list_parser.add_argument("--status", help="Filter by status")
    
    # Start command
    start_parser = subparsers.add_parser("start", help="Start a job")
    start_parser.add_argument("name", help="Job name")
    
    # Stop command
    stop_parser = subparsers.add_parser("stop", help="Stop a job")
    stop_parser.add_argument("name", help="Job name")
    
    # Status command
    status_parser = subparsers.add_parser("status", help="Show job status")
    status_parser.add_argument("name", help="Job name")
    
    # Worker command
    worker_parser = subparsers.add_parser("worker", help="Run worker process")
    worker_parser.add_argument("--batch", type=int, default=1, help="Batch size")
    
    # Stats command
    subparsers.add_parser("stats", help="Show system statistics")
    
    # Cleanup command
    cleanup_parser = subparsers.add_parser("cleanup", help="Clean up old jobs")
    cleanup_parser.add_argument("--days", type=int, default=7, help="Keep jobs newer than N days")
    
    # Reconcile command
    subparsers.add_parser("reconcile", help="Reconcile systemd job statuses")
    
    args = parser.parse_args()
    orchestrator = Orchestrator()
    
    if args.command == "add":
        env = {}
        if args.env:
            for e in args.env:
                if "=" in e:
                    k, v = e.split("=", 1)
                    env[k] = v
        
        scheduled_at = None
        if args.delay > 0:
            scheduled_at = int(time.time() * 1000) + (args.delay * 1000)
        
        job_id = orchestrator.add_job(
            name=args.name,
            cmd=args.command,
            args=args.args,
            env=env,
            cwd=args.cwd,
            mode=args.mode,
            priority=args.priority,
            scheduled_at=scheduled_at,
            dependencies=args.dependencies,
            schedule=args.schedule,
            max_retries=args.max_retries,
            rtprio=args.rtprio,
            nice=args.nice,
            slice=args.slice,
            cpu_weight=args.cpu_weight,
            mem_max_mb=args.mem_max_mb
        )
        
        print(f"Added job {job_id}")
        
        if args.start:
            if args.mode == "systemd":
                success = orchestrator.start_job_systemd(job_id)
                print(f"Started with systemd: {'Success' if success else 'Failed'}")
            else:
                # Job will be picked up by a worker
                print("Job queued for worker execution")
    
    elif args.command == "list":
        jobs = orchestrator.list_jobs(args.status)
        for job in jobs:
            print(f"{job['id']}: {job['name']} - {job['status']} ({job['mode']})")
    
    elif args.command == "start":
        job = orchestrator.get_job(name=args.name)
        if not job:
            print(f"Job '{args.name}' not found")
            return
        
        if job['mode'] == "systemd":
            success = orchestrator.start_job_systemd(job['id'])
            print(f"Started with systemd: {'Success' if success else 'Failed'}")
        else:
            # Reset status to queued so worker can pick it up
            with sqlite3.connect(orchestrator.db_path, isolation_level=None) as conn:
                conn.execute(
                    "UPDATE jobs SET status = 'queued' WHERE id = ?",
                    (job['id'],)
                )
            print("Job queued for worker execution")
    
    elif args.command == "stop":
        success = orchestrator.stop_job(name=args.name)
        print(f"Stopped: {'Success' if success else 'Failed'}")
    
    elif args.command == "status":
        job = orchestrator.get_job(name=args.name)
        if not job:
            print(f"Job '{args.name}' not found")
            return
        
        print(f"Status: {job['status']}")
        if job.get('unit'):
            try:
                result = subprocess.run(
                    SYSTEMCTL + ["show", job['unit'], "--property", "ActiveState"],
                    capture_output=True, text=True, check=True
                )
                print(f"Systemd: {result.stdout.strip()}")
            except subprocess.CalledProcessError:
                print("Systemd: Not found")
    
    elif args.command == "worker":
        worker = Worker(orchestrator)
        worker.run(args.batch)
    
    elif args.command == "stats":
        stats = orchestrator.get_stats()
        print(json.dumps(stats, indent=2))
    
    elif args.command == "cleanup":
        deleted = orchestrator.cleanup(args.days)
        print(f"Deleted {deleted} old jobs")
    
    elif args.command == "reconcile":
        orchestrator.reconcile_systemd_jobs()
        print("Reconciliation completed")

if __name__ == "__main__":
    main()
```

================================================================================

================================================================================
FILE: grok2.py
================================================================================

#!/usr/bin/env python3
"""
Unified Job Orchestrator: Enhanced with best features from all implementations.
SQLite state, systemd execution, deps, retries, scheduling, metrics table.
Supports job ID or name in CLI. Bench for perf testing. Under 500 lines.
"""
import argparse, json, os, shlex, sqlite3, subprocess, sys, time
from pathlib import Path
from typing import Dict, Any, Optional, Union
DB = Path.home() / ".unified_orchestrator.db"
UNIT_PREFIX = "uniorch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA mmap_size=268435456",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]
def sh(cmd: list[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)
def ok(cp): return cp.returncode == 0
def unit_name(identifier: str) -> str:
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in identifier)
    return f"{UNIT_PREFIX}{safe}.service"
def db_conn():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    for pragma in PRAGMAS:
        con.execute(pragma)
    con.executescript("""
        CREATE TABLE IF NOT EXISTS jobs(
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE,
            cmd TEXT NOT NULL,
            args TEXT,
            env TEXT,
            cwd TEXT,
            schedule TEXT,
            scheduled_at INTEGER,
            priority INTEGER DEFAULT 0,
            deps TEXT,
            max_retries INTEGER DEFAULT 3,
            retry_count INTEGER DEFAULT 0,
            timeout_sec INTEGER,
            rtprio INTEGER,
            nice INTEGER,
            slice TEXT,
            cpu_weight INTEGER,
            mem_max_mb INTEGER,
            unit TEXT,
            status TEXT DEFAULT 'added',
            created_at INTEGER DEFAULT (strftime('%s','now')*1000),
            started_at INTEGER,
            ended_at INTEGER,
            error TEXT
        );
        CREATE INDEX IF NOT EXISTS idx_queue ON jobs(status, priority DESC, scheduled_at, id)
        WHERE status IN ('added', 'queued', 'failed');
        CREATE TABLE IF NOT EXISTS metrics (
            job_id INTEGER PRIMARY KEY,
            queue_time REAL,
            exec_time REAL,
            FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
        );
    """)
    return con
def add_job(**kw) -> int:
    kw.setdefault("created_at", int(time.time() * 1000))
    with db_conn() as con:
        cursor = con.execute("""INSERT INTO jobs
            (name,cmd,args,env,cwd,schedule,scheduled_at,priority,deps,max_retries,retry_count,
             timeout_sec,rtprio,nice,slice,cpu_weight,mem_max_mb,unit,status,created_at,
             started_at,ended_at,error)
            VALUES(:name,:cmd,:args,:env,:cwd,:schedule,:scheduled_at,:priority,:deps,:max_retries,:retry_count,
                   :timeout_sec,:rtprio,:nice,:slice,:cpu_weight,:mem_max_mb,:unit,:status,:created_at,
                   :started_at,:ended_at,:error)""", kw)
        return cursor.lastrowid
def get_job(identifier: Union[int, str]) -> Optional[Dict[str, Any]]:
    with db_conn() as con:
        if isinstance(identifier, int):
            row = con.execute("SELECT * FROM jobs WHERE id=?", (identifier,)).fetchone()
        else:
            row = con.execute("SELECT * FROM jobs WHERE name=?", (identifier,)).fetchone()
        if row:
            job = dict(row)
            job['args'] = json.loads(job['args']) if job['args'] else []
            job['env'] = json.loads(job['env']) if job['env'] else {}
            job['deps'] = json.loads(job['deps']) if job['deps'] else []
            return job
        return None
def list_jobs(status: Optional[str] = None) -> list:
    with db_conn() as con:
        if status:
            rows = con.execute("SELECT * FROM jobs WHERE status=? ORDER BY created_at DESC", (status,)).fetchall()
        else:
            rows = con.execute("SELECT * FROM jobs ORDER BY created_at DESC").fetchall()
        return [dict(row) for row in rows]
def show(unit: str, *props: str) -> Dict[str, str]:
    out = sh(SYSTEMCTL + ["show", unit, *(["--property=" + p for p in props] if props else [])]).stdout
    return {k: v for k, v in (line.split("=", 1) for line in out.splitlines() if "=" in line)}
def start_transient(job: Dict[str, Any]) -> tuple[bool, str, str]:
    identifier = job["name"] or str(job["id"])
    unit = unit_name(identifier)
    props = [
        "--property=StandardOutput=journal",
        "--property=StandardError=journal",
        "--property=KillMode=control-group",
    ]
    if job["timeout_sec"]:
        props += [f"--property=TimeoutSec={job['timeout_sec']}"]
    if job["rtprio"]:
        props += ["--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={job['rtprio']}"]
    calc_nice = job["nice"]
    if calc_nice is None and job["priority"] is not None:
        calc_nice = 10 - job["priority"]
    if calc_nice is not None:
        props += [f"--property=Nice={int(calc_nice)}"]
    if job["slice"]:
        props += [f"--slice={job['slice']}"]
    if job["cpu_weight"]:
        props += [f"--property=CPUWeight={job['cpu_weight']}"]
    if job["mem_max_mb"]:
        props += [f"--property=MemoryMax={int(job['mem_max_mb'])}M"]
    if job["cwd"]:
        props += [f"--property=WorkingDirectory={job['cwd']}"]
    env = []
    if job["env"]:
        for k, v in job["env"].items():
            env += ["--setenv", f"{k}={v}"]
    when = []
    persistent = False
    now_s = time.time()
    if job["schedule"]:
        when = ["--on-calendar", job["schedule"]]
        persistent = True
    elif job["scheduled_at"]:
        sched_s = job["scheduled_at"] / 1000.0
        delay_s = max(0, sched_s - now_s)
        if delay_s > 0:
            when = ["--on-active", f"{int(delay_s + 0.5)}s"]
            persistent = True
    if persistent:
        props += ["--property=Persistent=true"]
    cmd_args = [job["cmd"], *job["args"]]
    cp = sh([*SYSDRUN, "--unit", unit, *props, *env, *when, "--", *cmd_args])
    with db_conn() as con:
        new_status = "scheduled" if when else "started"
        con.execute("UPDATE jobs SET unit=?, status=? WHERE id=?", (unit, new_status, job["id"]))
    return ok(cp), unit, cp.stderr.strip() or cp.stdout.strip()
def stop(identifier: Union[int, str]):
    job = get_job(identifier)
    if not job or not job["unit"]:
        return
    unit = job["unit"]
    sh(SYSTEMCTL + ["stop", unit])
    sh(SYSTEMCTL + ["stop", unit.replace(".service", ".timer")])
    with db_conn() as con:
        con.execute("UPDATE jobs SET status='stopped', unit=NULL WHERE id=?", (job["id"],))
def job_status(identifier: Union[int, str]) -> Dict[str, Any]:
    job = get_job(identifier)
    if not job:
        return {"active": "unknown"}
    if not job["unit"]:
        return {"unit": None, "active": job["status"]}
    info = show(job["unit"], "ActiveState", "Result", "MainPID")
    return {"unit": job["unit"], "active": info.get("ActiveState"), "result": info.get("Result"), "pid": info.get("MainPID")}
def reconcile():
    now_ms = int(time.time() * 1000)
    with db_conn() as con:
        jobs = con.execute("SELECT * FROM jobs").fetchall()
        for row in jobs:
            job = dict(row)
            if job["unit"]:
                unit = job["unit"]
                info = show(unit, "ActiveState", "Result", "ExecMainStartTimestamp", "ExecMainExitTimestamp")
                active = info.get("ActiveState")
                if active in ("inactive", "failed"):
                    result = info.get("Result")
                    new_status = "completed" if result == "success" else "failed"
                    err = None if new_status == "completed" else f"Systemd result: {result}. See journalctl -u {unit}"
                    con.execute("UPDATE jobs SET status=?, error=?, ended_at=? WHERE id=?",
                                (new_status, err, now_ms, job["id"]))
                    # Record metrics
                    start_ts = info.get("ExecMainStartTimestamp")
                    end_ts = info.get("ExecMainExitTimestamp")
                    if start_ts and start_ts.isdigit() and end_ts and end_ts.isdigit():
                        started_at = int(int(start_ts) / 1000)
                        ended_at = int(int(end_ts) / 1000)
                        qt = (started_at - job["created_at"]) / 1000.0
                        et = (ended_at - started_at) / 1000.0
                        con.execute("INSERT OR REPLACE INTO metrics(job_id, queue_time, exec_time) VALUES(?,?,?)",
                                    (job["id"], qt, et))
                elif active == "active":
                    if job["status"] != "running":
                        con.execute("UPDATE jobs SET status='running' WHERE id=?", (job["id"],))
                    start_ts = info.get("ExecMainStartTimestamp")
                    if start_ts and start_ts.isdigit() and not job["started_at"]:
                        started_at = int(int(start_ts) / 1000)
                        con.execute("UPDATE jobs SET started_at=? WHERE id=?", (started_at, job["id"]))
            if job["status"] in ("added", "failed"):
                if job["status"] == "failed" and job["retry_count"] >= job["max_retries"]:
                    continue
                deps_ok = True
                if job["deps"]:
                    for dep_id in job["deps"]:
                        dep = con.execute("SELECT status FROM jobs WHERE id=?", (dep_id,)).fetchone()
                        if not dep or dep["status"] != "completed":
                            deps_ok = False
                            break
                if not deps_ok:
                    continue
                if job["scheduled_at"] and job["scheduled_at"] > now_ms:
                    continue
                if job["status"] == "failed":
                    backoff_ms = job["retry_delay"] * (2 ** job["retry_count"])
                    if now_ms < (job["ended_at"] or 0) + backoff_ms:
                        continue
                    con.execute("UPDATE jobs SET retry_count=retry_count+1 WHERE id=?", (job["id"],))
                if job["unit"]:
                    stop(job["id"])
                ok_, unit, msg = start_transient(job)
                if not ok_:
                    continue
                con.execute("UPDATE jobs SET started_at=? WHERE id=? AND started_at IS NULL", (now_ms, job["id"]))
def stats() -> Dict[str, Any]:
    with db_conn() as con:
        counts = {r["status"]: r["c"] for r in con.execute("SELECT status, COUNT(*) c FROM jobs GROUP BY status")}
        perf = con.execute("""
            SELECT AVG(queue_time) avg_qt, AVG(exec_time) avg_et,
                   MAX(queue_time) max_qt, MAX(exec_time) max_et
            FROM metrics
        """).fetchone()
        return {"jobs": counts, "perf": dict(perf) if perf else {}}
def cleanup(days: int = 7) -> int:
    cutoff = int(time.time() * 1000) - (days * 86400000)
    with db_conn() as con:
        deleted = con.execute("DELETE FROM jobs WHERE status IN ('completed','failed') AND ended_at < ?", (cutoff,)).rowcount
        page_count = con.execute("PRAGMA page_count").fetchone()[0]
        freelist = con.execute("PRAGMA freelist_count").fetchone()[0]
        if freelist > page_count * 0.3:
            con.execute("VACUUM")
        return deleted
def systemd_unit():
    return f"""[Unit]
Description=Unified Job Orchestrator
After=network.target
[Service]
Type=simple
ExecStart=/usr/bin/python3 {os.path.abspath(__file__)} run
Restart=always
RestartSec=3
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=10
LimitNOFILE=65536
Nice=-5
PrivateTmp=yes
StandardOutput=journal
StandardError=journal
[Install]
WantedBy=multi-user.target"""
def bench():
    db_file = "bench.db"
    if os.path.exists(db_file):
        os.unlink(db_file)
    db = JobOrchestrator(db_file)
    start = time.time()
    for i in range(1000):
        db.add_job(name=f"test{i}", cmd="echo", args=[str(i)], priority=i%10)
    ins_time = (time.time() - start) * 1000
    start = time.time()
    db.list_jobs()
    list_time = (time.time() - start) * 1000
    print(f"1000 inserts: {ins_time:.2f}ms\nList: {list_time:.2f}ms")
def main():
    ap = argparse.ArgumentParser(description="Unified Job Orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)
    a = sub.add_parser("add", help="add job")
    a.add_argument("name", nargs="?")
    a.add_argument("command")
    a.add_argument("args", nargs="*")
    a.add_argument("--env", action="append", default=[])
    a.add_argument("--cwd")
    a.add_argument("--schedule")
    a.add_argument("--delay-ms", type=int)
    a.add_argument("--priority", type=int)
    a.add_argument("--deps", help="comma-separated IDs")
    a.add_argument("--max-retries", type=int)
    a.add_argument("--timeout-sec", type=int)
    a.add_argument("--rtprio", type=int)
    a.add_argument("--nice", type=int)
    a.add_argument("--slice")
    a.add_argument("--cpu-weight", type=int)
    a.add_argument("--mem-max-mb", type=int)
    a.add_argument("--start", action="store_true")
    l = sub.add_parser("list", help="list jobs")
    l.add_argument("--status")
    s = sub.add_parser("start", help="start job")
    s.add_argument("identifier")
    t = sub.add_parser("stop", help="stop job")
    t.add_argument("identifier")
    u = sub.add_parser("status", help="job status")
    u.add_argument("identifier")
    sub.add_parser("reconcile", help="reconcile states")
    sub.add_parser("run", help="run reconciler loop")
    sub.add_parser("stats", help="show stats")
    c = sub.add_parser("cleanup", help="cleanup old")
    c.add_argument("--days", type=int, default=7)
    sub.add_parser("systemd", help="generate unit file")
    sub.add_parser("bench", help="run benchmark")
    args = ap.parse_args()
    if args.cmd == "add":
        env_d = dict(e.split("=", 1) for e in args.env) if args.env else {}
        deps_l = [int(d) for d in args.deps.split(",")] if args.deps else []
        name = args.name or f"job-{int(time.time())}"
        job_id = add_job(
            name=name, cmd=args.command, args=json.dumps(args.args),
            env=json.dumps(env_d), cwd=args.cwd, schedule=args.schedule,
            scheduled_at=(args.delay_ms + int(time.time() * 1000) if args.delay_ms else None),
            priority=args.priority, deps=json.dumps(deps_l), max_retries=args.max_retries,
            timeout_sec=args.timeout_sec, rtprio=args.rtprio, nice=args.nice, slice=args.slice,
            cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb, unit=None, status="added",
            started_at=None, ended_at=None, error=None
        )
        print(f"Added job ID: {job_id} Name: {name}")
        if args.start:
            job = get_job(job_id)
            ok_, unit, msg = start_transient(job)
            print(unit if ok_ else f"ERROR: {msg}")
    elif args.cmd == "list":
        reconcile()
        jobs = list_jobs(args.status)
        if jobs:
            print("| ID | Name | Status | Unit | Schedule |")
            print("|--- |---- |------ |---- |-------- |")
            for job in jobs:
                sched = job["schedule"] or job["scheduled_at"] or "-"
                print(f"| {job['id']} | {job['name'] or '-'} | {job['status']} | {job['unit'] or '-'} | {sched} |")
        else:
            print("No jobs")
    elif args.cmd == "start":
        job = get_job(args.identifier)
        if not job:
            sys.exit("unknown job")
        ok_, unit, msg = start_transient(job)
        print(unit if ok_ else f"ERROR: {msg}")
    elif args.cmd == "stop":
        stop(args.identifier)
        print("stopped")
    elif args.cmd == "status":
        print(json.dumps(job_status(args.identifier), indent=2))
    elif args.cmd == "reconcile":
        reconcile()
        print("ok")
    elif args.cmd == "run":
        print("Orchestrator running...")
        while True:
            reconcile()
            time.sleep(1)
    elif args.cmd == "stats":
        print(json.dumps(stats(), indent=2))
    elif args.cmd == "cleanup":
        deleted = cleanup(args.days)
        print(f"Deleted {deleted} old jobs")
    elif args.cmd == "systemd":
        print(systemd_unit())
    elif args.cmd == "bench":
        bench()
if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: kimi2.py
================================================================================

#!/usr/bin/env python3
"""
aiosq â€“ tiny unified job-queue + systemd orchestrator
One SQLite file (~/.aiosq.db) + systemd-run for cgroups, auto cleanup, no zombies.
"""
from __future__ import annotations
import argparse, json, os, shlex, sqlite3, subprocess, sys, time, signal, threading
from pathlib import Path
from typing import Dict, Any, Optional, List

DB_PATH   = Path.home() / ".aiosq.db"
UNIT_PREF = "aiosq-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN   = ["systemd-run", "--user", "--collect", "--quiet"]

# ---------- SQLite helpers ---------------------------------------------------
def conn() -> sqlite3.Connection:
    c = sqlite3.connect(DB_PATH, isolation_level=None, check_same_thread=False)
    c.row_factory = sqlite3.Row
    c.executescript("""
        PRAGMA journal_mode=WAL;  PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS t(
            id INTEGER PRIMARY KEY,
            cmd TEXT NOT NULL,
            p  INTEGER DEFAULT 0,               -- priority
            s  TEXT DEFAULT 'q',                -- status
            at INTEGER DEFAULT 0,               -- scheduled_at ms
            w  TEXT,                            -- worker
            r  INTEGER DEFAULT 0,               -- retry
            e  TEXT,                            -- error
            res TEXT,                           -- result
            ct INTEGER DEFAULT (unixepoch()*1000),
            st INTEGER, et INTEGER,             -- start/end
            dep TEXT                            -- json list
        );
        CREATE INDEX IF NOT EXISTS ix ON t(s,p DESC,at,id) WHERE s IN ('q','r');
    """)
    return c

LOCK = threading.RLock()

# ---------- Core queue ops ---------------------------------------------------
def add(cmd:str, p:int=0, at:int=0, dep:Optional[List[int]]=None) -> int:
    with LOCK:
        return conn().execute(
            "INSERT INTO t(cmd,p,at,dep) VALUES(?,?,?,?)",
            (cmd, p, at or int(time.time()*1000), json.dumps(dep) if dep else None)
        ).lastrowid

def pop(worker:str) -> Optional[Dict[str,Any]]:
    now = int(time.time()*1000)
    with LOCK:
        row = conn().execute("""
            SELECT id,cmd FROM t
            WHERE s='q' AND at<=? AND
                  (dep IS NULL OR NOT EXISTS(
                      SELECT 1 FROM json_each(t.dep) j JOIN t d ON d.id=j.value WHERE d.s!='d'))
            ORDER BY p DESC,at,id LIMIT 1
        """, (now,)).fetchone()
        if not row: return None
        conn().execute("UPDATE t SET s='r',w=?,st=? WHERE id=? AND s='q'", (worker,now,row['id']))
        return dict(row)

def done(task_id:int, ok:bool=True, worker:str="", err:str=""):
    now = int(time.time()*1000)
    with LOCK:
        if ok:
            conn().execute("UPDATE t SET s='d',et=?,w=NULL,res=? WHERE id=? AND w=?",
                           (now, json.dumps(err) if err else None, task_id, worker))
        else:
            row = conn().execute("SELECT r FROM t WHERE id=? AND w=?", (task_id,worker)).fetchone()
            if row and row['r']<3:
                delay = 1000*(2**row['r'])
                conn().execute("UPDATE t SET s='q',at=?,r=r+1,e=?,w=NULL WHERE id=?",
                               (now+delay,err,task_id))
            else:
                conn().execute("UPDATE t SET s='f',et=?,e=?,w=NULL WHERE id=?", (now,err,task_id))

# ---------- systemd helpers --------------------------------------------------
def unit(name:str) -> str: return f"{UNIT_PREF}{name.replace('/','_')}.service"
def sh(cmd): return subprocess.run(cmd,text=True,capture_output=True)

def start_transient(name:str, cmdline:str, env:dict=None, nice:int=None, rtprio:int=None,
                    slice:str=None, cpuw:int=None, mem:int=None, sched:str=None):
    props = ["--property=StandardOutput=journal", "--property=StandardError=journal"]
    if nice is not None: props.append(f"--property=Nice={nice}")
    if rtprio: props += ["--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={rtprio}"]
    if slice: props.append(f"--slice={slice}")
    if cpuw: props.append(f"--property=CPUWeight={cpuw}")
    if mem: props.append(f"--property=MemoryMax={mem}M")
    eargs = []
    if env:
        for k,v in env.items(): eargs += ["--setenv",f"{k}={v}"]
    when = []
    if sched: when += ["--on-calendar", sched]
    u = unit(name)
    full = [*SYSDRUN, "--unit", u, *props, *eargs, *when, "--", *shlex.split(cmdline)]
    cp = sh(full)
    return cp.returncode==0, u, cp.stderr or cp.stdout

# ---------- Worker loop ------------------------------------------------------
class Worker:
    def __init__(self, batch:int=1):
        self.batch = batch
        self.running = True
        signal.signal(signal.SIGTERM, lambda *_: setattr(self,'running',False))
        signal.signal(signal.SIGINT,  lambda *_: setattr(self,'running',False))

    def run(self):
        wid = f"w{os.getpid()}"
        print(f"Worker {wid} started (batch={self.batch})")
        while self.running:
            tasks = []
            for _ in range(self.batch):
                t = pop(wid)
                if t: tasks.append(t)
            if not tasks:
                time.sleep(.05); continue
            for t in tasks:
                if not self.running: break
                try:
                    res = subprocess.run(t['cmd'], shell=True, capture_output=True,
                                         text=True, timeout=290)
                    done(t['id'], res.returncode==0, wid,
                         err=res.stderr[:500] if res.returncode else None)
                except Exception as e:
                    done(t['id'], False, wid, err=str(e))

# ---------- CLI --------------------------------------------------------------
def stats():
    c = conn()
    counts = {r['s']:r['c'] for r in c.execute("SELECT s,COUNT(*) c FROM t GROUP BY s")}
    return {"tasks":counts}

def cleanup(days:int=7):
    cutoff = int(time.time()*1000)-days*86400*1000
    with LOCK:
        deleted = conn().execute("DELETE FROM t WHERE s IN ('d','f') AND et<?",(cutoff,)).rowcount
    return deleted

def cli():
    ap = argparse.ArgumentParser(description="aiosq â€“ queue + systemd orchestrator")
    sp = ap.add_subparsers(dest="cmd",required=True)

    a = sp.add_parser("add", help="add task")
    a.add_argument("cmd"); a.add_argument("-p","--priority",type=int,default=0)
    a.add_argument("-d","--delay",type=int,default=0)
    a.add_argument("--dep",type=int,nargs="+")

    sp.add_parser("worker").add_argument("-b","--batch",type=int,default=1)
    sp.add_parser("stats")
    c = sp.add_parser("cleanup"); c.add_argument("days",type=int,nargs="?",default=7)

    r = sp.add_parser("run", help="run via systemd")
    r.add_argument("name"); r.add_argument("cmdline")
    r.add_argument("--nice",type=int); r.add_argument("--rtprio",type=int)
    r.add_argument("--slice"); r.add_argument("--cpu-weight",type=int)
    r.add_argument("--mem-max",type=int)
    r.add_argument("--on-calendar"); r.add_argument("--env",action="append",default=[])

    s = sp.add_parser("stop"); s.add_argument("name")
    st = sp.add_parser("status"); st.add_argument("name")

    args = ap.parse_args()

    if args.cmd == "add":
        at = int(time.time()*1000)+args.delay if args.delay else 0
        tid = add(args.cmd, args.priority, at, args.dep)
        print(tid)

    elif args.cmd == "worker":
        Worker(args.batch).run()

    elif args.cmd == "stats":
        print(json.dumps(stats(),indent=2))

    elif args.cmd == "cleanup":
        print(cleanup(args.days))

    elif args.cmd == "run":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        ok,u,msg = start_transient(args.name, args.cmdline, env, args.nice, args.rtprio,
                                   args.slice, args.cpu_weight, args.mem_max, args.on_calendar)
        print(u if ok else f"error: {msg}")

    elif args.cmd == "stop":
        u = unit(args.name)
        sh(SYSTEMCTL + ["stop", u])
        sh(SYSTEMCTL + ["stop", u.replace(".service",".timer")])
        print("stopped")

    elif args.cmd == "status":
        info = sh(SYSTEMCTL + ["show", unit(args.name), "--property=ActiveState,Result,MainPID"]).stdout
        print(info or "unknown")

if __name__ == "__main__":
    cli()

================================================================================

================================================================================
FILE: minimax2.py
================================================================================

#!/usr/bin/env python3
"""
Integrated Job Orchestrator: SQLite + systemd process management
"""
import sqlite3, subprocess, json, time, sys, os, signal, threading, argparse
from pathlib import Path

PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

class JobManager:
    def __init__(self, db="jobs.db"):
        self.db = db
        self.lock = threading.RLock()
        self._init_db()

    def _init_db(self):
        with self.lock, sqlite3.connect(self.db) as conn:
            conn.row_factory = sqlite3.Row
            for pragma in PRAGMAS:
                conn.execute(pragma)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    cmd TEXT NOT NULL,
                    args TEXT,
                    env TEXT,
                    cwd TEXT,
                    p INT DEFAULT 0,
                    s TEXT DEFAULT 'q',
                    at INT DEFAULT 0,
                    w TEXT,
                    r INT DEFAULT 0,
                    e TEXT,
                    res TEXT,
                    ct INT DEFAULT (strftime('%s','now')*1000),
                    st INT,
                    et INT,
                    dep TEXT,
                    schedule TEXT,
                    rtprio INTEGER,
                    nice INTEGER,
                    slice TEXT,
                    cpu_weight INTEGER,
                    mem_max_mb INTEGER,
                    unit TEXT
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS ix ON jobs(s,p DESC,at,id)
                WHERE s IN ('q','r')
            """)

    def add_job(self, cmd, args=None, env=None, cwd=None, priority=0, 
                scheduled_at=None, deps=None, schedule=None, rtprio=None,
                nice=None, slice=None, cpu_weight=None, mem_max_mb=None):
        now = int(time.time() * 1000)
        at = int(scheduled_at * 1000) if scheduled_at else now
        dep_json = json.dumps(deps) if deps else None
        
        with self.lock, sqlite3.connect(self.db) as conn:
            return conn.execute("""
                INSERT INTO jobs 
                (cmd, args, env, cwd, p, at, dep, schedule, rtprio, nice, slice, cpu_weight, mem_max_mb)
                VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)
            """, (
                cmd, 
                json.dumps(args) if args else None,
                json.dumps(env) if env else None,
                cwd,
                priority,
                at,
                dep_json,
                schedule,
                rtprio,
                nice,
                slice,
                cpu_weight,
                mem_max_mb
            )).lastrowid

    def get_ready_jobs(self):
        now = int(time.time() * 1000)
        with self.lock, sqlite3.connect(self.db) as conn:
            return [dict(row) for row in conn.execute("""
                SELECT * FROM jobs
                WHERE s='q' AND at <= ? 
                AND (dep IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(dep) AS d
                    JOIN jobs AS j ON j.id = d.value
                    WHERE j.s != 'd'
                ))
                ORDER BY p DESC, at, id
            """, (now,))]

    def start_job(self, job_id):
        with self.lock, sqlite3.connect(self.db) as conn:
            job = conn.execute("SELECT * FROM jobs WHERE id=?", (job_id,)).fetchone()
            if not job or job['s'] != 'q':
                return False
            
            unit = f"job-{job_id}.service"
            props = [
                "--property=StandardOutput=journal",
                "--property=StandardError=journal",
                "--property=KillMode=control-group"
            ]
            
            if job['rtprio']:
                props += [
                    "--property=CPUSchedulingPolicy=rr",
                    f"--property=CPUSchedulingPriority={job['rtprio']}"
                ]
            if job['nice'] is not None:
                props.append(f"--property=Nice={job['nice']}")
            if job['slice']:
                props.append(f"--slice={job['slice']}")
            if job['cpu_weight']:
                props.append(f"--property=CPUWeight={job['cpu_weight']}")
            if job['mem_max_mb']:
                props.append(f"--property=MemoryMax={job['mem_max_mb']}M")
            if job['cwd']:
                props.append(f"--property=WorkingDirectory={job['cwd']}")
            
            cmd = [*SYSDRUN, "--unit", unit, *props]
            
            if job['env']:
                for k, v in json.loads(job['env']).items():
                    cmd += ["--setenv", f"{k}={v}"]
            
            if job['schedule']:
                cmd += ["--on-calendar", job['schedule']]
            
            cmd += ["--", job['cmd']]
            if job['args']:
                cmd += json.loads(job['args'])
            
            if subprocess.run(cmd, capture_output=True).returncode == 0:
                conn.execute("UPDATE jobs SET s='r', unit=?, w=? WHERE id=?", 
                           (unit, os.getpid(), job_id))
                return True
            return False

    def reconcile(self):
        with self.lock, sqlite3.connect(self.db) as conn:
            for job in conn.execute("SELECT id, unit FROM jobs WHERE s='r' AND unit IS NOT NULL"):
                cp = subprocess.run(
                    [*SYSTEMCTL, "show", job['unit'], 
                     "--property=ActiveState", "--property=Result", 
                     "--property=ExecMainExitCode"],
                    capture_output=True, text=True
                )
                if cp.returncode != 0:
                    continue
                
                props = dict(line.split('=', 1) for line in cp.stdout.splitlines() if '=' in line)
                active = props.get("ActiveState", "")
                result = props.get("Result", "")
                
                if active in ("inactive", "failed"):
                    if result == "success":
                        conn.execute("""
                            UPDATE jobs SET s='d', et=?, res=? 
                            WHERE id=?
                        """, (int(time.time()*1000), json.dumps({'exit': props.get('ExecMainExitCode')}), job['id']))
                    else:
                        conn.execute("""
                            UPDATE jobs SET s='f', e=?, et=? 
                            WHERE id=?
                        """, (f"Exit {props.get('ExecMainExitCode')}", 
                              int(time.time()*1000), job['id']))

class Worker:
    def __init__(self, jm):
        self.jm = jm
        self.running = True
        signal.signal(signal.SIGTERM, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGINT,  lambda *_: setattr(self, 'running', False))

    def run(self):
        while self.running:
            self.jm.reconcile()
            for job in self.jm.get_ready_jobs():
                if not self.running:
                    break
                if self.jm.start_job(job['id']):
                    print(f"Started job {job['id']}")
            time.sleep(0.1)

def main():
    parser = argparse.ArgumentParser(description="Integrated Job Orchestrator")
    sub = parser.add_subparsers(dest="cmd", required=True)
    
    add = sub.add_parser("add", help="Add new job")
    add.add_argument("cmd", help="Command to execute")
    add.add_argument("--args", type=json.loads, default=[])
    add.add_argument("--env", type=json.loads, default={})
    add.add_argument("--cwd", help="Working directory")
    add.add_argument("--priority", type=int, default=0)
    add.add_argument("--schedule", help="Systemd calendar schedule")
    add.add_argument("--rtprio", type=int, help="Real-time priority")
    add.add_argument("--nice", type=int, help="Nice value")
    add.add_argument("--slice", help="Systemd slice")
    add.add_argument("--cpu-weight", type=int, help="CPU weight")
    add.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add.add_argument("--deps", type=json.loads, default=[], help="Dependency IDs")
    
    sub.add_parser("worker", help="Start worker process")
    
    args = parser.parse_args()
    
    jm = JobManager()
    
    if args.cmd == "add":
        job_id = jm.add_job(
            cmd=args.cmd,
            args=args.args,
            env=args.env,
            cwd=args.cwd,
            priority=args.priority,
            schedule=args.schedule,
            rtprio=args.rtprio,
            nice=args.nice,
            slice=args.slice,
            cpu_weight=args.cpu_weight,
            mem_max_mb=args.mem_max_mb,
            deps=args.deps
        )
        print(f"Added job {job_id}")
    
    elif args.cmd == "worker":
        Worker(jm).run()

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: qwen2.py
================================================================================

#!/usr/bin/env python3
"""
Orchestrator: High-Performance Job Queue + Systemd Integration (<500 lines)
Synthesizes the best patterns from all candidates into a minimal, robust system.
"""
import argparse, json, os, shlex, signal, sqlite3, subprocess, sys, time, threading
from pathlib import Path
from typing import Optional, Dict, Any

# --- Configuration & Constants ---
DB_PATH = Path.home() / ".orchestrator.db"
UNIT_PREFIX = "orch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

# Production-grade SQLite pragmas (claudeCodeD, chatgpt, geminiDeep)
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

def now_ms() -> int:
    return int(time.time() * 1000)

def unit_name(name: str) -> str:
    """Generate a safe systemd unit name."""
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
    return f"{UNIT_PREFIX}{safe}.service"

# --- Core Storage Engine ---
class Store:
    """High-performance, thread-safe SQLite storage with atomic operations."""
    def __init__(self, path=DB_PATH):
        path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        
        # Apply performance optimizations
        for pragma in PRAGMAS:
            self.conn.execute(pragma)
        
        # Unified, minimal schema (Synthesized from chatgpt, geminiDeep, grok)
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                cmd TEXT NOT NULL,
                args TEXT DEFAULT '[]',
                env TEXT,
                cwd TEXT,
                p INT DEFAULT 0,        -- priority
                s TEXT DEFAULT 'q',     -- status: q=queued, r=running, d=done, f=failed
                at INT DEFAULT 0,       -- scheduled_at (ms)
                w TEXT,                 -- worker_id
                r INT DEFAULT 0,        -- retry_count
                e TEXT,                 -- error_message
                res TEXT,               -- result
                ct INT DEFAULT (strftime('%s','now')*1000), -- created_at
                st INT, et INT,         -- started_at, ended_at
                dep TEXT,               -- JSON array of dependency IDs
                schedule TEXT,          -- systemd calendar format
                rtprio INTEGER,
                nice INTEGER,
                slice TEXT,
                cpu_weight INTEGER,
                mem_max_mb INTEGER,
                unit TEXT               -- systemd unit name
            );
            -- Optimized composite index for fetching (claudeCodeD pattern)
            CREATE INDEX IF NOT EXISTS idx_fetch ON tasks(s, p DESC, at, id) WHERE s = 'q';
            -- Metrics table (Chrome pattern from claudeCodeD)
            CREATE TABLE IF NOT EXISTS metrics (
                task_id INTEGER PRIMARY KEY,
                qt REAL,  -- queue_time (seconds)
                et REAL,  -- exec_time (seconds)
                FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
            );
        """)

    # --- Atomic Queue Operations ---
    def pop(self, worker_id: str) -> Optional[Dict[str, Any]]:
        """Atomically claim the next eligible task, respecting dependencies and schedule."""
        now = now_ms()
        query = """
            SELECT id, cmd, args, env, cwd, rtprio, nice, slice, cpu_weight, mem_max_mb, schedule
            FROM tasks t
            WHERE s='q' AND at<=:now
            AND (dep IS NULL OR NOT EXISTS (
                SELECT 1 FROM json_each(t.dep) AS d
                JOIN tasks AS dt ON dt.id = d.value
                WHERE dt.s != 'd'
            ))
            ORDER BY p DESC, at, id
            LIMIT 1
        """
        with self.lock:
            row = self.conn.execute(query, {'now': now}).fetchone()
            if not row:
                return None

            # Atomically claim using UPDATE...RETURNING
            try:
                claimed = self.conn.execute("""
                    UPDATE tasks SET s='r', w=?, st=? 
                    WHERE id=? AND s='q' 
                    RETURNING id, cmd, args, env, cwd, rtprio, nice, slice, cpu_weight, mem_max_mb, schedule
                """, (worker_id, now, row['id'])).fetchone()
                if claimed:
                    task = dict(claimed)
                    for key in ['args', 'env']:
                        if task[key]:
                            task[key] = json.loads(task[key])
                    return task
            except sqlite3.OperationalError:
                # Fallback for older SQLite versions
                self.conn.execute("BEGIN IMMEDIATE")
                updated = self.conn.execute(
                    "UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q'",
                    (worker_id, now, row['id'])
                ).rowcount
                self.conn.execute("COMMIT")
                if updated:
                    task = dict(row)
                    for key in ['args', 'env']:
                        if task[key]:
                            task[key] = json.loads(task[key])
                    return task
        return None

    def finalize(self, task_id: int, success: bool, error: str = None):
        """Mark task as complete (d/f) or schedule retry (q), and record metrics."""
        now = now_ms()
        with self.lock:
            task = self.conn.execute("SELECT r, ct, st, unit FROM tasks WHERE id=?", (task_id,)).fetchone()
            if not task:
                return

            if success:
                self.conn.execute("UPDATE tasks SET s='d', et=?, unit=NULL WHERE id=?", (now, task_id))
                # Record performance metrics
                if task['st'] and task['ct']:
                    queue_time = (task['st'] - task['ct']) / 1000.0
                    exec_time = (now - task['st']) / 1000.0
                    self.conn.execute(
                        "INSERT OR REPLACE INTO metrics(task_id, qt, et) VALUES(?,?,?)",
                        (task_id, queue_time, exec_time)
                    )
            else:
                if task['r'] < 3:
                    # Exponential backoff retry (1s, 2s, 4s)
                    delay = 1000 * (2 ** task['r'])
                    self.conn.execute(
                        "UPDATE tasks SET s='q', at=?, r=r+1, e=?, unit=NULL WHERE id=?",
                        (now + delay, error, task_id)
                    )
                else:
                    self.conn.execute(
                        "UPDATE tasks SET s='f', et=?, e=?, unit=NULL WHERE id=?",
                        (now, error, task_id)
                    )

    # --- CRUD Operations ---
    def add(self, **kw) -> int:
        """Add a new task to the queue."""
        with self.lock:
            cols = ",".join(kw.keys())
            qs = ",".join("?" for _ in kw)
            cursor = self.conn.execute(f"INSERT INTO tasks({cols}) VALUES({qs})", tuple(kw.values()))
            return cursor.lastrowid

    def get_by_name(self, name: str) -> Optional[sqlite3.Row]:
        with self.lock:
            return self.conn.execute("SELECT * FROM tasks WHERE name=?", (name,)).fetchone()

    def list(self) -> list:
        with self.lock:
            return self.conn.execute("SELECT * FROM tasks ORDER BY ct DESC").fetchall()

    def update_unit(self, task_id: int, unit_name: str, status: str):
        with self.lock:
            self.conn.execute("UPDATE tasks SET unit=?, s=? WHERE id=?", (unit_name, status, task_id))

    def stats(self) -> Dict[str, Any]:
        """Return comprehensive system statistics."""
        with self.lock:
            counts = {r['s']: r['c'] for r in self.conn.execute("SELECT s, COUNT(*) c FROM tasks GROUP BY s")}
            perf = self.conn.execute("SELECT AVG(qt) avg_qt, AVG(et) avg_et, COUNT(*) count FROM metrics").fetchone()
            return {
                'tasks': counts,
                'perf': dict(perf) if perf and perf['avg_qt'] is not None else {}
            }

    def cleanup(self, days: int = 7) -> int:
        """Clean up old completed/failed tasks."""
        cutoff = now_ms() - (days * 86400000)
        with self.lock:
            deleted = self.conn.execute("DELETE FROM tasks WHERE s IN ('d','f') AND et < ?", (cutoff,)).rowcount
            # Vacuum if significantly fragmented (Firefox pattern)
            page_count = self.conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = self.conn.execute("PRAGMA freelist_count").fetchone()[0]
            if freelist > page_count * 0.3:
                self.conn.execute("VACUUM")
            return deleted

# --- Systemd Execution Engine ---
class SystemdExecutor:
    """Handles execution of tasks via systemd-run for resource isolation and management."""
    def __init__(self, store: Store):
        self.store = store

    def launch(self, task: Dict[str, Any]) -> bool:
        """Launch a task using systemd-run with resource controls."""
        unit = unit_name(f"{task['name']}-{task['id']}")
        props = [
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",
        ]

        # Apply resource controls (Integration from Program 2)
        if task.get('rtprio'):
            props.extend([
                "--property=CPUSchedulingPolicy=rr",
                f"--property=CPUSchedulingPriority={task['rtprio']}"
            ])
        if task.get('nice') is not None:
            props.append(f"--property=Nice={int(task['nice'])}")
        if task.get('slice'):
            props.append(f"--slice={task['slice']}")
        if task.get('cpu_weight'):
            props.append(f"--property=CPUWeight={int(task['cpu_weight'])}")
        if task.get('mem_max_mb'):
            props.append(f"--property=MemoryMax={int(task['mem_max_mb'])}M")
        if task.get('cwd'):
            props.append(f"--property=WorkingDirectory={task['cwd']}")

        # Add environment variables
        env = []
        if task.get('env'):
            for k, v in task['env'].items():
                env.extend(["--setenv", f"{k}={v}"])

        # Handle scheduling
        schedule_args = []
        if task.get('schedule'):
            schedule_args.extend(["--on-calendar", task['schedule']])

        # Build and execute the systemd-run command
        cmd_to_run = [task['cmd']] + (task.get('args') or [])
        run_cmd = [*SYSDRUN, "--unit", unit, *props, *env, *schedule_args, "--", *cmd_to_run]

        try:
            cp = subprocess.run(run_cmd, text=True, capture_output=True, timeout=10)
            if cp.returncode == 0:
                status = "sched" if task.get('schedule') else "start"
                self.store.update_unit(task['id'], unit, status)
                return True
            else:
                error_msg = f"Launch failed: {cp.stderr.strip()[:500]}"
                self.store.finalize(task['id'], False, error=error_msg)
                return False
        except subprocess.TimeoutExpired:
            self.store.finalize(task['id'], False, error="Systemd launch timeout")
            return False
        except Exception as e:
            self.store.finalize(task['id'], False, error=f"Systemd launch error: {str(e)}")
            return False

    def reconcile(self) -> int:
        """Reconcile running systemd units with database state."""
        with self.store.lock:
            rows = self.store.conn.execute(
                "SELECT id, unit FROM tasks WHERE s IN ('r', 'start', 'sched') AND unit IS NOT NULL"
            ).fetchall()

        if not rows:
            return 0

        # Query systemd for all relevant units in a single call
        unit_names = [row['unit'] for row in rows]
        cmd = [*SYSTEMCTL, "show", *unit_names, "--property=Id,ActiveState,Result"]
        try:
            cp = subprocess.run(cmd, text=True, capture_output=True, timeout=10)
            if cp.returncode != 0:
                print(f"Warning: Reconciliation failed: {cp.stderr}", file=sys.stderr)
                return 0

            reconciled_count = 0
            # Parse output (blocks separated by empty lines)
            for block in cp.stdout.strip().split('\n\n'):
                if not block:
                    continue
                props = {}
                for line in block.splitlines():
                    if '=' in line:
                        key, value = line.split('=', 1)
                        props[key] = value

                unit_id = props.get("Id")
                task_row = next((r for r in rows if r['unit'] == unit_id), None)
                if not task_row:
                    continue

                # Check if the unit has finished
                if props.get("ActiveState") in ("inactive", "failed"):
                    success = (props.get("Result") == "success")
                    error = None if success else f"Systemd Result={props.get('Result')}"
                    self.store.finalize(task_row['id'], success, error)
                    reconciled_count += 1

            return reconciled_count

        except Exception as e:
            print(f"Error during reconciliation: {e}", file=sys.stderr)
            return 0

# --- Worker Process ---
class Worker:
    """Background worker that pulls tasks from the queue and executes them."""
    def __init__(self, store: Store, executor: SystemdExecutor):
        self.store = store
        self.executor = executor
        self.running = True
        self.worker_id = f"w{os.getpid()}"
        self._setup_signal_handlers()

    def _setup_signal_handlers(self):
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)

    def _shutdown(self, signum, frame):
        print(f"Received signal {signum}, shutting down gracefully...")
        self.running = False

    def run(self):
        """Main worker loop."""
        print(f"Worker {self.worker_id} started.")
        reconcile_timer = time.time()

        while self.running:
            # Periodic systemd unit reconciliation
            if time.time() - reconcile_timer > 5:
                try:
                    count = self.executor.reconcile()
                    if count > 0:
                        print(f"Reconciled {count} tasks.")
                except Exception as e:
                    print(f"Error in reconciliation: {e}", file=sys.stderr)
                reconcile_timer = time.time()

            # Pull and execute a task
            task = self.store.pop(self.worker_id)
            if task:
                print(f"Launching task {task['id']}: {task['name']}")
                success = self.executor.launch(task)
                if not success:
                    # If systemd launch failed, mark it as a failure immediately
                    self.store.finalize(task['id'], False, error="Failed to launch via systemd")
            else:
                time.sleep(0.1)  # Sleep briefly if no tasks are available

        print("Worker shutdown complete.")

# --- Command Line Interface ---
def main():
    parser = argparse.ArgumentParser(description="Production Job Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Add Task
    add_parser = subparsers.add_parser("add", help="Add a new task")
    add_parser.add_argument("name", help="Unique task name")
    add_parser.add_argument("cmd", help="Command to execute")
    add_parser.add_argument("args", nargs="*", help="Command arguments")
    add_parser.add_argument("-p", "--priority", type=int, default=0, help="Task priority (higher = sooner)")
    add_parser.add_argument("--delay", type=int, default=0, help="Delay execution by N seconds")
    add_parser.add_argument("--deps", help="Comma-separated list of dependency task IDs")
    # Resource controls
    add_parser.add_argument("--nice", type=int, help="Nice value for CPU scheduling")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority (1-99)")
    add_parser.add_argument("--cpu-weight", type=int, help="CPU weight (1-10000)")
    add_parser.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add_parser.add_argument("--schedule", help="Systemd calendar format (e.g., 'daily', '*-*-* 12:00:00')")
    add_parser.add_argument("--env", action="append", help="Environment variables (KEY=VALUE)")
    add_parser.add_argument("--cwd", help="Working directory")

    # Run Worker
    run_parser = subparsers.add_parser("worker", help="Start the task worker/executor")

    # Management Commands
    subparsers.add_parser("stats", help="Show queue statistics")
    cleanup_parser = subparsers.add_parser("cleanup", help="Clean up old tasks")
    cleanup_parser.add_argument("--days", type=int, default=7, help="Days to keep completed tasks")

    args = parser.parse_args()
    store = Store()

    if args.command == "add":
        try:
            deps = [int(d) for d in args.deps.split(',')] if args.deps else None
        except ValueError:
            sys.exit("Error: Dependencies must be comma-separated integers.")

        config = {}
        if args.nice is not None: config['nice'] = args.nice
        if args.rtprio is not None: config['rtprio'] = args.rtprio
        if args.cpu_weight: config['cpu_weight'] = args.cpu_weight
        if args.mem_max_mb: config['mem_max_mb'] = args.mem_max_mb

        env_dict = {}
        if args.env:
            for e in args.env:
                if "=" in e:
                    k, v = e.split("=", 1)
                    env_dict[k] = v

        at = now_ms() + (args.delay * 1000)
        task_id = store.add(
            name=args.name,
            cmd=args.cmd,
            args=json.dumps(args.args),
            env=json.dumps(env_dict) if env_dict else None,
            cwd=args.cwd,
            p=args.priority,
            at=at,
            dep=json.dumps(deps) if deps else None,
            schedule=args.schedule,
            **config
        )
        print(f"Added Task ID: {task_id}")

    elif args.command == "worker":
        if not shutil.which("systemd-run"):
            sys.exit("Error: 'systemd-run' not found. This orchestrator requires systemd.")
        executor = SystemdExecutor(store)
        worker = Worker(store, executor)
        worker.run()

    elif args.command == "stats":
        print(json.dumps(store.stats(), indent=2))

    elif args.command == "cleanup":
        deleted = store.cleanup(args.days)
        print(f"Cleaned up {deleted} old tasks")

if __name__ == "__main__":
    import shutil
    main()

================================================================================
