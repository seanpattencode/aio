================================================================================
CONSOLIDATED PYTHON SOLUTIONS
================================================================================

TABLE OF CONTENTS:
----------------------------------------
1. chatgpt2.py
2. claude2.py
3. claudeCore.py
4. deepseekCore.py
5. minimal_aiose.py
6. qwen2.py

================================================================================

================================================================================
FILE: chatgpt2.py
================================================================================

#!/usr/bin/env python3
"""
aiose — All-in-One Scheduler & Executor  (<500 lines)

Highlights (taken & improved):
- SQLite WAL + tuned pragmas (claude/chatgpt/gemini*)
- One table for everything; JSON deps/properties; metrics (chatgpt/claude)
- Atomic pop with json_each dep check + RETURNING fallback (chatgpt)
- Local subprocess runner + optional in-process Python function jobs ("py:module.func")
  (claude’s registry idea, simplified)
- systemd-run transient units with resource controls, OnCalendar or on-active delay, Persistent=true
  (aios_systemd/deepseek/grok/gemini*)
- Batch worker w/ reclaim of stalled, exponential backoff, stdout/stderr capture (chatgpt/glm)
- Efficient reconciliation of MANY units at once (geminiDeep)
- CLI: add | worker | start | stop | status | list | stats | cleanup | reconcile | install
"""
from __future__ import annotations
import argparse, json, os, shlex, signal, sqlite3, subprocess, sys, time, threading, importlib
from pathlib import Path
from typing import Optional, Dict, Any, List

# ---------- constants ----------
DB_PATH = Path.home() / ".aiose.db"
UNIT_PREFIX = "aiose-"
USE_USER = (os.geteuid() != 0)
SYSTEMCTL = ["systemctl", "--user"] if USE_USER else ["systemctl"]
SYSDRUN   = ["systemd-run", "--user", "--collect", "--quiet"] if USE_USER else ["systemd-run","--collect","--quiet"]
DEFAULT_TIMEOUT = 300
DEFAULT_RETRIES = 3

PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

# ---------- helpers ----------
def now_ms() -> int: return int(time.time()*1000)
def unit_name(ident: str|int) -> str:
    s = "".join(c if str(c).isalnum() or c in "._-:" else "_" for c in str(ident))
    return f"{UNIT_PREFIX}{s}.service"
def run(cmd: List[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

# ---------- storage ----------
class Store:
    """SQLite store + queue operations (thread-safe)."""
    def __init__(self, path=DB_PATH):
        path.parent.mkdir(parents=True, exist_ok=True)
        self.c = sqlite3.connect(path, isolation_level=None, check_same_thread=False)
        self.c.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        for p in PRAGMAS: self.c.execute(p)
        self.c.executescript("""
        CREATE TABLE IF NOT EXISTS tasks(
          id INTEGER PRIMARY KEY,
          name TEXT UNIQUE,                  -- optional human handle
          cmd  TEXT NOT NULL,                -- 'shell string' OR 'py:module.func'
          mode TEXT DEFAULT 'local',         -- 'local' | 'systemd' | 'py'
          args TEXT,                         -- JSON array of args (local/systemd join into shell)
          env  TEXT,                         -- JSON map
          cwd  TEXT,                         -- working dir
          p INT DEFAULT 0,                   -- priority
          s TEXT DEFAULT 'q',                -- q=queued r=running d=done f=failed sched=startable/start
          at INT DEFAULT 0,                  -- scheduled_at (ms) for local/py
          schedule TEXT,                     -- systemd OnCalendar (cron-like)
          w  TEXT,                           -- worker_id (local/py)
          r  INT DEFAULT 0,                  -- retry_count
          maxr INT DEFAULT 3,                -- max_retries
          timeout INT DEFAULT 300,           -- seconds (local/py)
          e TEXT,                            -- error message
          res TEXT,                          -- result (stdout/stderr json)
          ct INT DEFAULT (strftime('%s','now')*1000),
          st INT, et INT,                    -- start/end times
          dep TEXT,                          -- JSON array of dependency IDs
          -- Systemd props
          unit TEXT, rtprio INT, nice INT, slice TEXT,
          cpu_weight INT, mem_max_mb INT
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(s,p DESC,at,id) WHERE s IN ('q','r');
        CREATE TABLE IF NOT EXISTS metrics(task_id INTEGER PRIMARY KEY, qt REAL, et REAL,
          FOREIGN KEY(task_id) REFERENCES tasks(id) ON DELETE CASCADE);
        """)

    # CRUD
    def add(self, **kw) -> int:
        with self.lock:
            cols=",".join(kw.keys()); qs=",".join("?" for _ in kw)
            return self.c.execute(f"INSERT INTO tasks({cols}) VALUES({qs})", tuple(kw.values())).lastrowid
    def get_by_id(self, tid:int) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE id=?", (tid,)).fetchone()
    def get_by_name(self, name:str) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE name=?", (name,)).fetchone()
    def list(self) -> List[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks ORDER BY ct DESC").fetchall()
    def update(self, tid:int, **kw):
        if not kw: return
        with self.lock:
            sets=",".join(f"{k}=?" for k in kw)
            self.c.execute(f"UPDATE tasks SET {sets} WHERE id=?", (*kw.values(), tid))

    # Queue ops
    def pop(self, worker_id:str) -> Optional[Dict[str,Any]]:
        """Pop next eligible local/py task with dep check."""
        with self.lock:
            row = self.c.execute("""
              SELECT id, cmd, mode, args, env, cwd, timeout FROM tasks
              WHERE s='q' AND at<=? AND mode IN ('local','py')
                AND (dep IS NULL OR NOT EXISTS(
                  SELECT 1 FROM json_each(tasks.dep) AS d
                  JOIN tasks t2 ON t2.id=d.value WHERE t2.s!='d'))
              ORDER BY p DESC, at, id LIMIT 1
            """, (now_ms(),)).fetchone()
            if not row: return None
            try:
                claimed = self.c.execute(
                    "UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q' RETURNING id, cmd, mode, args, env, cwd, timeout",
                    (worker_id, now_ms(), row["id"])
                ).fetchone()
                if claimed: return dict(claimed)
            except sqlite3.OperationalError:
                self.c.execute("BEGIN IMMEDIATE")
                n = self.c.execute("UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q'",
                                   (worker_id, now_ms(), row["id"])).rowcount
                self.c.execute("COMMIT")
                if n:
                    return dict(row)
        return None

    def done(self, tid:int, ok:bool, result:Any=None, error:str=None, worker_id:str=""):
        with self.lock:
            t = self.c.execute("SELECT ct,st,r,maxr FROM tasks WHERE id=? AND (w=? OR w IS NULL)", (tid, worker_id)).fetchone()
            if not t: return
            tnow = now_ms()
            if ok:
                self.c.execute("UPDATE tasks SET s='d', et=?, res=?, w=NULL WHERE id=?",
                               (tnow, json.dumps(result)[:2000] if result else None, tid))
                if t["st"]:
                    qt = (t["st"]-t["ct"])/1000.0; et=(tnow - t["st"])/1000.0
                    self.c.execute("INSERT OR REPLACE INTO metrics(task_id,qt,et) VALUES(?,?,?)", (tid,qt,et))
            else:
                rc = t["r"] or 0; mr = t["maxr"] or DEFAULT_RETRIES
                if rc < mr:
                    delay = 1000*(2**rc)
                    self.c.execute("UPDATE tasks SET s='q', at=?, r=r+1, e=?, w=NULL WHERE id=?",
                                   (tnow+delay, (error or "")[:500], tid))
                else:
                    self.c.execute("UPDATE tasks SET s='f', et=?, e=?, w=NULL WHERE id=?", (tnow, (error or "")[:500], tid))

    def reclaim(self, timeout_ms:int=300000) -> int:
        with self.lock:
            cutoff = now_ms() - timeout_ms
            return self.c.execute("UPDATE tasks SET s='q', w=NULL, r=r+1 WHERE s='r' AND st<?", (cutoff,)).rowcount

    def running_units(self) -> Dict[str,int]:
        with self.lock:
            rows = self.c.execute("SELECT id,unit FROM tasks WHERE s IN ('start','sched','r') AND unit IS NOT NULL").fetchall()
            return {row["unit"]: row["id"] for row in rows}

    def stats(self) -> Dict[str,Any]:
        with self.lock:
            counts = {r["s"]: r["c"] for r in self.c.execute("SELECT s,COUNT(*) c FROM tasks GROUP BY s")}
            perf = self.c.execute("SELECT AVG(qt) avg_qt, AVG(et) avg_et, MAX(qt) max_qt, MAX(et) max_et FROM metrics").fetchone()
            return {"tasks": counts, "perf": dict(perf) if perf else {}}

    def cleanup(self, days:int=7) -> int:
        cutoff = now_ms() - days*86400000
        with self.lock:
            n = self.c.execute("DELETE FROM tasks WHERE s IN ('d','f') AND et<?", (cutoff,)).rowcount
            pc = self.c.execute("PRAGMA page_count").fetchone()[0]
            fr = self.c.execute("PRAGMA freelist_count").fetchone()[0]
            if fr > pc*0.3: self.c.execute("VACUUM")
            return n

# ---------- systemd orchestration ----------
def systemd_start(db:Store, task:sqlite3.Row) -> tuple[bool,str,str]:
    """Start via systemd-run; support OnCalendar & on-active delay; set Persistent=true."""
    unit = unit_name(task["name"] or task["id"])
    props = ["--property=StandardOutput=journal",
             "--property=StandardError=journal",
             "--property=KillMode=control-group",
             "--property=TimeoutSec={}".format(task["timeout"] or DEFAULT_TIMEOUT),
             "--property=Persistent=true"]
    if task["cwd"]:  props += [f"--property=WorkingDirectory={task['cwd']}"]
    if task["nice"] is not None: props += [f"--property=Nice={int(task['nice'])}"]
    if task["rtprio"] is not None:
        props += ["--property=CPUSchedulingPolicy=rr",
                  f"--property=CPUSchedulingPriority={int(task['rtprio'])}"]
    if task["slice"]: props += [f"--slice={task['slice']}"]
    if task["cpu_weight"]: props += [f"--property=CPUWeight={int(task['cpu_weight'])}"]
    if task["mem_max_mb"]: props += [f"--property=MemoryMax={int(task['mem_max_mb'])}M"]

    env=[]
    if task["env"]:
        for k,v in json.loads(task["env"]).items():
            env += ["--setenv", f"{k}={v}"]

    when=[]
    if task["schedule"]:
        when += ["--on-calendar", task["schedule"]]
    else:
        # if future 'at', translate to on-active delay
        delay_ms = max(0, (task["at"] or now_ms()) - now_ms())
        if delay_ms>0: when += ["--on-active", f"{int((delay_ms+999)/1000)}s"]

    # Build shell command: join cmd + args (if any)
    args = json.loads(task["args"] or "[]")
    shell = task["cmd"] if not args else task["cmd"] + " " + " ".join(shlex.quote(a) for a in args)
    cp = run([*SYSDRUN, "--unit", unit, *props, *env, *when, "--", "/bin/sh", "-lc", shell])
    db.update(task["id"], unit=unit, s=("sched" if when else "start"))
    msg = cp.stderr.strip() or cp.stdout.strip()
    return (cp.returncode==0, unit, msg)

def systemd_stop(db:Store, name_or_id:str):
    unit = unit_name(name_or_id)
    run(SYSTEMCTL + ["stop", unit])
    run(SYSTEMCTL + ["stop", unit.replace(".service",".timer")])
    # status reconciled later

def systemd_status(name_or_id:str) -> Dict[str,str]:
    unit = unit_name(name_or_id)
    cp = run(SYSTEMCTL + ["show", unit, "--property=ActiveState", "--property=Result", "--property=MainPID"])
    if cp.returncode!=0: return {"unit": unit, "active":"unknown"}
    m = dict(line.split("=",1) for line in cp.stdout.splitlines() if "=" in line)
    return {"unit": unit, "active": m.get("ActiveState"), "result": m.get("Result"), "pid": m.get("MainPID")}

def reconcile_systemd(db:Store) -> int:
    """Batch-check many units efficiently; mark tasks done/failed."""
    umap = db.running_units()
    if not umap: return 0
    cp = run([*SYSTEMCTL, "show", *umap.keys(), "--property=Id,ActiveState,Result"])
    if cp.returncode!=0: return 0
    count=0
    for block in cp.stdout.strip().split("\n\n"):
        props = dict(line.split("=",1) for line in block.splitlines() if "=" in line)
        unit = props.get("Id"); tid = umap.get(unit); 
        if not tid: continue
        active = props.get("ActiveState"); result = props.get("Result")
        if active in ("inactive","failed"):
            db.done(tid, ok=(result=="success"))
            count += 1
        elif active=="active":
            # ensure state marked running
            db.update(tid, s='r')
    return count

# ---------- local/py worker ----------
class Worker:
    def __init__(self, store:Store, wid:Optional[str]=None):
        self.db=store
        self.wid=wid or f"w{os.getpid()}"
        self.run_flag=True
        signal.signal(signal.SIGTERM, self._stop)
        signal.signal(signal.SIGINT,  self._stop)
    def _stop(self,*_): self.run_flag=False

    def _exec_py(self, cmd:str, args:List[str], timeout:int) -> tuple[bool,Dict[str,str]|None,str|None]:
        """Execute 'py:module.func' in-process (simple & fast)."""
        try:
            assert cmd.startswith("py:"), "not py job"
            mod_fn = cmd[3:]
            mod, fn = mod_fn.rsplit(".",1)
            module = importlib.import_module(mod)
            func = getattr(module, fn)
            # crude timeout via alarm (POSIX); fallback to no-timeout if not available
            ok=True; out=None; err=None
            if hasattr(signal, "SIGALRM"):
                def _raise(*_): raise TimeoutError("PY_TIMEOUT")
                signal.signal(signal.SIGALRM, _raise); signal.alarm(max(1, int(timeout)))
                result = func(*args) if args else func()
                signal.alarm(0)
            else:
                result = func(*args) if args else func()
            out = {"stdout": str(result)[:1000], "stderr": ""}
            return True, out, None
        except TimeoutError:
            return False, None, "TIMEOUT"
        except Exception as e:
            return False, None, repr(e)

    def loop(self, batch:int=1, idle_ms:int=50):
        print(f"Worker {self.wid} running (batch={batch})")
        tick=0
        while self.run_flag:
            tick += 1
            if tick%100==0:
                r=self.db.reclaim()
                if r: print(f"reclaimed {r} stalled")
                # also sync with systemd
                try: 
                    n=reconcile_systemd(self.db)
                    if n: print(f"reconciled {n} systemd tasks")
                except Exception: pass
            got=[]
            for _ in range(batch):
                item=self.db.pop(self.wid)
                if item: got.append(item)
            if not got:
                time.sleep(idle_ms/1000.0); continue
            for t in got:
                if not self.run_flag: break
                mode=t["mode"]; args=json.loads(t["args"] or "[]"); env=json.loads(t["env"] or "{}")
                try:
                    if mode=="py" and t["cmd"].startswith("py:"):
                        ok,res,err=self._exec_py(t["cmd"], args, int(t["timeout"] or DEFAULT_TIMEOUT))
                        self.db.done(t["id"], ok, res, err, self.wid); continue
                    # local subprocess
                    shell = t["cmd"] if not args else t["cmd"] + " " + " ".join(shlex.quote(a) for a in args)
                    proc=subprocess.run(shell, shell=True, capture_output=True, text=True,
                                        timeout=int(t["timeout"] or DEFAULT_TIMEOUT),
                                        cwd=(t["cwd"] or None),
                                        env={**os.environ, **env})
                    self.db.done(t["id"], proc.returncode==0,
                                 {"stdout": proc.stdout[:1000], "stderr": proc.stderr[:1000]},
                                 proc.stderr if proc.returncode!=0 else None, self.wid)
                except subprocess.TimeoutExpired:
                    self.db.done(t["id"], False, error="TIMEOUT", worker_id=self.wid)
                except Exception as e:
                    self.db.done(t["id"], False, error=str(e), worker_id=self.wid)

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(description="aiose — All-in-One Scheduler & Executor")
    sub = ap.add_subparsers(dest="cmd", required=True)

    # add
    pa = sub.add_parser("add", help="add task")
    pa.add_argument("cmd", help="shell string OR py:module.func")
    pa.add_argument("args", nargs="*", help="args (joined for shell; passed to py func)")
    pa.add_argument("--name")
    pa.add_argument("--mode", choices=["local","systemd","py"], default="local")
    pa.add_argument("--priority", type=int, default=0)
    pa.add_argument("--delay-ms", type=int, default=0)
    pa.add_argument("--deps", help="JSON list of task IDs")
    pa.add_argument("--env", action="append", default=[], help="KEY=VAL")
    pa.add_argument("--cwd")
    pa.add_argument("--on-calendar", help="systemd OnCalendar expression")
    pa.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    pa.add_argument("--retries", type=int, default=DEFAULT_RETRIES)
    # systemd props
    pa.add_argument("--rtprio", type=int); pa.add_argument("--nice", type=int)
    pa.add_argument("--slice"); pa.add_argument("--cpu-weight", type=int); pa.add_argument("--mem-max-mb", type=int)
    pa.add_argument("--start", action="store_true", help="start immediately (systemd)")

    # worker
    pw = sub.add_parser("worker", help="run local/py worker")
    pw.add_argument("--batch", type=int, default=1); pw.add_argument("--idle-ms", type=int, default=50)

    # systemd management
    ps = sub.add_parser("start", help="start a systemd task by id/name")
    ps.add_argument("name_or_id")
    pk = sub.add_parser("stop", help="stop systemd task by id/name"); pk.add_argument("name_or_id")
    pz = sub.add_parser("status", help="status for id/name");        pz.add_argument("name_or_id")

    # ops
    sub.add_parser("list", help="list tasks")
    sub.add_parser("stats", help="show stats")
    pc = sub.add_parser("cleanup", help="delete old done/failed"); pc.add_argument("--days", type=int, default=7)
    sub.add_parser("reconcile", help="reconcile systemd units")
    sub.add_parser("install", help="print a systemd user service for aiose worker")

    args = ap.parse_args()
    db = Store()

    if args.cmd=="add":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        at  = now_ms() + max(0, args.delay_ms)
        dep = json.dumps(json.loads(args.deps)) if args.deps else None
        tid = db.add(
            name=args.name, cmd=args.cmd, mode=args.mode, args=json.dumps(args.args) if args.args else None,
            env=(json.dumps(env) if env else None), cwd=args.cwd, p=args.priority, s="q",
            at=at, schedule=args.on_calendar, dep=dep, timeout=args.timeout, maxr=args.retries,
            rtprio=args.rtprio, nice=args.nice, slice=args.slice, cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb
        )
        if args.mode=="systemd" and (args.start or args.on_calendar or args.delay_ms>0):
            row = db.get_by_id(tid)
            ok, unit, msg = systemd_start(db, row)
            print(unit if ok else f"ERROR: {msg}")
        else:
            print(f"queued task {tid}")

    elif args.cmd=="worker":
        Worker(db).loop(batch=args.batch, idle_ms=args.idle_ms)

    elif args.cmd=="start":
        key = args.name_or_id
        row = db.get_by_id(int(key)) if key.isdigit() else db.get_by_name(key)
        if not row: sys.exit("unknown task")
        if row["mode"]!="systemd": sys.exit("task is not systemd mode")
        ok, unit, msg = systemd_start(db, row)
        print(unit if ok else f"ERROR: {msg}")

    elif args.cmd=="stop":
        systemd_stop(db, args.name_or_id); print("stopped")

    elif args.cmd=="status":
        print(json.dumps(systemd_status(args.name_or_id), indent=2))

    elif args.cmd=="list":
        try: reconcile_systemd(db)
        except Exception: pass
        for r in db.list():
            ident=r["name"] or r["id"]; sched=r["schedule"] or "-"
            unit=r["unit"] or "-"; mode=r["mode"]; s=r["s"]
            print(f"{ident:20} id={r['id']:4} mode={mode:7} s={s:5} p={r['p']:2} at={r['at']} unit={unit} sched={sched}")

    elif args.cmd=="stats":
        print(json.dumps(db.stats(), indent=2))

    elif args.cmd=="cleanup":
        print(f"deleted {db.cleanup(args.days)}")

    elif args.cmd=="reconcile":
        n=reconcile_systemd(db); print(f"ok (updated {n})")

    elif args.cmd=="install":
        exe = sys.executable; here=os.path.abspath(__file__)
        print(f"""[Unit]
Description=aiose worker
After=default.target

[Service]
Type=simple
ExecStart={exe} {here} worker
Restart=always
RestartSec=3
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=default.target
""")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: claude2.py
================================================================================

#!/usr/bin/env python3
"""
Pure Python Job Orchestrator - Zero subprocess calls
Combines the best patterns from all implementations with multiprocessing execution
"""
import sqlite3, json, time, sys, os, signal, threading, traceback, pickle
from multiprocessing import Process, Queue, Manager, cpu_count
from typing import Optional, Dict, Any, List, Callable
from pathlib import Path
import importlib.util

# Configuration
DB_PATH = Path.home() / ".pyorch.db"

# Optimized SQLite pragmas (best from all implementations)
PRAGMAS = """
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA cache_size=-8000;
PRAGMA temp_store=MEMORY;
PRAGMA mmap_size=268435456;
PRAGMA busy_timeout=5000;
PRAGMA wal_autocheckpoint=1000;
"""

class JobRegistry:
    """Registry for Python callable jobs"""
    _jobs = {}
    
    @classmethod
    def register(cls, name: str, func: Callable, module: str = None):
        """Register a Python function as a job"""
        cls._jobs[name] = {'func': func, 'module': module}
    
    @classmethod
    def get(cls, name: str) -> Optional[Callable]:
        """Get a registered function"""
        job = cls._jobs.get(name)
        return job['func'] if job else None
    
    @classmethod
    def load_module(cls, path: str):
        """Load jobs from a Python module"""
        spec = importlib.util.spec_from_file_location("jobs", path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        for name in dir(module):
            obj = getattr(module, name)
            if callable(obj) and not name.startswith('_'):
                cls.register(name, obj, path)
        return list(cls._jobs.keys())

class Orchestrator:
    """High-performance job orchestrator with multiprocessing"""
    
    def __init__(self, db_path: Path = DB_PATH, max_workers: int = None):
        self.db_path = db_path
        self.max_workers = max_workers or cpu_count()
        self.processes = {}
        self.manager = Manager()
        self.result_queue = self.manager.Queue()
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Initialize database with best schema patterns"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            conn.executescript(PRAGMAS + """
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE,
                    func TEXT NOT NULL,
                    args TEXT,
                    kwargs TEXT,
                    
                    -- Scheduling
                    priority INT DEFAULT 0,
                    scheduled_at INT DEFAULT 0,
                    schedule TEXT,  -- cron-like pattern
                    
                    -- State
                    status TEXT DEFAULT 'pending' CHECK(status IN 
                        ('pending','running','completed','failed','cancelled')),
                    pid INT,
                    worker TEXT,
                    
                    -- Execution
                    retry_count INT DEFAULT 0,
                    max_retries INT DEFAULT 3,
                    timeout_sec INT DEFAULT 300,
                    
                    -- Results
                    error TEXT,
                    result TEXT,
                    traceback TEXT,
                    
                    -- Dependencies
                    depends_on TEXT,  -- JSON array of job IDs
                    
                    -- Resource limits
                    nice INT,
                    cpu_affinity TEXT,  -- JSON array of CPU indices
                    memory_limit_mb INT,
                    
                    -- Timestamps
                    created_at INT DEFAULT (strftime('%s','now')*1000),
                    started_at INT,
                    completed_at INT
                );
                
                -- Optimized composite index
                CREATE INDEX IF NOT EXISTS idx_queue ON jobs(
                    status, priority DESC, scheduled_at, id
                ) WHERE status IN ('pending','running');
                
                CREATE INDEX IF NOT EXISTS idx_deps ON jobs(depends_on) 
                    WHERE depends_on IS NOT NULL;
                
                -- Metrics table
                CREATE TABLE IF NOT EXISTS metrics (
                    job_id INTEGER PRIMARY KEY,
                    queue_time REAL,
                    exec_time REAL,
                    cpu_time REAL,
                    memory_peak_mb REAL,
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
                
                -- Artifacts table for job outputs
                CREATE TABLE IF NOT EXISTS artifacts (
                    id INTEGER PRIMARY KEY,
                    job_id INTEGER,
                    key TEXT,
                    value BLOB,
                    created_at INT DEFAULT (strftime('%s','now')*1000),
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
            """)
    
    def add(self, name: str, func: str, args: tuple = None, kwargs: dict = None,
            priority: int = 0, scheduled_at: int = None, depends_on: List[int] = None,
            max_retries: int = 3, timeout_sec: int = 300, schedule: str = None,
            nice: int = None, cpu_affinity: List[int] = None, 
            memory_limit_mb: int = None) -> int:
        """Add a job with comprehensive options"""
        now = int(time.time() * 1000)
        scheduled_at = scheduled_at or now
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            cursor = conn.execute("""
                INSERT OR REPLACE INTO jobs 
                (name, func, args, kwargs, priority, scheduled_at, schedule,
                 depends_on, max_retries, timeout_sec, nice, cpu_affinity, memory_limit_mb)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                name, func,
                json.dumps(args) if args else None,
                json.dumps(kwargs) if kwargs else None,
                priority, scheduled_at, schedule,
                json.dumps(depends_on) if depends_on else None,
                max_retries, timeout_sec, nice,
                json.dumps(cpu_affinity) if cpu_affinity else None,
                memory_limit_mb
            ))
            return cursor.lastrowid
    
    def get_next(self) -> Optional[Dict[str, Any]]:
        """Atomically get next eligible job with dependency checking"""
        now = int(time.time() * 1000)
        
        with self.lock:
            conn = sqlite3.connect(self.db_path, isolation_level=None)
            conn.row_factory = sqlite3.Row
            
            try:
                # Atomic claim with RETURNING (most efficient)
                result = conn.execute("""
                    UPDATE jobs SET status='running', started_at=?, pid=?
                    WHERE id = (
                        SELECT id FROM jobs
                        WHERE status='pending' AND scheduled_at <= ?
                        AND (depends_on IS NULL OR NOT EXISTS (
                            SELECT 1 FROM json_each(jobs.depends_on) AS d
                            JOIN jobs AS dj ON dj.id = d.value
                            WHERE dj.status != 'completed'
                        ))
                        ORDER BY priority DESC, scheduled_at, id
                        LIMIT 1
                    )
                    RETURNING *
                """, (now, os.getpid(), now)).fetchone()
                
                if result:
                    return dict(result)
                    
            except sqlite3.OperationalError:
                # Fallback for older SQLite
                row = conn.execute("""
                    SELECT id FROM jobs
                    WHERE status='pending' AND scheduled_at <= ?
                    AND (depends_on IS NULL OR NOT EXISTS (
                        SELECT 1 FROM json_each(jobs.depends_on) AS d
                        JOIN jobs AS dj ON dj.id = d.value
                        WHERE dj.status != 'completed'
                    ))
                    ORDER BY priority DESC, scheduled_at, id
                    LIMIT 1
                """, (now,)).fetchone()
                
                if row:
                    conn.execute(
                        "UPDATE jobs SET status='running', started_at=?, pid=? WHERE id=? AND status='pending'",
                        (now, os.getpid(), row['id'])
                    )
                    return dict(conn.execute("SELECT * FROM jobs WHERE id=?", (row['id'],)).fetchone())
            finally:
                conn.close()
        
        return None
    
    def execute(self, job: Dict) -> bool:
        """Execute job in separate process with resource limits"""
        job_id = job['id']
        
        # Parse job data
        args = json.loads(job['args']) if job['args'] else ()
        kwargs = json.loads(job['kwargs']) if job['kwargs'] else {}
        
        # Create worker process
        p = Process(
            target=self._worker_process,
            args=(job_id, job['func'], args, kwargs, 
                  self.result_queue, job.get('timeout_sec', 300),
                  job.get('nice'), job.get('cpu_affinity'))
        )
        
        # Start process
        p.start()
        self.processes[job_id] = p
        
        # Update job with process ID
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute("UPDATE jobs SET pid=? WHERE id=?", (p.pid, job_id))
        
        return True
    
    def _worker_process(self, job_id: int, func_name: str, args: tuple,
                       kwargs: dict, result_queue: Queue, timeout: int,
                       nice: Optional[int], cpu_affinity: Optional[str]):
        """Worker process that executes a job"""
        try:
            # Set process nice value
            if nice is not None:
                os.nice(nice)
            
            # Set CPU affinity if specified
            if cpu_affinity:
                try:
                    import psutil
                    p = psutil.Process()
                    p.cpu_affinity(json.loads(cpu_affinity))
                except ImportError:
                    pass  # psutil not available
            
            # Setup timeout
            def timeout_handler(signum, frame):
                raise TimeoutError(f"Job exceeded {timeout}s timeout")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout)
            
            # Get and execute function
            func = JobRegistry.get(func_name)
            if not func:
                # Try to import dynamically
                if '.' in func_name:
                    module_name, func_name = func_name.rsplit('.', 1)
                    module = importlib.import_module(module_name)
                    func = getattr(module, func_name)
                else:
                    raise ValueError(f"Function {func_name} not found")
            
            # Execute
            result = func(*args, **kwargs) if args or kwargs else func()
            
            signal.alarm(0)  # Cancel timeout
            result_queue.put((job_id, True, result, None))
            
        except Exception as e:
            signal.alarm(0)
            result_queue.put((job_id, False, None, {
                'error': str(e),
                'traceback': traceback.format_exc()
            }))
    
    def complete(self, job_id: int, success: bool, result: Any = None,
                error_info: Dict = None):
        """Complete job with retry logic and metrics"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            if success:
                # Success - mark completed and record metrics
                conn.execute("""
                    UPDATE jobs 
                    SET status='completed', completed_at=?, result=?, pid=NULL
                    WHERE id=?
                """, (now, json.dumps(result) if result else None, job_id))
                
                # Calculate and store metrics
                job = conn.execute(
                    "SELECT created_at, started_at FROM jobs WHERE id=?", 
                    (job_id,)
                ).fetchone()
                
                if job and job['started_at']:
                    queue_time = (job['started_at'] - job['created_at']) / 1000.0
                    exec_time = (now - job['started_at']) / 1000.0
                    
                    conn.execute("""
                        INSERT OR REPLACE INTO metrics 
                        (job_id, queue_time, exec_time)
                        VALUES (?, ?, ?)
                    """, (job_id, queue_time, exec_time))
            else:
                # Failure - check retry logic
                job = conn.execute(
                    "SELECT retry_count, max_retries FROM jobs WHERE id=?",
                    (job_id,)
                ).fetchone()
                
                if job and job['retry_count'] < job['max_retries']:
                    # Exponential backoff: 1s, 2s, 4s, 8s...
                    delay = 1000 * (2 ** job['retry_count'])
                    conn.execute("""
                        UPDATE jobs 
                        SET status='pending', scheduled_at=?, retry_count=retry_count+1,
                            error=?, traceback=?, pid=NULL
                        WHERE id=?
                    """, (now + delay, error_info.get('error'),
                         error_info.get('traceback'), job_id))
                else:
                    # Final failure
                    conn.execute("""
                        UPDATE jobs 
                        SET status='failed', completed_at=?, error=?, traceback=?, pid=NULL
                        WHERE id=?
                    """, (now, error_info.get('error'),
                         error_info.get('traceback'), job_id))
        
        # Clean up process reference
        if job_id in self.processes:
            del self.processes[job_id]
    
    def process_results(self):
        """Process completed job results from queue"""
        processed = 0
        while not self.result_queue.empty():
            try:
                job_id, success, result, error = self.result_queue.get_nowait()
                self.complete(job_id, success, result, error)
                processed += 1
            except:
                break
        return processed
    
    def reclaim_stalled(self, timeout_ms: int = 300000) -> int:
        """Reclaim stalled jobs and terminate stuck processes"""
        cutoff = int(time.time() * 1000) - timeout_ms
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            stalled = conn.execute("""
                SELECT id, pid FROM jobs 
                WHERE status='running' AND started_at < ?
            """, (cutoff,)).fetchall()
            
            for job in stalled:
                # Terminate process if still running
                if job['id'] in self.processes:
                    self.processes[job['id']].terminate()
                    del self.processes[job['id']]
                
                # Reset job for retry
                conn.execute("""
                    UPDATE jobs 
                    SET status='pending', pid=NULL, retry_count=retry_count+1
                    WHERE id=?
                """, (job['id'],))
        
        return len(stalled)
    
    def stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            # Job counts by status
            counts = {row['status']: row['c'] for row in conn.execute(
                "SELECT status, COUNT(*) c FROM jobs GROUP BY status"
            )}
            
            # Performance metrics
            perf = conn.execute("""
                SELECT AVG(queue_time) avg_qt, AVG(exec_time) avg_et,
                       MAX(queue_time) max_qt, MAX(exec_time) max_et,
                       MIN(queue_time) min_qt, MIN(exec_time) min_et,
                       COUNT(*) total
                FROM metrics
            """).fetchone()
            
            # Recent failures
            failures = conn.execute("""
                SELECT name, error FROM jobs 
                WHERE status='failed' 
                ORDER BY completed_at DESC LIMIT 5
            """).fetchall()
            
            return {
                'jobs': counts,
                'performance': dict(perf) if perf else {},
                'active_processes': len(self.processes),
                'max_workers': self.max_workers,
                'recent_failures': [dict(f) for f in failures]
            }
    
    def save_artifact(self, job_id: int, key: str, value: Any):
        """Save job output artifact"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute(
                "INSERT INTO artifacts (job_id, key, value) VALUES (?, ?, ?)",
                (job_id, key, pickle.dumps(value))
            )
    
    def get_artifact(self, job_id: int, key: str) -> Any:
        """Retrieve job output artifact"""
        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                "SELECT value FROM artifacts WHERE job_id=? AND key=?",
                (job_id, key)
            ).fetchone()
            return pickle.loads(row['value']) if row else None
    
    def cleanup(self, days: int = 7) -> int:
        """Clean old completed jobs and vacuum database"""
        cutoff = int(time.time() * 1000) - (days * 86400000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            deleted = conn.execute("""
                DELETE FROM jobs 
                WHERE status IN ('completed', 'failed') AND completed_at < ?
            """, (cutoff,)).rowcount
            
            # Vacuum if significantly fragmented
            page_count = conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = conn.execute("PRAGMA freelist_count").fetchone()[0]
            
            if freelist > page_count * 0.3:
                conn.execute("VACUUM")
        
        return deleted

class Scheduler:
    """Main scheduler with worker pool management"""
    
    def __init__(self, orchestrator: Orchestrator, batch_size: int = 1):
        self.orch = orchestrator
        self.batch_size = batch_size
        self.running = True
        
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)
    
    def _shutdown(self, *_):
        self.running = False
        print("\nScheduler shutting down...")
        
        # Terminate all processes
        for p in self.orch.processes.values():
            p.terminate()
            p.join(timeout=1)
    
    def run(self):
        """Main scheduler loop with intelligent job management"""
        print(f"Scheduler started (workers={self.orch.max_workers}, batch={self.batch_size})")
        maintenance_counter = 0
        
        while self.running:
            maintenance_counter += 1
            
            # Process completed jobs
            processed = self.orch.process_results()
            if processed:
                print(f"Processed {processed} job results")
            
            # Periodic maintenance
            if maintenance_counter % 50 == 0:
                reclaimed = self.orch.reclaim_stalled()
                if reclaimed:
                    print(f"Reclaimed {reclaimed} stalled jobs")
            
            # Clean up finished processes
            for job_id, process in list(self.orch.processes.items()):
                if not process.is_alive():
                    process.join(timeout=0.1)
                    if job_id in self.orch.processes:
                        del self.orch.processes[job_id]
            
            # Start new jobs if capacity available
            while len(self.orch.processes) < self.orch.max_workers:
                job = self.orch.get_next()
                
                if not job:
                    break
                
                print(f"Starting job {job['id']}: {job['name']} ({job['func']})")
                self.orch.execute(job)
            
            time.sleep(0.1)
        
        print("Scheduler stopped")

# Built-in job functions
def example_add(x: int, y: int) -> int:
    """Example job that adds two numbers"""
    time.sleep(1)
    return x + y

def example_fail():
    """Example job that always fails"""
    raise ValueError("This job always fails")

def example_long(duration: int = 10) -> str:
    """Example long-running job"""
    time.sleep(duration)
    return f"Completed after {duration} seconds"

def example_data_processor(input_file: str, output_file: str) -> Dict:
    """Example data processing job"""
    # Simulate processing
    time.sleep(2)
    return {"processed": True, "records": 1000, "output": output_file}

# Register built-in jobs
JobRegistry.register("add", example_add)
JobRegistry.register("fail", example_fail)
JobRegistry.register("long", example_long)
JobRegistry.register("process", example_data_processor)

def main():
    """CLI interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pure Python Job Orchestrator")
    sub = parser.add_subparsers(dest="cmd", required=True)
    
    # Add command
    add_p = sub.add_parser("add", help="Add a job")
    add_p.add_argument("name", help="Job name")
    add_p.add_argument("func", help="Function name")
    add_p.add_argument("--args", nargs="+", help="Arguments")
    add_p.add_argument("--kwargs", type=json.loads, help="Keyword arguments (JSON)")
    add_p.add_argument("--priority", type=int, default=0)
    add_p.add_argument("--depends", type=int, nargs="+", help="Dependency job IDs")
    add_p.add_argument("--retries", type=int, default=3, help="Max retries")
    add_p.add_argument("--timeout", type=int, default=300, help="Timeout seconds")
    add_p.add_argument("--nice", type=int, help="Process nice value")
    add_p.add_argument("--delay", type=int, default=0, help="Delay seconds")
    
    # Scheduler command
    sched_p = sub.add_parser("scheduler", help="Run scheduler")
    sched_p.add_argument("--workers", type=int, help="Max workers")
    sched_p.add_argument("--batch", type=int, default=1, help="Batch size")
    
    # List command
    list_p = sub.add_parser("list", help="List jobs")
    list_p.add_argument("--status", help="Filter by status")
    list_p.add_argument("--limit", type=int, default=50, help="Limit results")
    
    # Stats command
    sub.add_parser("stats", help="Show statistics")
    
    # Cleanup command
    cleanup_p = sub.add_parser("cleanup", help="Clean old jobs")
    cleanup_p.add_argument("--days", type=int, default=7)
    
    # Load module command
    load_p = sub.add_parser("load", help="Load job module")
    load_p.add_argument("module", help="Python module path")
    
    # Get artifact command
    art_p = sub.add_parser("artifact", help="Get job artifact")
    art_p.add_argument("job_id", type=int)
    art_p.add_argument("key")
    
    args = parser.parse_args()
    orch = Orchestrator()
    
    if args.cmd == 'add':
        # Parse arguments
        parsed_args = []
        if args.args:
            for arg in args.args:
                # Try to parse as number
                try:
                    parsed_args.append(int(arg))
                except ValueError:
                    try:
                        parsed_args.append(float(arg))
                    except ValueError:
                        parsed_args.append(arg)
        
        scheduled_at = None
        if args.delay:
            scheduled_at = int(time.time() * 1000) + (args.delay * 1000)
        
        job_id = orch.add(
            name=args.name,
            func=args.func,
            args=tuple(parsed_args) if parsed_args else None,
            kwargs=args.kwargs,
            priority=args.priority,
            depends_on=args.depends,
            max_retries=args.retries,
            timeout_sec=args.timeout,
            nice=args.nice,
            scheduled_at=scheduled_at
        )
        print(f"Added job {job_id}: {args.name}")
    
    elif args.cmd == 'scheduler':
        if args.workers:
            orch.max_workers = args.workers
        scheduler = Scheduler(orch, args.batch)
        scheduler.run()
    
    elif args.cmd == 'list':
        with sqlite3.connect(orch.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            query = "SELECT * FROM jobs"
            params = []
            
            if args.status:
                query += " WHERE status=?"
                params.append(args.status)
            
            query += " ORDER BY created_at DESC LIMIT ?"
            params.append(args.limit)
            
            jobs = conn.execute(query, params).fetchall()
            
            for job in jobs:
                created = time.strftime('%Y-%m-%d %H:%M:%S',
                                       time.localtime(job['created_at'] / 1000))
                print(f"[{job['id']}] {job['name']} ({job['func']}): "
                      f"{job['status']} (pri={job['priority']}, created={created})")
    
    elif args.cmd == 'stats':
        stats = orch.stats()
        print(json.dumps(stats, indent=2))
    
    elif args.cmd == 'cleanup':
        deleted = orch.cleanup(args.days)
        print(f"Deleted {deleted} old jobs")
    
    elif args.cmd == 'load':
        jobs = JobRegistry.load_module(args.module)
        print(f"Loaded module {args.module}")
        print(f"Available jobs: {jobs}")
    
    elif args.cmd == 'artifact':
        value = orch.get_artifact(args.job_id, args.key)
        if value:
            print(json.dumps(value) if not isinstance(value, bytes) else value)
        else:
            print(f"No artifact '{args.key}' for job {args.job_id}")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: claudeCore.py
================================================================================

Of course. This is an excellent request that gets to the heart of software design: finding the right balance between simplicity and functionality.

Here is an enhanced version of the "Minimal Job Queue." It selectively adds back the most critical features from the more advanced scripts (aiose, Orchestrator) while intentionally leaving out the most complex ones (like systemd integration) to maintain its lightweight, dependency-free spirit.

This version is arguably the best balanced of the three for general-purpose use, representing a "sweet spot" of power and simplicity.

Enhanced Minimal AIOSE (aiose-core)

This script reintroduces the following essential features:

Delayed Exponential Backoff: Failed tasks are now rescheduled in the future (e.g., +2s, +4s, +8s), which is crucial for not overwhelming a temporarily failing resource.

Task Dependencies: You can now specify that a task should only run after other tasks have successfully completed.

Named Tasks: Tasks can be given a unique name, making them easier to identify and use in dependency chains.

Per-Task Timeouts: The timeout is no longer hardcoded and can be set for each task.

Environment Variables & Working Directory: Common and essential features for controlling the execution context of a command.

It remains under 300 lines and has no external dependencies.

code
Python
download
content_copy
expand_less
#!/usr/bin/env python3
"""
aiose-core: The essential, balanced job queue in <300 lines.
Adds back key features: delayed retries, dependencies, named tasks, and per-task timeouts.
"""
import argparse, json, os, sqlite3, subprocess, sys, time, threading, signal, shlex
from pathlib import Path

DB_PATH = Path.home() / ".aiose_core.db"

class Store:
    """Enhanced SQLite queue with dependencies and delayed retries."""
    def __init__(self):
        self.db = sqlite3.connect(DB_PATH, isolation_level=None, check_same_thread=False)
        self.db.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        self.db.executescript("""
        PRAGMA journal_mode=WAL; PRAGMA synchronous=NORMAL; PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS tasks(
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE,
            cmd TEXT NOT NULL,
            args TEXT,
            env TEXT,
            cwd TEXT,
            status TEXT DEFAULT 'q', -- q=queued r=running d=done f=failed
            priority INT DEFAULT 0,
            scheduled_at INT DEFAULT 0,
            dependencies TEXT,       -- JSON list of task IDs
            timeout INT DEFAULT 60,
            retry INT DEFAULT 0,
            error TEXT,
            result TEXT,
            created_at INT DEFAULT (strftime('%s','now')*1000),
            started_at INT,
            ended_at INT
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(status, priority DESC, scheduled_at, id) WHERE status='q';
        """)

    def add(self, cmd, **kwargs):
        with self.lock:
            # Prepare kwargs for DB insertion
            kwargs['cmd'] = cmd
            kwargs['scheduled_at'] = int(time.time()*1000) + kwargs.pop('delay_ms', 0)
            for key in ['args', 'env', 'dependencies']:
                if key in kwargs and kwargs[key] is not None:
                    kwargs[key] = json.dumps(kwargs[key])
            
            cols = ",".join(kwargs.keys())
            placeholders = ",".join("?" for _ in kwargs)
            return self.db.execute(
                f"INSERT INTO tasks({cols}) VALUES({placeholders})", tuple(kwargs.values())
            ).lastrowid

    def pop(self):
        """Atomically get the next task that is scheduled and has its dependencies met."""
        with self.lock:
            now = int(time.time()*1000)
            query = """
                SELECT id, cmd, args, env, cwd, timeout FROM tasks t
                WHERE status='q' AND scheduled_at <= :now
                AND (dependencies IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(t.dependencies) AS d
                    JOIN tasks AS dep_task ON dep_task.id = d.value
                    WHERE dep_task.status != 'd'
                ))
                ORDER BY priority DESC, scheduled_at, id
                LIMIT 1
            """
            row = self.db.execute(query, {'now': now}).fetchone()
            if not row:
                return None

            try: # Atomic UPDATE...RETURNING
                claimed = self.db.execute("""
                    UPDATE tasks SET status='r', started_at=?
                    WHERE id=? AND status='q'
                    RETURNING id, cmd, args, env, cwd, timeout
                """, (now, row['id'])).fetchone()
                return dict(claimed) if claimed else None
            except sqlite3.OperationalError: # Fallback for older SQLite
                updated = self.db.execute(
                    "UPDATE tasks SET status='r', started_at=? WHERE id=? AND status='q'",
                    (now, row['id'])
                ).rowcount
                return dict(row) if updated else None

    def complete(self, task_id, success, result=None, error=None):
        with self.lock:
            now = int(time.time()*1000)
            task = self.db.execute("SELECT retry FROM tasks WHERE id=?", (task_id,)).fetchone()
            if not task: return

            if success:
                self.db.execute(
                    "UPDATE tasks SET status='d', ended_at=?, result=? WHERE id=?",
                    (now, json.dumps(result)[:1000] if result else None, task_id)
                )
            elif task['retry'] < 3: # Max 3 retries
                delay_s = 2 ** task['retry'] # 1, 2, 4 seconds -> converted to s
                next_schedule = now + (delay_s * 1000)
                self.db.execute(
                    "UPDATE tasks SET status='q', retry=retry+1, scheduled_at=?, error=? WHERE id=?",
                    (next_schedule, str(error)[:500], task_id)
                )
                print(f"Task {task_id} failed, will retry in {delay_s}s.")
            else:
                self.db.execute(
                    "UPDATE tasks SET status='f', ended_at=?, error=? WHERE id=?",
                    (now, str(error)[:500], task_id)
                )

    def list_tasks(self):
        with self.lock:
            return self.db.execute("SELECT id, name, cmd, status, priority, error FROM tasks ORDER BY created_at DESC LIMIT 50").fetchall()

    def stats(self):
        with self.lock:
            rows = self.db.execute("SELECT status, COUNT(*) c FROM tasks GROUP BY status").fetchall()
            return {r['status']: r['c'] for r in rows}

class Worker:
    """Worker that processes tasks with timeout, env, and cwd support."""
    def __init__(self, store):
        self.store = store
        self.running = True
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)

    def _shutdown(self, *args):
        print("Signal received, shutting down gracefully...")
        self.running = False

    def run(self):
        print(f"Worker started (PID {os.getpid()})")
        while self.running:
            task = self.store.pop()
            if not task:
                time.sleep(0.2) # Sleep a bit longer if idle
                continue

            print(f"Running task {task['id']}: {task['cmd']}")
            try:
                cmd_list = [task['cmd']]
                if task['args']:
                    cmd_list.extend(json.loads(task['args']))
                
                env = os.environ.copy()
                if task['env']:
                    env.update(json.loads(task['env']))

                proc = subprocess.run(
                    cmd_list, capture_output=True, text=True,
                    timeout=task['timeout'],
                    env=env,
                    cwd=task['cwd'] # Can be None
                )
                self.store.complete(
                    task['id'],
                    proc.returncode == 0,
                    {'stdout': proc.stdout[:1000], 'stderr': proc.stderr[:1000]},
                    proc.stderr if proc.returncode != 0 else None
                )
                print(f"Task {task['id']}: {'success' if proc.returncode == 0 else 'failed'}")

            except subprocess.TimeoutExpired:
                self.store.complete(task['id'], False, error="Timeout")
                print(f"Task {task['id']}: timeout")
            except Exception as e:
                self.store.complete(task['id'], False, error=str(e))
                print(f"Task {task['id']}: error - {e}")

        print("Worker stopped")

def main():
    parser = argparse.ArgumentParser(description="aiose-core: A balanced, essential job queue.")
    sub = parser.add_subparsers(dest="cmd", required=True)

    add_p = sub.add_parser("add", help="Add a task")
    add_p.add_argument("command", help="Command to run")
    add_p.add_argument("args", nargs="*", help="Arguments for the command")
    add_p.add_argument("--name", help="Unique name for the task")
    add_p.add_argument("-p", "--priority", type=int, default=0, help="Higher is sooner")
    add_p.add_argument("--timeout", type=int, default=60, help="Timeout in seconds")
    add_p.add_argument("--deps", type=int, nargs='*', help="List of task IDs this task depends on")
    add_p.add_argument("--env", action="append", help="Set environment variables (e.g., KEY=VALUE)")
    add_p.add_argument("--cwd", help="Set the working directory")

    sub.add_parser("worker", help="Run the task processing worker")
    sub.add_parser("list", help="List recent tasks")
    sub.add_parser("stats", help="Show queue statistics")

    args = parser.parse_args()
    store = Store()

    if args.cmd == "add":
        env_dict = dict(e.split("=", 1) for e in args.env) if args.env else None
        task_id = store.add(
            args.command,
            args=args.args if args.args else None,
            name=args.name,
            priority=args.priority,
            timeout=args.timeout,
            dependencies=args.deps,
            env=env_dict,
            cwd=args.cwd
        )
        print(f"Added task {task_id}")

    elif args.cmd == "worker":
        Worker(store).run()

    elif args.cmd == "list":
        for task in store.list_tasks():
            name = f" ({task['name']})" if task['name'] else ""
            error_info = f" | ERR: {task['error'][:50]}" if task['status'] == 'f' and task['error'] else ""
            print(f"[{task['id']:<4}] {task['status']:<7} P={task['priority']} | {task['cmd']}{name}{error_info}")

    elif args.cmd == "stats":
        print(json.dumps(store.stats(), indent=2))

if __name__ == "__main__":
    main()
Evaluation: Why This Version is "Better" as a General-Purpose Tool

Solves Real-World Problems: The "Minimal" script was a great demo, but it would fail in many real scenarios. The lack of delayed retries would hammer a failing API. The lack of dependencies would make it impossible to build multi-step workflows (e.g., "download data," then "process data," then "upload results"). This version solves those fundamental problems.

Maintains Simplicity: The core logic is still just two classes (Store, Worker) and a CLI. It introduces no external libraries or platform-specific dependencies like systemd. It remains highly portable and easy to understand.

Improved Ergonomics: The addition of named tasks, per-task timeouts, and environment variables makes it far more pleasant and practical to use for anything beyond a trivial script. The worker is also more robust now because it passes a list of arguments to subprocess.run instead of using shell=True with a manually quoted string, which is safer.

Represents a Mature Compromise: This version demonstrates the programming maturity of knowing what to include and what to leave out. It takes the most critical 80% of features for a job queue and implements them cleanly, leaving out the most complex 20% that require a different architectural approach.

In the spectrum of the four scripts, this one hits the ideal middle ground for a powerful, reliable, and yet simple-to-manage local job queue.

================================================================================

================================================================================
FILE: deepseekCore.py
================================================================================

#!/usr/bin/env python3
"""Essential Job Queue - Core aiose features in <250 lines"""
import argparse, json, os, sqlite3, subprocess, sys, time, threading, signal
from pathlib import Path
from typing import Optional, List

DB_PATH = Path.home() / ".essential_aiose.db"

class Store:
    """SQLite queue with essential production features"""
    def __init__(self):
        self.db = sqlite3.connect(DB_PATH, isolation_level=None, check_same_thread=False)
        self.db.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        self.db.executescript("""
        PRAGMA journal_mode=WAL; PRAGMA synchronous=NORMAL; PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS tasks(
            id INTEGER PRIMARY KEY,
            cmd TEXT NOT NULL,
            args TEXT,
            mode TEXT DEFAULT 'local',  -- local or systemd
            status TEXT DEFAULT 'q',    -- q=queued r=running d=done f=failed
            priority INT DEFAULT 0,
            retry INT DEFAULT 0,
            max_retries INT DEFAULT 3,
            timeout INT DEFAULT 300,
            error TEXT,
            result TEXT,
            created INT DEFAULT (strftime('%s','now')*1000),
            started INT,
            ended INT,
            scheduled INT DEFAULT 0,    -- when to run (for delays)
            deps TEXT,                  -- JSON array of dependency task IDs
            unit TEXT                   -- systemd unit name
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(status, priority DESC, scheduled, id) 
        WHERE status='q';
        """)

    def add(self, cmd, args=None, mode='local', priority=0, timeout=300, 
            max_retries=3, scheduled=0, deps=None):
        with self.lock:
            return self.db.execute("""
                INSERT INTO tasks(cmd, args, mode, priority, timeout, max_retries, scheduled, deps) 
                VALUES (?,?,?,?,?,?,?,?)
            """, (
                cmd, 
                json.dumps(args) if args else None, 
                mode,
                priority, 
                timeout,
                max_retries,
                scheduled,
                json.dumps(deps) if deps else None
            )).lastrowid

    def pop(self, worker_id: str) -> Optional[dict]:
        """Atomically get next task with dependency check"""
        with self.lock:
            now = int(time.time() * 1000)
            
            # Find eligible task (ready, deps met, not future-scheduled)
            query = """
                SELECT id, cmd, args, mode, timeout FROM tasks 
                WHERE status='q' AND scheduled <= ?
                AND (deps IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(tasks.deps) AS d
                    JOIN tasks t2 ON t2.id = d.value WHERE t2.status != 'd'
                ))
                ORDER BY priority DESC, scheduled, id LIMIT 1
            """
            row = self.db.execute(query, (now,)).fetchone()
            if not row:
                return None

            # Atomic claim
            try:
                claimed = self.db.execute("""
                    UPDATE tasks SET status='r', started=?, unit=NULL 
                    WHERE id=? AND status='q' RETURNING id, cmd, args, mode, timeout
                """, (now, row['id'])).fetchone()
                if claimed:
                    task = dict(claimed)
                    if task['args']:
                        task['args'] = json.loads(task['args'])
                    return task
            except sqlite3.OperationalError:
                # Fallback for older SQLite
                self.db.execute("BEGIN IMMEDIATE")
                updated = self.db.execute(
                    "UPDATE tasks SET status='r', started=?, unit=NULL WHERE id=? AND status='q'",
                    (now, row['id'])
                ).rowcount
                self.db.execute("COMMIT")
                if updated:
                    task = dict(row)
                    if task['args']:
                        task['args'] = json.loads(task['args'])
                    return task
            return None

    def complete(self, task_id: int, success: bool, result=None, error=None):
        with self.lock:
            now = int(time.time() * 1000)
            task = self.db.execute(
                "SELECT retry, max_retries FROM tasks WHERE id=?", (task_id,)
            ).fetchone()
            if not task:
                return

            if success:
                self.db.execute(
                    "UPDATE tasks SET status='d', ended=?, result=? WHERE id=?",
                    (now, json.dumps(result)[:1000] if result else None, task_id)
                )
            else:
                retry_count = task['retry'] or 0
                max_retries = task['max_retries'] or 3
                
                if retry_count < max_retries:
                    # Exponential backoff: 1s, 2s, 4s, 8s...
                    delay_ms = 1000 * (2 ** retry_count)
                    self.db.execute(
                        "UPDATE tasks SET status='q', retry=retry+1, scheduled=?, error=? WHERE id=?",
                        (now + delay_ms, str(error)[:500], task_id)
                    )
                else:
                    self.db.execute(
                        "UPDATE tasks SET status='f', ended=?, error=? WHERE id=?",
                        (now, str(error)[:500], task_id)
                    )

    def reclaim_stalled(self, timeout_seconds: int = 300) -> int:
        """Reclaim tasks stuck in 'running' status"""
        with self.lock:
            cutoff = int(time.time() * 1000) - (timeout_seconds * 1000)
            return self.db.execute(
                "UPDATE tasks SET status='q', retry=retry+1, scheduled=? WHERE status='r' AND started < ?",
                (int(time.time() * 1000), cutoff)
            ).rowcount

    def list_tasks(self, limit: int = 50):
        with self.lock:
            return self.db.execute(
                "SELECT * FROM tasks ORDER BY created DESC LIMIT ?", (limit,)
            ).fetchall()

    def stats(self):
        with self.lock:
            status_counts = {r['status']: r['c'] for r in 
                self.db.execute("SELECT status, COUNT(*) c FROM tasks GROUP BY status").fetchall()}
            
            avg_times = self.db.execute("""
                SELECT AVG(started - created) as avg_queue_time, 
                       AVG(ended - started) as avg_run_time 
                FROM tasks WHERE status='d' AND started IS NOT NULL AND ended IS NOT NULL
            """).fetchone()
            
            return {
                'status': status_counts,
                'performance': dict(avg_times) if avg_times[0] else {}
            }

class Worker:
    """Worker with essential production features"""
    def __init__(self, store: Store, worker_id: str = None):
        self.store = store
        self.worker_id = worker_id or f"w{os.getpid()}"
        self.running = True
        signal.signal(signal.SIGTERM, self._stop)
        signal.signal(signal.SIGINT, self._stop)

    def _stop(self, *args):
        print(f"\nShutting down worker {self.worker_id}...")
        self.running = False

    def _execute_safe(self, cmd: str, args: List[str], timeout: int) -> tuple[bool, dict, str]:
        """Execute command safely without shell injection"""
        try:
            # Build command array safely
            if args:
                command = [cmd] + [str(a) for a in args]
            else:
                # If no args, check if cmd is a single command or needs shell
                if ' ' in cmd or '|' in cmd or '>' in cmd or '&' in cmd:
                    command = ['/bin/sh', '-c', cmd]
                else:
                    command = [cmd]

            proc = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            result = {
                'stdout': proc.stdout[:1000],
                'stderr': proc.stderr[:1000],
                'returncode': proc.returncode
            }
            
            return proc.returncode == 0, result, proc.stderr if proc.returncode != 0 else None
            
        except subprocess.TimeoutExpired:
            return False, None, "Timeout"
        except Exception as e:
            return False, None, str(e)

    def run(self):
        print(f"Worker {self.worker_id} started (PID {os.getpid()})")
        last_reclaim = time.time()
        
        while self.running:
            # Reclaim stalled tasks every 30 seconds
            if time.time() - last_reclaim > 30:
                reclaimed = self.store.reclaim_stalled()
                if reclaimed:
                    print(f"Reclaimed {reclaimed} stalled tasks")
                last_reclaim = time.time()

            # Get and process task
            task = self.store.pop(self.worker_id)
            if not task:
                time.sleep(0.1)
                continue

            print(f"Running task {task['id']}: {task['cmd']}")
            success, result, error = self._execute_safe(
                task['cmd'],
                task.get('args', []),
                task.get('timeout', 300)
            )

            self.store.complete(task['id'], success, result, error)
            status = "success" if success else f"failed: {error}"
            print(f"Task {task['id']}: {status}")

        print("Worker stopped")

def main():
    parser = argparse.ArgumentParser(description="Essential Job Queue")
    sub = parser.add_subparsers(dest="cmd", required=True)

    # Add command
    add_p = sub.add_parser("add", help="Add task")
    add_p.add_argument("command", help="Command to run")
    add_p.add_argument("args", nargs="*", help="Arguments")
    add_p.add_argument("--mode", choices=['local', 'systemd'], default='local')
    add_p.add_argument("-p", "--priority", type=int, default=0)
    add_p.add_argument("--timeout", type=int, default=300)
    add_p.add_argument("--retries", type=int, default=3)
    add_p.add_argument("--delay", type=int, default=0, help="Delay execution by N seconds")
    add_p.add_argument("--deps", help="Comma-separated dependency task IDs")

    # Worker
    worker_p = sub.add_parser("worker", help="Run worker")
    worker_p.add_argument("--id", help="Worker ID")

    # List
    sub.add_parser("list", help="List recent tasks")

    # Stats
    sub.add_parser("stats", help="Show stats")

    # Reclaim
    sub.add_parser("reclaim", help="Reclaim stalled tasks")

    args = parser.parse_args()
    store = Store()

    if args.cmd == "add":
        deps = [int(d.strip()) for d in args.deps.split(',')] if args.deps else None
        scheduled = int(time.time() * 1000) + (args.delay * 1000) if args.delay > 0 else 0
        
        task_id = store.add(
            cmd=args.command,
            args=args.args,
            mode=args.mode,
            priority=args.priority,
            timeout=args.timeout,
            max_retries=args.retries,
            scheduled=scheduled,
            deps=deps
        )
        print(f"Added task {task_id}")

    elif args.cmd == "worker":
        Worker(store, args.id).run()

    elif args.cmd == "list":
        for task in store.list_tasks():
            deps = json.loads(task['deps']) if task['deps'] else []
            print(f"[{task['id']}] {task['cmd']} - {task['status']} "
                  f"(p={task['priority']}, retry={task['retry']}, deps={deps})")

    elif args.cmd == "stats":
        stats = store.stats()
        print(json.dumps(stats, indent=2))

    elif args.cmd == "reclaim":
        reclaimed = store.reclaim_stalled()
        print(f"Reclaimed {reclaimed} stalled tasks")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: minimal_aiose.py
================================================================================

#!/usr/bin/env python3
"""Minimal Job Queue - Essential aiose in <200 lines"""
import argparse, json, os, sqlite3, subprocess, sys, time, threading, signal
from pathlib import Path

DB_PATH = Path.home() / ".minimal_aiose.db"

class Store:
    """Minimal SQLite queue with essential operations"""
    def __init__(self):
        self.db = sqlite3.connect(DB_PATH, isolation_level=None, check_same_thread=False)
        self.db.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        self.db.executescript("""
        PRAGMA journal_mode=WAL; PRAGMA synchronous=NORMAL; PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS tasks(
            id INTEGER PRIMARY KEY,
            cmd TEXT NOT NULL,
            args TEXT,
            status TEXT DEFAULT 'q',  -- q=queued r=running d=done f=failed
            priority INT DEFAULT 0,
            retry INT DEFAULT 0,
            error TEXT,
            result TEXT,
            created INT DEFAULT (strftime('%s','now')*1000),
            started INT,
            ended INT
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(status, priority DESC, id) WHERE status='q';
        """)

    def add(self, cmd, args=None, priority=0):
        with self.lock:
            return self.db.execute(
                "INSERT INTO tasks(cmd, args, priority) VALUES (?,?,?)",
                (cmd, json.dumps(args) if args else None, priority)
            ).lastrowid

    def pop(self):
        """Atomically get next task"""
        with self.lock:
            now = int(time.time()*1000)
            # Try atomic RETURNING first
            try:
                row = self.db.execute("""
                    UPDATE tasks SET status='r', started=?
                    WHERE id=(SELECT id FROM tasks WHERE status='q' ORDER BY priority DESC, id LIMIT 1)
                    RETURNING id, cmd, args
                """, (now,)).fetchone()
                return dict(row) if row else None
            except:
                # Fallback for older SQLite
                row = self.db.execute(
                    "SELECT id, cmd, args FROM tasks WHERE status='q' ORDER BY priority DESC, id LIMIT 1"
                ).fetchone()
                if row:
                    n = self.db.execute(
                        "UPDATE tasks SET status='r', started=? WHERE id=? AND status='q'",
                        (now, row['id'])
                    ).rowcount
                    return dict(row) if n else None
        return None

    def complete(self, task_id, success, result=None, error=None):
        with self.lock:
            now = int(time.time()*1000)
            task = self.db.execute("SELECT retry FROM tasks WHERE id=?", (task_id,)).fetchone()
            if not task: return

            if success:
                self.db.execute(
                    "UPDATE tasks SET status='d', ended=?, result=? WHERE id=?",
                    (now, json.dumps(result)[:500] if result else None, task_id)
                )
            elif task['retry'] < 3:
                # Retry with exponential backoff
                self.db.execute(
                    "UPDATE tasks SET status='q', retry=retry+1, error=? WHERE id=?",
                    (str(error)[:500], task_id)
                )
            else:
                self.db.execute(
                    "UPDATE tasks SET status='f', ended=?, error=? WHERE id=?",
                    (now, str(error)[:500], task_id)
                )

    def list_tasks(self):
        with self.lock:
            return self.db.execute("SELECT * FROM tasks ORDER BY created DESC LIMIT 50").fetchall()

    def stats(self):
        with self.lock:
            rows = self.db.execute("SELECT status, COUNT(*) c FROM tasks GROUP BY status").fetchall()
            return {r['status']: r['c'] for r in rows}

class Worker:
    """Minimal worker that processes tasks"""
    def __init__(self, store):
        self.store = store
        self.running = True
        signal.signal(signal.SIGTERM, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGINT, lambda *_: setattr(self, 'running', False))

    def run(self):
        print(f"Worker started (PID {os.getpid()})")
        while self.running:
            task = self.store.pop()
            if not task:
                time.sleep(0.1)
                continue

            print(f"Running task {task['id']}: {task['cmd']}")
            try:
                # Build command
                cmd = task['cmd']
                if task['args']:
                    args = json.loads(task['args'])
                    import shlex
                    cmd = f"{cmd} {' '.join(shlex.quote(str(a)) for a in args)}"

                # Execute
                proc = subprocess.run(
                    cmd, shell=True, capture_output=True, text=True, timeout=60
                )

                # Complete
                self.store.complete(
                    task['id'],
                    proc.returncode == 0,
                    {'stdout': proc.stdout[:500], 'stderr': proc.stderr[:500]},
                    proc.stderr if proc.returncode != 0 else None
                )
                print(f"Task {task['id']}: {'success' if proc.returncode == 0 else 'failed'}")

            except subprocess.TimeoutExpired:
                self.store.complete(task['id'], False, error="Timeout")
                print(f"Task {task['id']}: timeout")
            except Exception as e:
                self.store.complete(task['id'], False, error=str(e))
                print(f"Task {task['id']}: error - {e}")

        print("Worker stopped")

def main():
    parser = argparse.ArgumentParser(description="Minimal Job Queue")
    sub = parser.add_subparsers(dest="cmd", required=True)

    # Add command
    add_p = sub.add_parser("add", help="Add task")
    add_p.add_argument("command", help="Command to run")
    add_p.add_argument("-p", "--priority", type=int, default=0, help="Priority")
    add_p.add_argument("args", nargs="*", help="Arguments")

    # Worker
    sub.add_parser("worker", help="Run worker")

    # List
    sub.add_parser("list", help="List tasks")

    # Stats
    sub.add_parser("stats", help="Show stats")

    args = parser.parse_args()
    store = Store()

    if args.cmd == "add":
        task_id = store.add(args.command, args.args if args.args else None, args.priority)
        print(f"Added task {task_id}")

    elif args.cmd == "worker":
        Worker(store).run()

    elif args.cmd == "list":
        for task in store.list_tasks():
            print(f"[{task['id']}] {task['cmd']} - {task['status']} (p={task['priority']})")

    elif args.cmd == "stats":
        stats = store.stats()
        print(json.dumps(stats, indent=2))

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: qwen2.py
================================================================================

#!/usr/bin/env python3
"""
Orchestrator: High-Performance Job Queue + Systemd Integration (<500 lines)
Synthesizes the best patterns from all candidates into a minimal, robust system.
"""
import argparse, json, os, shlex, signal, sqlite3, subprocess, sys, time, threading
from pathlib import Path
from typing import Optional, Dict, Any

# --- Configuration & Constants ---
DB_PATH = Path.home() / ".orchestrator.db"
UNIT_PREFIX = "orch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

# Production-grade SQLite pragmas (claudeCodeD, chatgpt, geminiDeep)
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

def now_ms() -> int:
    return int(time.time() * 1000)

def unit_name(name: str) -> str:
    """Generate a safe systemd unit name."""
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
    return f"{UNIT_PREFIX}{safe}.service"

# --- Core Storage Engine ---
class Store:
    """High-performance, thread-safe SQLite storage with atomic operations."""
    def __init__(self, path=DB_PATH):
        path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        
        # Apply performance optimizations
        for pragma in PRAGMAS:
            self.conn.execute(pragma)
        
        # Unified, minimal schema (Synthesized from chatgpt, geminiDeep, grok)
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                cmd TEXT NOT NULL,
                args TEXT DEFAULT '[]',
                env TEXT,
                cwd TEXT,
                p INT DEFAULT 0,        -- priority
                s TEXT DEFAULT 'q',     -- status: q=queued, r=running, d=done, f=failed
                at INT DEFAULT 0,       -- scheduled_at (ms)
                w TEXT,                 -- worker_id
                r INT DEFAULT 0,        -- retry_count
                e TEXT,                 -- error_message
                res TEXT,               -- result
                ct INT DEFAULT (strftime('%s','now')*1000), -- created_at
                st INT, et INT,         -- started_at, ended_at
                dep TEXT,               -- JSON array of dependency IDs
                schedule TEXT,          -- systemd calendar format
                rtprio INTEGER,
                nice INTEGER,
                slice TEXT,
                cpu_weight INTEGER,
                mem_max_mb INTEGER,
                unit TEXT               -- systemd unit name
            );
            -- Optimized composite index for fetching (claudeCodeD pattern)
            CREATE INDEX IF NOT EXISTS idx_fetch ON tasks(s, p DESC, at, id) WHERE s = 'q';
            -- Metrics table (Chrome pattern from claudeCodeD)
            CREATE TABLE IF NOT EXISTS metrics (
                task_id INTEGER PRIMARY KEY,
                qt REAL,  -- queue_time (seconds)
                et REAL,  -- exec_time (seconds)
                FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
            );
        """)

    # --- Atomic Queue Operations ---
    def pop(self, worker_id: str) -> Optional[Dict[str, Any]]:
        """Atomically claim the next eligible task, respecting dependencies and schedule."""
        now = now_ms()
        query = """
            SELECT id, cmd, args, env, cwd, rtprio, nice, slice, cpu_weight, mem_max_mb, schedule
            FROM tasks t
            WHERE s='q' AND at<=:now
            AND (dep IS NULL OR NOT EXISTS (
                SELECT 1 FROM json_each(t.dep) AS d
                JOIN tasks AS dt ON dt.id = d.value
                WHERE dt.s != 'd'
            ))
            ORDER BY p DESC, at, id
            LIMIT 1
        """
        with self.lock:
            row = self.conn.execute(query, {'now': now}).fetchone()
            if not row:
                return None

            # Atomically claim using UPDATE...RETURNING
            try:
                claimed = self.conn.execute("""
                    UPDATE tasks SET s='r', w=?, st=? 
                    WHERE id=? AND s='q' 
                    RETURNING id, cmd, args, env, cwd, rtprio, nice, slice, cpu_weight, mem_max_mb, schedule
                """, (worker_id, now, row['id'])).fetchone()
                if claimed:
                    task = dict(claimed)
                    for key in ['args', 'env']:
                        if task[key]:
                            task[key] = json.loads(task[key])
                    return task
            except sqlite3.OperationalError:
                # Fallback for older SQLite versions
                self.conn.execute("BEGIN IMMEDIATE")
                updated = self.conn.execute(
                    "UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q'",
                    (worker_id, now, row['id'])
                ).rowcount
                self.conn.execute("COMMIT")
                if updated:
                    task = dict(row)
                    for key in ['args', 'env']:
                        if task[key]:
                            task[key] = json.loads(task[key])
                    return task
        return None

    def finalize(self, task_id: int, success: bool, error: str = None):
        """Mark task as complete (d/f) or schedule retry (q), and record metrics."""
        now = now_ms()
        with self.lock:
            task = self.conn.execute("SELECT r, ct, st, unit FROM tasks WHERE id=?", (task_id,)).fetchone()
            if not task:
                return

            if success:
                self.conn.execute("UPDATE tasks SET s='d', et=?, unit=NULL WHERE id=?", (now, task_id))
                # Record performance metrics
                if task['st'] and task['ct']:
                    queue_time = (task['st'] - task['ct']) / 1000.0
                    exec_time = (now - task['st']) / 1000.0
                    self.conn.execute(
                        "INSERT OR REPLACE INTO metrics(task_id, qt, et) VALUES(?,?,?)",
                        (task_id, queue_time, exec_time)
                    )
            else:
                if task['r'] < 3:
                    # Exponential backoff retry (1s, 2s, 4s)
                    delay = 1000 * (2 ** task['r'])
                    self.conn.execute(
                        "UPDATE tasks SET s='q', at=?, r=r+1, e=?, unit=NULL WHERE id=?",
                        (now + delay, error, task_id)
                    )
                else:
                    self.conn.execute(
                        "UPDATE tasks SET s='f', et=?, e=?, unit=NULL WHERE id=?",
                        (now, error, task_id)
                    )

    # --- CRUD Operations ---
    def add(self, **kw) -> int:
        """Add a new task to the queue."""
        with self.lock:
            cols = ",".join(kw.keys())
            qs = ",".join("?" for _ in kw)
            cursor = self.conn.execute(f"INSERT INTO tasks({cols}) VALUES({qs})", tuple(kw.values()))
            return cursor.lastrowid

    def get_by_name(self, name: str) -> Optional[sqlite3.Row]:
        with self.lock:
            return self.conn.execute("SELECT * FROM tasks WHERE name=?", (name,)).fetchone()

    def list(self) -> list:
        with self.lock:
            return self.conn.execute("SELECT * FROM tasks ORDER BY ct DESC").fetchall()

    def update_unit(self, task_id: int, unit_name: str, status: str):
        with self.lock:
            self.conn.execute("UPDATE tasks SET unit=?, s=? WHERE id=?", (unit_name, status, task_id))

    def stats(self) -> Dict[str, Any]:
        """Return comprehensive system statistics."""
        with self.lock:
            counts = {r['s']: r['c'] for r in self.conn.execute("SELECT s, COUNT(*) c FROM tasks GROUP BY s")}
            perf = self.conn.execute("SELECT AVG(qt) avg_qt, AVG(et) avg_et, COUNT(*) count FROM metrics").fetchone()
            return {
                'tasks': counts,
                'perf': dict(perf) if perf and perf['avg_qt'] is not None else {}
            }

    def cleanup(self, days: int = 7) -> int:
        """Clean up old completed/failed tasks."""
        cutoff = now_ms() - (days * 86400000)
        with self.lock:
            deleted = self.conn.execute("DELETE FROM tasks WHERE s IN ('d','f') AND et < ?", (cutoff,)).rowcount
            # Vacuum if significantly fragmented (Firefox pattern)
            page_count = self.conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = self.conn.execute("PRAGMA freelist_count").fetchone()[0]
            if freelist > page_count * 0.3:
                self.conn.execute("VACUUM")
            return deleted

# --- Systemd Execution Engine ---
class SystemdExecutor:
    """Handles execution of tasks via systemd-run for resource isolation and management."""
    def __init__(self, store: Store):
        self.store = store

    def launch(self, task: Dict[str, Any]) -> bool:
        """Launch a task using systemd-run with resource controls."""
        unit = unit_name(f"{task['name']}-{task['id']}")
        props = [
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",
        ]

        # Apply resource controls (Integration from Program 2)
        if task.get('rtprio'):
            props.extend([
                "--property=CPUSchedulingPolicy=rr",
                f"--property=CPUSchedulingPriority={task['rtprio']}"
            ])
        if task.get('nice') is not None:
            props.append(f"--property=Nice={int(task['nice'])}")
        if task.get('slice'):
            props.append(f"--slice={task['slice']}")
        if task.get('cpu_weight'):
            props.append(f"--property=CPUWeight={int(task['cpu_weight'])}")
        if task.get('mem_max_mb'):
            props.append(f"--property=MemoryMax={int(task['mem_max_mb'])}M")
        if task.get('cwd'):
            props.append(f"--property=WorkingDirectory={task['cwd']}")

        # Add environment variables
        env = []
        if task.get('env'):
            for k, v in task['env'].items():
                env.extend(["--setenv", f"{k}={v}"])

        # Handle scheduling
        schedule_args = []
        if task.get('schedule'):
            schedule_args.extend(["--on-calendar", task['schedule']])

        # Build and execute the systemd-run command
        cmd_to_run = [task['cmd']] + (task.get('args') or [])
        run_cmd = [*SYSDRUN, "--unit", unit, *props, *env, *schedule_args, "--", *cmd_to_run]

        try:
            cp = subprocess.run(run_cmd, text=True, capture_output=True, timeout=10)
            if cp.returncode == 0:
                status = "sched" if task.get('schedule') else "start"
                self.store.update_unit(task['id'], unit, status)
                return True
            else:
                error_msg = f"Launch failed: {cp.stderr.strip()[:500]}"
                self.store.finalize(task['id'], False, error=error_msg)
                return False
        except subprocess.TimeoutExpired:
            self.store.finalize(task['id'], False, error="Systemd launch timeout")
            return False
        except Exception as e:
            self.store.finalize(task['id'], False, error=f"Systemd launch error: {str(e)}")
            return False

    def reconcile(self) -> int:
        """Reconcile running systemd units with database state."""
        with self.store.lock:
            rows = self.store.conn.execute(
                "SELECT id, unit FROM tasks WHERE s IN ('r', 'start', 'sched') AND unit IS NOT NULL"
            ).fetchall()

        if not rows:
            return 0

        # Query systemd for all relevant units in a single call
        unit_names = [row['unit'] for row in rows]
        cmd = [*SYSTEMCTL, "show", *unit_names, "--property=Id,ActiveState,Result"]
        try:
            cp = subprocess.run(cmd, text=True, capture_output=True, timeout=10)
            if cp.returncode != 0:
                print(f"Warning: Reconciliation failed: {cp.stderr}", file=sys.stderr)
                return 0

            reconciled_count = 0
            # Parse output (blocks separated by empty lines)
            for block in cp.stdout.strip().split('\n\n'):
                if not block:
                    continue
                props = {}
                for line in block.splitlines():
                    if '=' in line:
                        key, value = line.split('=', 1)
                        props[key] = value

                unit_id = props.get("Id")
                task_row = next((r for r in rows if r['unit'] == unit_id), None)
                if not task_row:
                    continue

                # Check if the unit has finished
                if props.get("ActiveState") in ("inactive", "failed"):
                    success = (props.get("Result") == "success")
                    error = None if success else f"Systemd Result={props.get('Result')}"
                    self.store.finalize(task_row['id'], success, error)
                    reconciled_count += 1

            return reconciled_count

        except Exception as e:
            print(f"Error during reconciliation: {e}", file=sys.stderr)
            return 0

# --- Worker Process ---
class Worker:
    """Background worker that pulls tasks from the queue and executes them."""
    def __init__(self, store: Store, executor: SystemdExecutor):
        self.store = store
        self.executor = executor
        self.running = True
        self.worker_id = f"w{os.getpid()}"
        self._setup_signal_handlers()

    def _setup_signal_handlers(self):
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)

    def _shutdown(self, signum, frame):
        print(f"Received signal {signum}, shutting down gracefully...")
        self.running = False

    def run(self):
        """Main worker loop."""
        print(f"Worker {self.worker_id} started.")
        reconcile_timer = time.time()

        while self.running:
            # Periodic systemd unit reconciliation
            if time.time() - reconcile_timer > 5:
                try:
                    count = self.executor.reconcile()
                    if count > 0:
                        print(f"Reconciled {count} tasks.")
                except Exception as e:
                    print(f"Error in reconciliation: {e}", file=sys.stderr)
                reconcile_timer = time.time()

            # Pull and execute a task
            task = self.store.pop(self.worker_id)
            if task:
                print(f"Launching task {task['id']}: {task['name']}")
                success = self.executor.launch(task)
                if not success:
                    # If systemd launch failed, mark it as a failure immediately
                    self.store.finalize(task['id'], False, error="Failed to launch via systemd")
            else:
                time.sleep(0.1)  # Sleep briefly if no tasks are available

        print("Worker shutdown complete.")

# --- Command Line Interface ---
def main():
    parser = argparse.ArgumentParser(description="Production Job Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Add Task
    add_parser = subparsers.add_parser("add", help="Add a new task")
    add_parser.add_argument("name", help="Unique task name")
    add_parser.add_argument("cmd", help="Command to execute")
    add_parser.add_argument("args", nargs="*", help="Command arguments")
    add_parser.add_argument("-p", "--priority", type=int, default=0, help="Task priority (higher = sooner)")
    add_parser.add_argument("--delay", type=int, default=0, help="Delay execution by N seconds")
    add_parser.add_argument("--deps", help="Comma-separated list of dependency task IDs")
    # Resource controls
    add_parser.add_argument("--nice", type=int, help="Nice value for CPU scheduling")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority (1-99)")
    add_parser.add_argument("--cpu-weight", type=int, help="CPU weight (1-10000)")
    add_parser.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add_parser.add_argument("--schedule", help="Systemd calendar format (e.g., 'daily', '*-*-* 12:00:00')")
    add_parser.add_argument("--env", action="append", help="Environment variables (KEY=VALUE)")
    add_parser.add_argument("--cwd", help="Working directory")

    # Run Worker
    run_parser = subparsers.add_parser("worker", help="Start the task worker/executor")

    # Management Commands
    subparsers.add_parser("stats", help="Show queue statistics")
    cleanup_parser = subparsers.add_parser("cleanup", help="Clean up old tasks")
    cleanup_parser.add_argument("--days", type=int, default=7, help="Days to keep completed tasks")

    args = parser.parse_args()
    store = Store()

    if args.command == "add":
        try:
            deps = [int(d) for d in args.deps.split(',')] if args.deps else None
        except ValueError:
            sys.exit("Error: Dependencies must be comma-separated integers.")

        config = {}
        if args.nice is not None: config['nice'] = args.nice
        if args.rtprio is not None: config['rtprio'] = args.rtprio
        if args.cpu_weight: config['cpu_weight'] = args.cpu_weight
        if args.mem_max_mb: config['mem_max_mb'] = args.mem_max_mb

        env_dict = {}
        if args.env:
            for e in args.env:
                if "=" in e:
                    k, v = e.split("=", 1)
                    env_dict[k] = v

        at = now_ms() + (args.delay * 1000)
        task_id = store.add(
            name=args.name,
            cmd=args.cmd,
            args=json.dumps(args.args),
            env=json.dumps(env_dict) if env_dict else None,
            cwd=args.cwd,
            p=args.priority,
            at=at,
            dep=json.dumps(deps) if deps else None,
            schedule=args.schedule,
            **config
        )
        print(f"Added Task ID: {task_id}")

    elif args.command == "worker":
        if not shutil.which("systemd-run"):
            sys.exit("Error: 'systemd-run' not found. This orchestrator requires systemd.")
        executor = SystemdExecutor(store)
        worker = Worker(store, executor)
        worker.run()

    elif args.command == "stats":
        print(json.dumps(store.stats(), indent=2))

    elif args.command == "cleanup":
        deleted = store.cleanup(args.days)
        print(f"Cleaned up {deleted} old tasks")

if __name__ == "__main__":
    import shutil
    main()

================================================================================

SUMMARY:
----------------------------------------
Total files consolidated: 6
Files included:
  - chatgpt2.py: 438 lines, 20,830 bytes
  - claude2.py: 681 lines, 26,464 bytes
  - claudeCore.py: 262 lines, 12,180 bytes
  - deepseekCore.py: 306 lines, 11,806 bytes
  - minimal_aiose.py: 182 lines, 6,787 bytes
  - qwen2.py: 452 lines, 18,986 bytes
