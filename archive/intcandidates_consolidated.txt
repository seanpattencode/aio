CONSOLIDATED FILES FROM Integration/IntCandidates
================================================================================


================================================================================
FILE: chatgpt.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Orchestrator â€” unified, <500 lines
- Single SQLite DB for queue + orchestration metadata
- Two run modes:
  * local: fast in-process worker executes shell cmds with retries/backoff
  * systemd: isolation via transient user units (systemd-run), optional timers
- Dependencies, priorities, scheduling (at/on-calendar), metrics, cleanup
- Simple CLI: add | worker | start | stop | list | status | stats | reconcile | cleanup
"""
import argparse, json, os, shlex, signal, sqlite3, subprocess, sys, time, threading
from pathlib import Path
from typing import Optional, Dict, Any

# ---------- constants ----------
DB_PATH = Path.home() / ".aios_orchestrator.db"
UNIT_PREFIX = "aios-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN   = ["systemd-run", "--user", "--collect", "--quiet"]

PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA mmap_size=268435456",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

# ---------- helpers ----------
def now_ms() -> int: return int(time.time() * 1000)
def unit_name(name_or_id: str) -> str:
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in str(name_or_id))
    return f"{UNIT_PREFIX}{safe}.service"
def run(cmd:list[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

# ---------- storage ----------
class Store:
    """SQLite with production pragmas; unified 'tasks' table for both modes."""
    def __init__(self, path=DB_PATH):
        self.c = sqlite3.connect(path, isolation_level=None, check_same_thread=False)
        self.c.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        for p in PRAGMAS: self.c.execute(p)
        self.c.executescript("""
        CREATE TABLE IF NOT EXISTS tasks(
          id INTEGER PRIMARY KEY,
          name TEXT UNIQUE,                -- optional human handle
          cmd  TEXT NOT NULL,              -- shell command
          mode TEXT DEFAULT 'local',       -- 'local' | 'systemd'
          p INT DEFAULT 0,                 -- priority
          s TEXT DEFAULT 'q',              -- q=queued r=running d=done f=failed sched=scheduled start=started
          at INT DEFAULT 0,                -- scheduled_at (ms) for local
          w  TEXT,                         -- worker_id (local)
          r  INT DEFAULT 0,                -- retry_count (local)
          e  TEXT,                         -- error_message
          res TEXT,                        -- result (truncated stdout/stderr, local)
          ct INT DEFAULT (strftime('%s','now')*1000),
          st INT,                          -- started_at
          et INT,                          -- ended_at
          dep TEXT,                        -- JSON array of dependency IDs (local)
          -- Orchestration extras
          env TEXT, cwd TEXT, schedule TEXT,
          rtprio INTEGER, nice INTEGER, slice TEXT,
          cpu_weight INTEGER, mem_max_mb INTEGER,
          unit TEXT                        -- systemd unit name if used
        );
        CREATE INDEX IF NOT EXISTS idx_q ON tasks(s, p DESC, at, id) WHERE s IN ('q','r');
        CREATE TABLE IF NOT EXISTS metrics(task_id INTEGER PRIMARY KEY, qt REAL, et REAL,
            FOREIGN KEY(task_id) REFERENCES tasks(id) ON DELETE CASCADE);
        """)
    # ---- CRUD / queries ----
    def add(self, **kw) -> int:
        with self.lock:
            cols = ",".join(kw.keys()); qs = ",".join("?" for _ in kw)
            return self.c.execute(f"INSERT INTO tasks({cols}) VALUES({qs})", tuple(kw.values())).lastrowid
    def get_by_name(self, name:str) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE name=?", (name,)).fetchone()
    def get_by_id(self, tid:int) -> Optional[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks WHERE id=?", (tid,)).fetchone()
    def list(self) -> list[sqlite3.Row]:
        with self.lock: return self.c.execute("SELECT * FROM tasks ORDER BY ct DESC").fetchall()
    def update(self, tid:int, **kw):
        if not kw: return
        with self.lock:
            sets = ",".join(f"{k}=?" for k in kw)
            self.c.execute(f"UPDATE tasks SET {sets} WHERE id=?", (*kw.values(), tid))
    # ---- queue ops (local) ----
    def pop(self, worker_id:str) -> Optional[Dict[str,Any]]:
        with self.lock:
            row = self.c.execute("""
              SELECT id, cmd FROM tasks
              WHERE mode='local' AND s='q' AND at<=?
                AND (dep IS NULL OR NOT EXISTS(
                    SELECT 1 FROM json_each(tasks.dep) d
                    JOIN tasks t2 ON t2.id = d.value
                    WHERE t2.s != 'd'
                ))
              ORDER BY p DESC, at, id
              LIMIT 1
            """, (now_ms(),)).fetchone()
            if not row: return None
            try:
                claimed = self.c.execute(
                    "UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q' RETURNING id, cmd",
                    (worker_id, now_ms(), row["id"])
                ).fetchone()
                if claimed: return {"id": claimed["id"], "cmd": claimed["cmd"]}
            except sqlite3.OperationalError:
                self.c.execute("BEGIN IMMEDIATE")
                n = self.c.execute("UPDATE tasks SET s='r', w=?, st=? WHERE id=? AND s='q'",
                                   (worker_id, now_ms(), row["id"])).rowcount
                self.c.execute("COMMIT")
                if n: return {"id": row["id"], "cmd": row["cmd"]}
        return None
    def done(self, tid:int, ok:bool, result:Any=None, error:str=None, worker_id:str=""):
        with self.lock:
            t = self.c.execute("SELECT ct, st FROM tasks WHERE id=? AND (w=? OR w IS NULL)", (tid, worker_id)).fetchone()
            if not t: return
            tnow = now_ms()
            if ok:
                self.c.execute("UPDATE tasks SET s='d', et=?, res=?, w=NULL WHERE id=?",
                               (tnow, json.dumps(result)[:2000] if result else None, tid))
                if t["st"]:
                    qt = (t["st"] - t["ct"]) / 1000.0; et = (tnow - t["st"]) / 1000.0
                    self.c.execute("INSERT OR REPLACE INTO metrics(task_id,qt,et) VALUES(?,?,?)", (tid, qt, et))
            else:
                row = self.c.execute("SELECT r FROM tasks WHERE id=?", (tid,)).fetchone()
                rc = (row["r"] if row else 0)
                if rc < 3:
                    delay = 1000 * (2 ** rc)
                    self.c.execute("UPDATE tasks SET s='q', at=?, r=r+1, e=?, w=NULL WHERE id=?",
                                   (now_ms()+delay, error, tid))
                else:
                    self.c.execute("UPDATE tasks SET s='f', et=?, e=?, w=NULL WHERE id=?", (tnow, error, tid))
    def reclaim(self, timeout_ms:int=300000) -> int:
        with self.lock:
            cutoff = now_ms() - timeout_ms
            return self.c.execute("UPDATE tasks SET s='q', w=NULL, r=r+1 WHERE s='r' AND st < ?", (cutoff,)).rowcount
    def stats(self) -> Dict[str,Any]:
        with self.lock:
            counts = {r["s"]: r["c"] for r in self.c.execute("SELECT s, COUNT(*) c FROM tasks GROUP BY s")}
            perf = self.c.execute("SELECT AVG(qt) avg_qt, AVG(et) avg_et, MAX(qt) max_qt, MAX(et) max_et FROM metrics").fetchone()
            wal = self.c.execute("PRAGMA wal_checkpoint(PASSIVE)").fetchone()
            return {"tasks": counts, "perf": dict(perf) if perf else {}, "wal_pages": wal[1] if wal else 0}
    def cleanup(self, days:int=7) -> int:
        cutoff = now_ms() - days*86400000
        with self.lock:
            n = self.c.execute("DELETE FROM tasks WHERE s IN ('d','f') AND et < ?", (cutoff,)).rowcount
            pc = self.c.execute("PRAGMA page_count").fetchone()[0]
            fr = self.c.execute("PRAGMA freelist_count").fetchone()[0]
            if fr > pc * 0.3: self.c.execute("VACUUM")
            return n

# ---------- systemd orchestration ----------
def systemd_start(store:Store, task:sqlite3.Row) -> tuple[bool,str,str]:
    unit = unit_name(task["name"] or task["id"])
    props = ["--property=StandardOutput=journal",
             "--property=StandardError=journal",
             "--property=KillMode=control-group"]
    if task["cwd"]:  props += [f"--property=WorkingDirectory={task['cwd']}"]
    if task["nice"] is not None: props += [f"--property=Nice={int(task['nice'])}"]
    if task["rtprio"] is not None:
        props += ["--property=CPUSchedulingPolicy=rr",
                  f"--property=CPUSchedulingPriority={int(task['rtprio'])}"]
    if task["slice"]: props += [f"--slice={task['slice']}"]
    if task["cpu_weight"]: props += [f"--property=CPUWeight={int(task['cpu_weight'])}"]
    if task["mem_max_mb"]: props += [f"--property=MemoryMax={int(task['mem_max_mb'])}M"]
    env = []
    if task["env"]:
        for k,v in json.loads(task["env"]).items():
            env += ["--setenv", f"{k}={v}"]
    when = []
    if task["schedule"]: when += ["--on-calendar", task["schedule"]]
    cmd = [*SYSDRUN, "--unit", unit, *props, *env, *when, "--", "/bin/sh", "-lc", task["cmd"]]
    cp = run(cmd)
    store.update(task["id"], unit=unit, s=("sched" if task["schedule"] else "start"))
    msg = cp.stderr.strip() or cp.stdout.strip()
    return (cp.returncode==0, unit, msg)

def systemd_stop(store:Store, name_or_id:str):
    unit = unit_name(name_or_id)
    run(SYSTEMCTL + ["stop", unit])
    run(SYSTEMCTL + ["stop", unit.replace(".service",".timer")])
    # status will be reconciled later

def systemd_status(name_or_id:str) -> Dict[str,str]:
    unit = unit_name(name_or_id)
    cp = run(SYSTEMCTL + ["show", unit, "--property=ActiveState", "--property=Result", "--property=MainPID"])
    if cp.returncode != 0: return {"unit": unit, "active": "unknown"}
    m = {}
    for line in cp.stdout.splitlines():
        if "=" in line:
            k,v = line.split("=",1); m[k]=v
    return {"unit": unit, "active": m.get("ActiveState"), "result": m.get("Result"), "pid": m.get("MainPID")}

def reconcile_systemd(store:Store):
    rows = store.list()
    for r in rows:
        if r["mode"]!="systemd" or not r["unit"]: continue
        cp = run(SYSTEMCTL + ["show", r["unit"], "--property=ActiveState", "--property=Result"])
        if cp.returncode!=0: continue
        m = dict(x.split("=",1) for x in cp.stdout.splitlines() if "=" in x)
        active, result = m.get("ActiveState"), m.get("Result")
        if active in ("inactive","failed"):
            store.update(r["id"], s=("d" if result=="success" else "f"), et=now_ms())

# ---------- local worker ----------
class Worker:
    def __init__(self, store:Store, wid:Optional[str]=None):
        self.db = store
        self.wid = wid or f"w{os.getpid()}"
        self.run_flag = True
        signal.signal(signal.SIGTERM, self._stop)
        signal.signal(signal.SIGINT,  self._stop)
    def _stop(self, *_): self.run_flag = False
    def loop(self, batch:int=1, idle_ms:int=50, timeout_s:int=290):
        print(f"Worker {self.wid} running (batch={batch})")
        tick = 0
        while self.run_flag:
            tick += 1
            if tick % 100 == 0:
                r = self.db.reclaim()
                if r: print(f"reclaimed {r} stalled")
            got = []
            for _ in range(batch):
                item = self.db.pop(self.wid)
                if item: got.append(item)
            if not got:
                time.sleep(idle_ms/1000.0); continue
            for t in got:
                if not self.run_flag: break
                try:
                    proc = subprocess.run(t["cmd"], shell=True, capture_output=True, text=True, timeout=timeout_s)
                    ok = (proc.returncode==0)
                    self.db.done(t["id"], ok,
                                 result={"stdout": proc.stdout[:1000], "stderr": proc.stderr[:1000]},
                                 error=(proc.stderr if not ok else None),
                                 worker_id=self.wid)
                except subprocess.TimeoutExpired:
                    self.db.done(t["id"], False, error="TIMEOUT", worker_id=self.wid)
                except Exception as e:
                    self.db.done(t["id"], False, error=str(e), worker_id=self.wid)

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(description="AIOS unified orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)

    # add task (works for both modes)
    pa = sub.add_parser("add", help="add task (local default)"); 
    pa.add_argument("cmd"); pa.add_argument("args", nargs="*")
    pa.add_argument("--name")
    pa.add_argument("--mode", choices=["local","systemd"], default="local")
    pa.add_argument("--priority", type=int, default=0)
    pa.add_argument("--delay-ms", type=int, default=0, help="for local 'at'")
    pa.add_argument("--deps", help="JSON list of task IDs (local)")
    pa.add_argument("--env", action="append", default=[], help="KEY=VAL")
    pa.add_argument("--cwd")
    pa.add_argument("--on-calendar", help="systemd OnCalendar (systemd mode)")
    pa.add_argument("--rtprio", type=int)
    pa.add_argument("--nice", type=int)
    pa.add_argument("--slice")
    pa.add_argument("--cpu-weight", type=int)
    pa.add_argument("--mem-max-mb", type=int)
    pa.add_argument("--start", action="store_true", help="start now (systemd mode)")

    # worker
    pw = sub.add_parser("worker", help="run local worker")
    pw.add_argument("--batch", type=int, default=1)
    pw.add_argument("--idle-ms", type=int, default=50)
    pw.add_argument("--timeout-s", type=int, default=290)

    # start/stop/status for named/id (systemd preferred)
    ps = sub.add_parser("start", help="start systemd task by name/id"); ps.add_argument("name_or_id")
    pk = sub.add_parser("stop",  help="stop systemd task by name/id");  pk.add_argument("name_or_id")
    pz = sub.add_parser("status",help="status by name/id");             pz.add_argument("name_or_id")

    # list/stats/cleanup/reconcile
    sub.add_parser("list", help="list tasks")
    sub.add_parser("stats", help="aggregate stats")
    pc = sub.add_parser("cleanup", help="delete old done/failed"); pc.add_argument("--days", type=int, default=7)
    sub.add_parser("reconcile", help="refresh systemd task statuses")

    args = ap.parse_args()
    db = Store()

    if args.cmd == "add":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        cmd = args.cmd + (" " + " ".join(shlex.quote(a) for a in args.args) if args.args else "")
        at = (now_ms() + args.delay_ms) if args.mode=="local" and args.delay_ms>0 else now_ms()
        dep = args.deps if not args.deps else json.dumps(json.loads(args.deps))
        tid = db.add(
            name=args.name, cmd=cmd, mode=args.mode, p=args.priority, s=("q" if args.mode=="local" else "q"),
            at=at, dep=dep, env=(json.dumps(env) if env else None), cwd=args.cwd,
            schedule=args.on_calendar, rtprio=args.rtprio, nice=args.nice, slice=args.slice,
            cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb
        )
        if args.mode=="systemd" and (args.start or args.on_calendar):
            row = db.get_by_id(tid)
            ok, unit, msg = systemd_start(db, row)
            print(unit if ok else f"ERROR: {msg}")
        else:
            print(f"queued task {tid}")

    elif args.cmd == "worker":
        Worker(db).loop(batch=args.batch, idle_ms=args.idle_ms, timeout_s=args.timeout_s)

    elif args.cmd == "start":
        # allow numeric id or name
        row = db.get_by_id(int(args.name_or_id)) if args.name_or_id.isdigit() else db.get_by_name(args.name_or_id)
        if not row: sys.exit("unknown task")
        if row["mode"]!="systemd": sys.exit("task is not systemd mode")
        ok, unit, msg = systemd_start(db, row)
        print(unit if ok else f"ERROR: {msg}")

    elif args.cmd == "stop":
        systemd_stop(db, args.name_or_id); print("stopped")

    elif args.cmd == "status":
        st = systemd_status(args.name_or_id); print(json.dumps(st, indent=2))

    elif args.cmd == "list":
        reconcile_systemd(db)
        for r in db.list():
            ident = r["name"] or r["id"]
            sched = r["schedule"] or "-"
            unit  = r["unit"] or "-"
            print(f"{ident:20} id={r['id']:4} mode={r['mode']:7} s={r['s']:5} p={r['p']:2} at={r['at']} unit={unit} sched={sched}")

    elif args.cmd == "stats":
        print(json.dumps(db.stats(), indent=2))

    elif args.cmd == "cleanup":
        n = db.cleanup(args.days); print(f"deleted {n}")

    elif args.cmd == "reconcile":
        reconcile_systemd(db); print("ok")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: claude.py
================================================================================

#!/usr/bin/env python3
"""
Pure Python Job Orchestrator - No subprocess calls
Uses multiprocessing for job execution with full orchestration features
"""
import sqlite3, json, time, sys, os, signal, threading, traceback
from multiprocessing import Process, Queue, Manager
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Dict, Any, List, Callable
from pathlib import Path
import importlib.util
import pickle

# Database configuration
DB_PATH = Path.home() / ".pyorch.db"

# Optimized SQLite pragmas
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL", 
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
]

class JobRegistry:
    """Registry for Python callable jobs"""
    _jobs = {}
    
    @classmethod
    def register(cls, name: str, func: Callable):
        """Register a Python function as a job"""
        cls._jobs[name] = func
    
    @classmethod
    def get(cls, name: str) -> Optional[Callable]:
        return cls._jobs.get(name)
    
    @classmethod
    def load_module(cls, path: str):
        """Load jobs from a Python module"""
        spec = importlib.util.spec_from_file_location("jobs", path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        for name in dir(module):
            obj = getattr(module, name)
            if callable(obj) and not name.startswith('_'):
                cls.register(name, obj)

class Orchestrator:
    """Pure Python job orchestrator with no subprocess usage"""
    
    def __init__(self, db_path: Path = DB_PATH, max_workers: int = 4):
        self.db_path = db_path
        self.max_workers = max_workers
        self.processes = {}
        self.manager = Manager()
        self.result_queue = self.manager.Queue()
        self.lock = threading.RLock()
        self._init_db()
        
    def _init_db(self):
        """Initialize database"""
        conn = self._get_conn()
        
        for pragma in PRAGMAS:
            conn.execute(pragma)
            
        conn.executescript("""
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                func_name TEXT NOT NULL,
                args TEXT,
                kwargs TEXT,
                
                -- Scheduling
                priority INT DEFAULT 0,
                scheduled_at INT DEFAULT 0,
                schedule_pattern TEXT,
                
                -- State
                status TEXT DEFAULT 'pending' CHECK(status IN 
                    ('pending','running','completed','failed','cancelled')),
                pid INT,
                
                -- Execution
                retry_count INT DEFAULT 0,
                max_retries INT DEFAULT 3,
                timeout_sec INT DEFAULT 300,
                error TEXT,
                result TEXT,
                traceback TEXT,
                
                -- Dependencies
                depends_on TEXT,
                
                -- Resource limits
                memory_limit_mb INT,
                cpu_affinity TEXT,
                
                -- Timestamps
                created_at INT DEFAULT (strftime('%s','now')*1000),
                started_at INT,
                completed_at INT
            );
            
            CREATE INDEX IF NOT EXISTS idx_queue ON jobs(status, priority DESC, scheduled_at, id)
                WHERE status IN ('pending','running');
                
            CREATE INDEX IF NOT EXISTS idx_deps ON jobs(depends_on) 
                WHERE depends_on IS NOT NULL;
            
            -- Metrics
            CREATE TABLE IF NOT EXISTS metrics (
                job_id INTEGER PRIMARY KEY,
                queue_time REAL,
                exec_time REAL,
                memory_peak_mb REAL,
                FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
            );
            
            -- Job outputs/artifacts
            CREATE TABLE IF NOT EXISTS artifacts (
                id INTEGER PRIMARY KEY,
                job_id INTEGER,
                key TEXT,
                value BLOB,
                created_at INT DEFAULT (strftime('%s','now')*1000),
                FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
            );
        """)
        conn.close()
    
    def _get_conn(self) -> sqlite3.Connection:
        """Get a new database connection"""
        conn = sqlite3.connect(self.db_path, isolation_level=None)
        conn.row_factory = sqlite3.Row
        return conn
    
    def add_job(self, name: str, func_name: str, args: tuple = None, 
                kwargs: dict = None, priority: int = 0, 
                depends_on: List[int] = None, scheduled_at: int = None,
                max_retries: int = 3, timeout_sec: int = 300,
                schedule_pattern: str = None) -> int:
        """Add a job to the queue"""
        conn = self._get_conn()
        scheduled_at = scheduled_at or int(time.time() * 1000)
        
        cursor = conn.execute("""
            INSERT INTO jobs (name, func_name, args, kwargs, priority, 
                depends_on, scheduled_at, max_retries, timeout_sec, schedule_pattern)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            name, func_name,
            json.dumps(args) if args else None,
            json.dumps(kwargs) if kwargs else None,
            priority,
            json.dumps(depends_on) if depends_on else None,
            scheduled_at, max_retries, timeout_sec, schedule_pattern
        ))
        
        job_id = cursor.lastrowid
        conn.close()
        return job_id
    
    def _check_dependencies(self, conn: sqlite3.Connection, job_id: int) -> bool:
        """Check if all dependencies are completed"""
        row = conn.execute(
            "SELECT depends_on FROM jobs WHERE id=?", (job_id,)
        ).fetchone()
        
        if not row or not row['depends_on']:
            return True
            
        deps = json.loads(row['depends_on'])
        incomplete = conn.execute("""
            SELECT COUNT(*) as c FROM jobs 
            WHERE id IN ({}) AND status != 'completed'
        """.format(','.join('?' * len(deps))), deps).fetchone()
        
        return incomplete['c'] == 0
    
    def _worker_process(self, job_id: int, func_name: str, args: tuple, 
                       kwargs: dict, result_queue: Queue, timeout: int):
        """Worker process that executes a job"""
        try:
            # Set up signal handling for timeout
            def timeout_handler(signum, frame):
                raise TimeoutError(f"Job exceeded {timeout}s timeout")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout)
            
            # Get the function
            func = JobRegistry.get(func_name)
            if not func:
                # Try to import it as module.function
                if '.' in func_name:
                    module_name, func_name = func_name.rsplit('.', 1)
                    module = importlib.import_module(module_name)
                    func = getattr(module, func_name)
                else:
                    raise ValueError(f"Function {func_name} not found in registry")
            
            # Execute the function
            result = func(*args, **kwargs) if args or kwargs else func()
            
            signal.alarm(0)  # Cancel timeout
            result_queue.put((job_id, True, result, None))
            
        except Exception as e:
            signal.alarm(0)
            result_queue.put((job_id, False, None, {
                'error': str(e),
                'traceback': traceback.format_exc()
            }))
    
    def execute_job(self, job: Dict) -> bool:
        """Execute a job in a separate process"""
        job_id = job['id']
        
        # Parse arguments
        args = json.loads(job['args']) if job['args'] else ()
        kwargs = json.loads(job['kwargs']) if job['kwargs'] else {}
        
        # Create process
        p = Process(
            target=self._worker_process,
            args=(job_id, job['func_name'], args, kwargs, 
                  self.result_queue, job.get('timeout_sec', 300))
        )
        
        # Start process
        p.start()
        self.processes[job_id] = p
        
        # Update job status
        conn = self._get_conn()
        conn.execute(
            "UPDATE jobs SET status='running', pid=?, started_at=? WHERE id=?",
            (p.pid, int(time.time() * 1000), job_id)
        )
        conn.close()
        
        return True
    
    def get_next_job(self) -> Optional[Dict]:
        """Get the next eligible job"""
        conn = self._get_conn()
        now = int(time.time() * 1000)
        
        with self.lock:
            eligible = conn.execute("""
                SELECT * FROM jobs 
                WHERE status='pending' AND scheduled_at <= ?
                ORDER BY priority DESC, scheduled_at, id
                LIMIT 10
            """, (now,)).fetchall()
            
            for job in eligible:
                if self._check_dependencies(conn, job['id']):
                    # Claim the job
                    conn.execute(
                        "UPDATE jobs SET status='running' WHERE id=? AND status='pending'",
                        (job['id'],)
                    )
                    conn.close()
                    return dict(job)
            
            conn.close()
            return None
    
    def complete_job(self, job_id: int, success: bool, result: Any = None, 
                    error_info: Dict = None):
        """Mark a job as completed"""
        conn = self._get_conn()
        now = int(time.time() * 1000)
        
        if success:
            conn.execute("""
                UPDATE jobs 
                SET status='completed', completed_at=?, result=?, pid=NULL
                WHERE id=?
            """, (now, json.dumps(result) if result else None, job_id))
            
            # Record metrics
            job = conn.execute(
                "SELECT created_at, started_at FROM jobs WHERE id=?", (job_id,)
            ).fetchone()
            
            if job and job['started_at']:
                queue_time = (job['started_at'] - job['created_at']) / 1000.0
                exec_time = (now - job['started_at']) / 1000.0
                
                conn.execute("""
                    INSERT OR REPLACE INTO metrics (job_id, queue_time, exec_time)
                    VALUES (?, ?, ?)
                """, (job_id, queue_time, exec_time))
        else:
            # Check retry logic
            job = conn.execute(
                "SELECT retry_count, max_retries FROM jobs WHERE id=?", (job_id,)
            ).fetchone()
            
            if job and job['retry_count'] < job['max_retries']:
                # Retry with exponential backoff
                delay = 1000 * (2 ** job['retry_count'])
                conn.execute("""
                    UPDATE jobs 
                    SET status='pending', scheduled_at=?, retry_count=retry_count+1,
                        error=?, traceback=?, pid=NULL
                    WHERE id=?
                """, (now + delay, error_info.get('error'), 
                     error_info.get('traceback'), job_id))
            else:
                conn.execute("""
                    UPDATE jobs 
                    SET status='failed', completed_at=?, error=?, traceback=?, pid=NULL
                    WHERE id=?
                """, (now, error_info.get('error'), 
                     error_info.get('traceback'), job_id))
        
        # Clean up process
        if job_id in self.processes:
            del self.processes[job_id]
        
        conn.close()
    
    def process_results(self):
        """Process results from the result queue"""
        while not self.result_queue.empty():
            try:
                job_id, success, result, error = self.result_queue.get_nowait()
                self.complete_job(job_id, success, result, error)
            except:
                break
    
    def reclaim_stalled(self, timeout_ms: int = 300000) -> int:
        """Reclaim jobs that have been running too long"""
        conn = self._get_conn()
        cutoff = int(time.time() * 1000) - timeout_ms
        
        stalled = conn.execute("""
            SELECT id, pid FROM jobs 
            WHERE status='running' AND started_at < ?
        """, (cutoff,)).fetchall()
        
        for job in stalled:
            # Terminate process if still running
            if job['id'] in self.processes:
                self.processes[job['id']].terminate()
                del self.processes[job['id']]
            
            # Reset job for retry
            conn.execute("""
                UPDATE jobs 
                SET status='pending', pid=NULL, retry_count=retry_count+1
                WHERE id=?
            """, (job['id'],))
        
        conn.close()
        return len(stalled)
    
    def schedule_recurring(self):
        """Handle recurring scheduled jobs"""
        conn = self._get_conn()
        
        # Find completed recurring jobs that need rescheduling
        recurring = conn.execute("""
            SELECT id, name, func_name, args, kwargs, priority, 
                   schedule_pattern, max_retries, timeout_sec
            FROM jobs 
            WHERE status='completed' AND schedule_pattern IS NOT NULL
        """).fetchall()
        
        for job in recurring:
            # Calculate next run time (simplified - would need cron parser)
            next_run = int(time.time() * 1000) + 86400000  # Next day
            
            # Create new instance
            self.add_job(
                f"{job['name']}_recurring",
                job['func_name'],
                json.loads(job['args']) if job['args'] else None,
                json.loads(job['kwargs']) if job['kwargs'] else None,
                job['priority'],
                scheduled_at=next_run,
                max_retries=job['max_retries'],
                timeout_sec=job['timeout_sec'],
                schedule_pattern=job['schedule_pattern']
            )
        
        conn.close()
    
    def stats(self) -> Dict[str, Any]:
        """Get orchestrator statistics"""
        conn = self._get_conn()
        
        # Job counts
        counts = {row['status']: row['c'] for row in conn.execute(
            "SELECT status, COUNT(*) c FROM jobs GROUP BY status"
        )}
        
        # Performance metrics
        perf = conn.execute("""
            SELECT AVG(queue_time) avg_qt, AVG(exec_time) avg_et,
                   MAX(queue_time) max_qt, MAX(exec_time) max_et,
                   COUNT(*) total
            FROM metrics
        """).fetchone()
        
        conn.close()
        
        return {
            'jobs': counts,
            'performance': dict(perf) if perf else {},
            'active_processes': len(self.processes),
            'max_workers': self.max_workers
        }
    
    def cleanup(self, days: int = 7) -> int:
        """Remove old completed jobs"""
        conn = self._get_conn()
        cutoff = int(time.time() * 1000) - (days * 86400000)
        
        deleted = conn.execute("""
            DELETE FROM jobs 
            WHERE status IN ('completed', 'failed') AND completed_at < ?
        """, (cutoff,)).rowcount
        
        conn.execute("VACUUM")
        conn.close()
        
        return deleted
    
    def save_artifact(self, job_id: int, key: str, value: Any):
        """Save job output artifact"""
        conn = self._get_conn()
        conn.execute(
            "INSERT INTO artifacts (job_id, key, value) VALUES (?, ?, ?)",
            (job_id, key, pickle.dumps(value))
        )
        conn.close()
    
    def get_artifact(self, job_id: int, key: str) -> Any:
        """Retrieve job output artifact"""
        conn = self._get_conn()
        row = conn.execute(
            "SELECT value FROM artifacts WHERE job_id=? AND key=?",
            (job_id, key)
        ).fetchone()
        conn.close()
        
        return pickle.loads(row['value']) if row else None

class Scheduler:
    """Main scheduler that coordinates job execution"""
    
    def __init__(self, orchestrator: Orchestrator):
        self.orch = orchestrator
        self.running = True
        self.executor = ThreadPoolExecutor(max_workers=orchestrator.max_workers)
        
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)
    
    def _shutdown(self, *_):
        self.running = False
        print("\nScheduler shutting down...")
        
        # Terminate all processes
        for p in self.orch.processes.values():
            p.terminate()
    
    def run(self):
        """Main scheduler loop"""
        print(f"Scheduler started (max_workers={self.orch.max_workers})")
        maintenance_counter = 0
        
        while self.running:
            maintenance_counter += 1
            
            # Process results
            self.orch.process_results()
            
            # Periodic maintenance
            if maintenance_counter % 50 == 0:
                reclaimed = self.orch.reclaim_stalled()
                if reclaimed:
                    print(f"Reclaimed {reclaimed} stalled jobs")
                
                self.orch.schedule_recurring()
            
            # Check worker capacity
            if len(self.orch.processes) < self.orch.max_workers:
                job = self.orch.get_next_job()
                
                if job:
                    print(f"Executing job {job['id']}: {job['name']}")
                    self.orch.execute_job(job)
            
            time.sleep(0.1)
        
        self.executor.shutdown(wait=True)

# Example job functions
def example_job(x: int, y: int) -> int:
    """Example job that adds two numbers"""
    time.sleep(1)  # Simulate work
    return x + y

def failing_job():
    """Example job that fails"""
    raise ValueError("This job always fails")

def long_job(duration: int = 10):
    """Example long-running job"""
    time.sleep(duration)
    return f"Slept for {duration} seconds"

# Register example jobs
JobRegistry.register("add", example_job)
JobRegistry.register("fail", failing_job)  
JobRegistry.register("sleep", long_job)

def main():
    """CLI interface"""
    if len(sys.argv) < 2:
        print("""Usage:
  add <name> <func> [--args ...] [--kwargs ...] [--priority N] [--depends ID,ID]
  scheduler [--workers N]
  list
  stats
  cleanup [--days N]
  load <module.py>  # Load job functions from module
        """)
        sys.exit(1)
    
    cmd = sys.argv[1]
    orch = Orchestrator()
    
    if cmd == 'add':
        if len(sys.argv) < 4:
            print("Usage: add <name> <func_name> [options]")
            sys.exit(1)
        
        name = sys.argv[2]
        func_name = sys.argv[3]
        
        # Parse arguments
        kwargs = {'name': name, 'func_name': func_name}
        i = 4
        
        while i < len(sys.argv):
            if sys.argv[i] == '--args':
                args = []
                i += 1
                while i < len(sys.argv) and not sys.argv[i].startswith('--'):
                    # Try to parse as number
                    try:
                        args.append(int(sys.argv[i]))
                    except:
                        args.append(sys.argv[i])
                    i += 1
                kwargs['args'] = tuple(args)
            elif sys.argv[i] == '--priority':
                kwargs['priority'] = int(sys.argv[i + 1])
                i += 2
            elif sys.argv[i] == '--depends':
                kwargs['depends_on'] = [int(x) for x in sys.argv[i + 1].split(',')]
                i += 2
            else:
                i += 1
        
        job_id = orch.add_job(**kwargs)
        print(f"Added job {job_id}")
    
    elif cmd == 'scheduler':
        workers = 4
        if len(sys.argv) > 2 and sys.argv[2] == '--workers':
            workers = int(sys.argv[3])
        
        orch.max_workers = workers
        scheduler = Scheduler(orch)
        scheduler.run()
    
    elif cmd == 'list':
        conn = orch._get_conn()
        jobs = conn.execute("""
            SELECT id, name, func_name, status, priority, created_at 
            FROM jobs ORDER BY created_at DESC LIMIT 50
        """).fetchall()
        
        for job in jobs:
            created = time.strftime('%Y-%m-%d %H:%M:%S', 
                                   time.localtime(job['created_at'] / 1000))
            print(f"[{job['id']}] {job['name']} ({job['func_name']}): "
                  f"{job['status']} (pri={job['priority']}, created={created})")
        
        conn.close()
    
    elif cmd == 'stats':
        stats = orch.stats()
        print(json.dumps(stats, indent=2))
    
    elif cmd == 'cleanup':
        days = int(sys.argv[2]) if len(sys.argv) > 2 else 7
        deleted = orch.cleanup(days)
        print(f"Deleted {deleted} old jobs")
    
    elif cmd == 'load':
        if len(sys.argv) < 3:
            print("Usage: load <module.py>")
            sys.exit(1)
        
        JobRegistry.load_module(sys.argv[2])
        print(f"Loaded jobs from {sys.argv[2]}")
        print(f"Available jobs: {list(JobRegistry._jobs.keys())}")
    
    else:
        print(f"Unknown command: {cmd}")
        sys.exit(1)

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: deepseek.py
================================================================================

#!/usr/bin/env python3
"""
Unified Job Orchestration System
Combines task queue management with systemd integration for production deployment
"""
import sqlite3, subprocess, json, time, sys, os, signal, threading, argparse
from typing import Optional, Dict, Any, List
from pathlib import Path

# Optimized database settings
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL", 
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
]

class JobOrchestrator:
    """Unified job manager with systemd integration"""
    
    def __init__(self, db_path: str = "jobs.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        
        for pragma in PRAGMAS:
            self.conn.execute(pragma)
            
        self._init_schema()
    
    def _init_schema(self):
        """Initialize unified database schema"""
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE,
                command TEXT NOT NULL,
                args TEXT DEFAULT '[]',
                env TEXT,
                working_dir TEXT,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'queued',  -- queued, running, completed, failed
                schedule TEXT,  -- systemd calendar format or NULL for immediate
                worker_id TEXT,
                retries INTEGER DEFAULT 0,
                error_msg TEXT,
                result TEXT,
                created_at INTEGER DEFAULT (strftime('%s','now')),
                started_at INTEGER,
                completed_at INTEGER,
                dependencies TEXT,  -- JSON array of job names
                systemd_props TEXT  -- JSON of systemd properties
            );
            
            CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs(status, priority DESC, created_at);
            CREATE INDEX IF NOT EXISTS idx_jobs_schedule ON jobs(schedule) WHERE schedule IS NOT NULL;
        """)
    
    def add_job(self, name: str, command: str, args: List[str] = None, 
                env: Dict[str, str] = None, working_dir: str = None,
                priority: int = 0, schedule: str = None, 
                dependencies: List[str] = None, **systemd_props) -> int:
        """Add a job with optional systemd properties"""
        with self.lock:
            return self.conn.execute(
                """INSERT INTO jobs (name, command, args, env, working_dir, 
                   priority, schedule, dependencies, systemd_props) 
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (name, command, json.dumps(args or []), json.dumps(env) if env else None,
                 working_dir, priority, schedule, json.dumps(dependencies) if dependencies else None,
                 json.dumps(systemd_props) if systemd_props else None)
            ).lastrowid
    
    def get_pending_jobs(self, worker_id: str, limit: int = 1) -> List[Dict[str, Any]]:
        """Get jobs ready for execution with dependency resolution"""
        with self.lock:
            now = int(time.time())
            jobs = self.conn.execute("""
                SELECT id, name, command, args, env, working_dir, systemd_props 
                FROM jobs 
                WHERE status = 'queued' AND (schedule IS NULL OR schedule <= ?)
                AND (dependencies IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(jobs.dependencies) AS dep
                    JOIN jobs AS dep_job ON dep_job.name = dep.value
                    WHERE dep_job.status != 'completed'
                ))
                ORDER BY priority DESC, created_at
                LIMIT ?
            """, (now, limit)).fetchall()
            
            # Atomically claim jobs
            job_ids = [job['id'] for job in jobs]
            if job_ids:
                self.conn.execute(
                    f"UPDATE jobs SET status='running', worker_id=?, started_at=? WHERE id IN ({','.join('?'*len(job_ids))})",
                    [worker_id, now] + job_ids
                )
            
            return [dict(job) for job in jobs]
    
    def complete_job(self, job_id: int, success: bool = True, 
                    result: Any = None, error: str = None):
        """Mark job as completed or failed with retry logic"""
        with self.lock:
            job = self.conn.execute(
                "SELECT retries FROM jobs WHERE id=?", (job_id,)
            ).fetchone()
            
            if not job:
                return
                
            now = int(time.time())
            if success:
                self.conn.execute(
                    "UPDATE jobs SET status='completed', completed_at=?, result=? WHERE id=?",
                    (now, json.dumps(result) if result else None, job_id)
                )
            else:
                if job['retries'] < 3:
                    # Exponential backoff
                    delay = 60 * (2 ** job['retries'])  # 1, 2, 4 minutes
                    self.conn.execute(
                        "UPDATE jobs SET status='queued', retries=retries+1, error_msg=? WHERE id=?",
                        (error, job_id)
                    )
                else:
                    self.conn.execute(
                        "UPDATE jobs SET status='failed', completed_at=?, error_msg=? WHERE id=?",
                        (now, error, job_id)
                    )
    
    def reconcile_systemd_jobs(self):
        """Update status of systemd-managed jobs"""
        with self.lock:
            running_jobs = self.conn.execute(
                "SELECT id, name FROM jobs WHERE status='running' AND schedule IS NOT NULL"
            ).fetchall()
            
            for job in running_jobs:
                # Check systemd unit status
                unit_status = self._get_systemd_status(job['name'])
                if unit_status and unit_status.get('ActiveState') == 'inactive':
                    result = 'success' if unit_status.get('Result') == 'success' else 'failed'
                    self.complete_job(job['id'], result == 'success')
    
    def _get_systemd_status(self, job_name: str) -> Optional[Dict[str, str]]:
        """Get systemd unit status"""
        try:
            result = subprocess.run([
                'systemctl', '--user', 'show', f"aios-{job_name}.service",
                '--property=ActiveState,Result,MainPID'
            ], capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                return dict(line.split('=', 1) for line in result.stdout.strip().split('\n') if '=')
        except:
            pass
        return None
    
    def start_systemd_job(self, job: Dict[str, Any]) -> bool:
        """Start a job as a systemd transient unit"""
        try:
            cmd = [
                'systemd-run', '--user', '--collect', '--quiet',
                '--unit', f"aios-{job['name']}.service",
                '--property=StandardOutput=journal',
                '--property=StandardError=journal',
            ]
            
            # Add systemd properties
            props = json.loads(job.get('systemd_props', '{}'))
            if props.get('nice'):
                cmd.extend(['--nice', str(props['nice'])])
            if props.get('working_dir'):
                cmd.extend(['--working-directory', props['working_dir']])
            
            # Add environment variables
            env = json.loads(job.get('env', '{}'))
            for k, v in env.items():
                cmd.extend(['--setenv', f"{k}={v}"])
            
            # Add command and arguments
            cmd.append(job['command'])
            cmd.extend(json.loads(job.get('args', '[]')))
            
            result = subprocess.run(cmd, timeout=10)
            return result.returncode == 0
        except:
            return False
    
    def cleanup_old_jobs(self, days: int = 7):
        """Remove old completed jobs"""
        cutoff = int(time.time()) - (days * 86400)
        with self.lock:
            deleted = self.conn.execute(
                "DELETE FROM jobs WHERE status IN ('completed', 'failed') AND completed_at < ?",
                (cutoff,)
            ).rowcount
            
            # Vacuum if significantly fragmented
            if deleted > 100:
                self.conn.execute("VACUUM")
            
            return deleted
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        with self.lock:
            counts = dict(self.conn.execute(
                "SELECT status, COUNT(*) FROM jobs GROUP BY status"
            ).fetchall())
            
            performance = self.conn.execute("""
                SELECT AVG(completed_at - started_at) as avg_duration,
                       MAX(completed_at - started_at) as max_duration
                FROM jobs WHERE status = 'completed'
            """).fetchone()
            
            return {
                'job_counts': counts,
                'performance': dict(performance) if performance else {},
                'total_jobs': sum(counts.values())
            }

class Worker:
    """Unified job worker"""
    
    def __init__(self, orchestrator: JobOrchestrator, worker_id: str = None):
        self.orchestrator = orchestrator
        self.worker_id = worker_id or f"worker-{os.getpid()}-{int(time.time())}"
        self.running = True
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)
    
    def _shutdown(self, signum, frame):
        self.running = False
    
    def run(self, batch_size: int = 1):
        """Main worker loop"""
        print(f"Worker {self.worker_id} started (batch_size={batch_size})")
        
        while self.running:
            try:
                # Reconcile systemd jobs periodically
                self.orchestrator.reconcile_systemd_jobs()
                
                # Get pending jobs
                jobs = self.orchestrator.get_pending_jobs(self.worker_id, batch_size)
                
                if not jobs:
                    time.sleep(1)
                    continue
                
                for job in jobs:
                    if not self.running:
                        break
                    
                    success = self._execute_job(job)
                    self.orchestrator.complete_job(
                        job['id'], 
                        success, 
                        error=None if success else "Execution failed"
                    )
                    
            except Exception as e:
                print(f"Worker error: {e}")
                time.sleep(5)
    
    def _execute_job(self, job: Dict[str, Any]) -> bool:
        """Execute a single job"""
        if job.get('schedule'):
            # Systemd scheduled job
            return self.orchestrator.start_systemd_job(job)
        else:
            # Immediate execution
            try:
                args = json.loads(job.get('args', '[]'))
                env = os.environ.copy()
                env.update(json.loads(job.get('env', '{}')))
                
                result = subprocess.run(
                    [job['command']] + args,
                    env=env,
                    cwd=job.get('working_dir'),
                    timeout=300,
                    capture_output=True,
                    text=True
                )
                
                return result.returncode == 0
            except Exception as e:
                print(f"Job execution failed: {e}")
                return False

def main():
    parser = argparse.ArgumentParser(description="Unified Job Orchestrator")
    subparsers = parser.add_subparsers(dest='command', required=True)
    
    # Add job command
    add_parser = subparsers.add_parser('add', help='Add a new job')
    add_parser.add_argument('name', help='Job name')
    add_parser.add_argument('command', help='Command to execute')
    add_parser.add_argument('args', nargs='*', help='Command arguments')
    add_parser.add_argument('--env', nargs='*', help='Environment variables (KEY=VALUE)')
    add_parser.add_argument('--working-dir', help='Working directory')
    add_parser.add_argument('--priority', type=int, default=0, help='Job priority')
    add_parser.add_argument('--schedule', help='Systemd calendar schedule')
    add_parser.add_argument('--dependencies', nargs='*', help='Job dependencies')
    add_parser.add_argument('--nice', type=int, help='Nice level')
    
    # Worker command
    worker_parser = subparsers.add_parser('worker', help='Start worker')
    worker_parser.add_argument('--batch-size', type=int, default=1, help='Batch size')
    worker_parser.add_argument('--worker-id', help='Worker identifier')
    
    # Management commands
    subparsers.add_parser('stats', help='Show statistics')
    subparsers.add_parser('reconcile', help='Reconcile systemd jobs')
    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup old jobs')
    cleanup_parser.add_argument('--days', type=int, default=7, help='Days to keep')
    
    args = parser.parse_args()
    orchestrator = JobOrchestrator()
    
    if args.command == 'add':
        env_dict = {}
        if args.env:
            for env_var in args.env:
                k, v = env_var.split('=', 1)
                env_dict[k] = v
        
        systemd_props = {}
        if args.nice:
            systemd_props['nice'] = args.nice
        if args.working_dir:
            systemd_props['working_dir'] = args.working_dir
            
        job_id = orchestrator.add_job(
            name=args.name,
            command=args.command,
            args=args.args,
            env=env_dict,
            working_dir=args.working_dir,
            priority=args.priority,
            schedule=args.schedule,
            dependencies=args.dependencies,
            **systemd_props
        )
        print(f"Added job {job_id}: {args.name}")
        
    elif args.command == 'worker':
        worker = Worker(orchestrator, args.worker_id)
        worker.run(args.batch_size)
        
    elif args.command == 'stats':
        stats = orchestrator.get_stats()
        print(json.dumps(stats, indent=2))
        
    elif args.command == 'reconcile':
        orchestrator.reconcile_systemd_jobs()
        print("Systemd jobs reconciled")
        
    elif args.command == 'cleanup':
        deleted = orchestrator.cleanup_old_jobs(args.days)
        print(f"Cleaned up {deleted} old jobs")

if __name__ == '__main__':
    main()

================================================================================

================================================================================
FILE: geminiDeep.py
================================================================================

#!/usr/bin/env python3
"""
Integrated Job Orchestrator: High-Performance Queue + Asynchronous Systemd Execution
"""
import sqlite3, subprocess, json, time, sys, os, signal, threading, argparse, shutil
from typing import Optional, Dict, Any, List
from pathlib import Path

# --- Configuration ---
DB_PATH = Path.home() / ".orchestrator.db"
MAX_RETRIES = 3
UNIT_PREFIX = "orch-"

# Optimized SQLite pragmas
PRAGMAS = [
    "PRAGMA journal_mode=WAL", "PRAGMA synchronous=NORMAL",
    "PRAGMA busy_timeout=5000", "PRAGMA temp_store=MEMORY"
]

# Systemd configuration - Use user scope if not root
USE_USER = (os.geteuid() != 0)
SYSTEMD_ARGS = ["--user"] if USE_USER else []
# systemd-run without --collect returns immediately (Asynchronous)
SYSDRUN = ["systemd-run", *SYSTEMD_ARGS, "--quiet"]
SYSTEMCTL = ["systemctl", *SYSTEMD_ARGS]

# --- Task Queue Logic ---

class TaskQueue:
    """Thread-safe, high-performance queue with dependencies and resource management."""

    def __init__(self, db_path=DB_PATH):
        # Persistent connection, isolation_level=None for speed, manual locking
        self.conn = sqlite3.connect(str(db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.lock = threading.RLock()
        self._init_db()

    def _init_db(self):
        with self.lock:
            for pragma in PRAGMAS:
                self.conn.execute(pragma)

            # Unified Schema. S (Status): Q=queued R=running D=done F=failed
            self.conn.executescript("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id INTEGER PRIMARY KEY,
                    cmd TEXT NOT NULL,
                    P INTEGER DEFAULT 0,    -- Priority
                    S TEXT DEFAULT 'Q',     -- Status
                    AT INTEGER DEFAULT 0,   -- Scheduled At (ms)
                    R INTEGER DEFAULT 0,    -- Retries
                    DEP TEXT,               -- JSON dependencies
                    CONF TEXT,              -- JSON resource config {nice, cpu, mem}
                    UNIT TEXT,              -- Systemd Unit Name
                    ERR TEXT,               -- Error snippet
                    CT INTEGER DEFAULT (CAST(strftime('%s', 'now') AS INTEGER) * 1000), -- Created
                    ST INTEGER, ET INTEGER  -- Start/End time
                );
                -- Optimized index for fetching tasks
                CREATE INDEX IF NOT EXISTS idx_fetch ON tasks(S, P DESC, AT, id)
                WHERE S = 'Q';

                -- Metrics table
                CREATE TABLE IF NOT EXISTS metrics (
                    task_id INTEGER PRIMARY KEY, qt_s REAL, et_s REAL,
                    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
                );
            """)

    def add(self, cmd: str, priority: int = 0, delay_s: int = 0, deps: Optional[list] = None, config: Optional[dict] = None) -> int:
        """Add a task to the queue."""
        at = int(time.time() * 1000) + (delay_s * 1000)
        dep_json = json.dumps(deps) if deps else None
        conf_json = json.dumps(config) if config else None

        with self.lock:
            return self.conn.execute(
                "INSERT INTO tasks(cmd, P, AT, DEP, CONF) VALUES(?,?,?,?,?)",
                (cmd, priority, at, dep_json, conf_json)
            ).lastrowid

    def pop(self) -> Optional[Dict[str, Any]]:
        """Atomically claim the next eligible task, respecting dependencies."""
        now = int(time.time() * 1000)

        # Query finds the highest priority, oldest scheduled task where dependencies are 'D'one.
        query = """
            SELECT id FROM tasks t
            WHERE S='Q' AND AT<=:now
            AND (DEP IS NULL OR NOT EXISTS (
                SELECT 1 FROM json_each(t.DEP) AS d
                JOIN tasks AS dt ON dt.id = d.value
                WHERE dt.S != 'D'
            ))
            ORDER BY P DESC, AT, id
            LIMIT 1
        """

        with self.lock:
            # 1. Find the task ID
            row = self.conn.execute(query, {'now': now}).fetchone()
            if not row:
                return None

            # 2. Atomically claim it using UPDATE...RETURNING (prevents race conditions)
            try:
                result = self.conn.execute("""
                    UPDATE tasks SET S='R', ST=:now
                    WHERE id=:id AND S='Q'
                    RETURNING id, cmd, CONF
                """, {'now': now, 'id': row['id']}).fetchone()

                if result:
                    task = dict(result)
                    task['CONF'] = json.loads(task['CONF']) if task['CONF'] else {}
                    return task
            except sqlite3.OperationalError:
                 # Handle older SQLite versions if necessary (omitted for brevity)
                 pass
            return None

    def update_running(self, task_id: int, unit_name: str):
        """Associate the systemd unit with the task."""
        with self.lock:
            self.conn.execute("UPDATE tasks SET UNIT=? WHERE id=?", (unit_name, task_id))

    def finalize(self, task_id: int, success: bool, error: str = None):
        """Mark task as complete (D/F) or schedule retry (Q), and record metrics."""
        now = int(time.time() * 1000)

        with self.lock:
            task = self.conn.execute("SELECT R, CT, ST FROM tasks WHERE id=?", (task_id,)).fetchone()
            if not task: return

            if success:
                self.conn.execute("UPDATE tasks SET S='D', ET=?, UNIT=NULL WHERE id=?", (now, task_id))
                self._record_metrics(task_id, task, now)
            elif task['R'] < MAX_RETRIES:
                # Exponential backoff (1s, 2s, 4s...)
                delay = 1000 * (2 ** task['R'])
                self.conn.execute(
                    "UPDATE tasks SET S='Q', AT=?, R=R+1, ERR=?, UNIT=NULL WHERE id=?",
                    (now + delay, error, task_id)
                )
            else:
                self.conn.execute(
                    "UPDATE tasks SET S='F', ET=?, ERR=?, UNIT=NULL WHERE id=?",
                    (now, error, task_id)
                )
                self._record_metrics(task_id, task, now)

    def _record_metrics(self, task_id, task, now):
        if task['ST'] and task['CT']:
            qt = (task['ST'] - task['CT']) / 1000.0
            et = (now - task['ST']) / 1000.0
            self.conn.execute(
                "INSERT OR REPLACE INTO metrics(task_id, qt_s, et_s) VALUES(?,?,?)",
                (task_id, qt, et)
            )

    def get_running_units(self) -> Dict[str, int]:
        """Get map of unit names to task IDs for running tasks."""
        with self.lock:
            rows = self.conn.execute("SELECT id, UNIT FROM tasks WHERE S='R' AND UNIT IS NOT NULL").fetchall()
            return {row['UNIT']: row['id'] for row in rows}

    def stats(self) -> Dict[str, Any]:
        with self.lock:
            counts = {r['S']: r['C'] for r in self.conn.execute("SELECT S, COUNT(*) C FROM tasks GROUP BY S")}
            perf = self.conn.execute("SELECT AVG(qt_s) avg_qt, AVG(et_s) avg_et, COUNT(*) count FROM metrics").fetchone()
            return {'tasks': counts, 'perf_s': dict(perf) if perf and perf['avg_qt'] is not None else {}}

# --- Executor Logic ---

class AsyncExecutor:
    """Pulls tasks and executes them asynchronously using systemd-run."""

    def __init__(self, queue: TaskQueue):
        self.queue = queue
        self.running = True

    def launch_task(self, task: Dict[str, Any]):
        """Launch a single task using systemd-run with resource controls."""
        task_id = task['id']
        config = task['CONF']
        # Unique unit name ensures clean runs, especially for retries
        unit_name = f"{UNIT_PREFIX}{task_id}-{int(time.time())}.service"

        props = ["--property=KillMode=control-group"]
        
        # Apply resource controls (Integration from Program 2)
        if config.get('nice') is not None:
            props.append(f"--property=Nice={int(config['nice'])}")
        if config.get('cpu_weight'):
             props.append(f"--property=CPUWeight={int(config['cpu_weight'])}")
        if config.get('mem_max_mb'):
             props.append(f"--property=MemoryMax={int(config['mem_max_mb'])}M")

        # Wrap command in 'sh -c' for shell interpretation
        cmd_to_run = ["sh", "-c", task['cmd']]
        run_cmd = [*SYSDRUN, "--unit", unit_name, *props, "--", *cmd_to_run]

        # Launch the command (returns immediately)
        cp = subprocess.run(run_cmd, text=True, capture_output=True)

        if cp.returncode == 0:
            # Successfully launched, update DB so reconciler can track it
            self.queue.update_running(task_id, unit_name)
        else:
            # Failed to launch (e.g., systemd issue)
            error_msg = f"Launch failed: {cp.stderr.strip()[:500]}"
            self.queue.finalize(task_id, False, error=error_msg)

    def reconcile(self):
        """Check the status of running systemd units and update the database."""
        unit_map = self.queue.get_running_units()
        if not unit_map:
            return 0

        # Efficiently query systemd for multiple units at once
        cmd = [*SYSTEMCTL, "show", *unit_map.keys(), "--property=Id,ActiveState,Result"]
        cp = subprocess.run(cmd, text=True, capture_output=True)

        if cp.returncode != 0:
            print(f"Warning: Reconciliation failed: {cp.stderr}", file=sys.stderr)
            return 0

        # Parse the output (blocks separated by empty lines)
        reconciled_count = 0
        for block in cp.stdout.strip().split('\n\n'):
            props = dict(line.split('=', 1) for line in block.splitlines() if '=' in line)
            unit_id = props.get("Id")
            task_id = unit_map.get(unit_id)

            if not task_id: continue

            # Check if the unit has finished
            if props.get("ActiveState") in ("inactive", "failed"):
                success = (props.get("Result") == "success")
                error = None if success else f"Systemd Result={props.get('Result')}"
                self.queue.finalize(task_id, success, error)
                reconciled_count += 1
        return reconciled_count

    def run_loop(self, poll_interval=0.5):
        """Main processing loop."""
        print(f"Executor started. Systemd scope: {'User' if USE_USER else 'System'}")
        self._setup_signals()
        reconcile_timer = time.time()

        while self.running:
            # 1. Periodic Reconciliation (e.g., every 5 seconds)
            if time.time() - reconcile_timer > 5:
                try:
                    count = self.reconcile()
                    if count > 0: print(f"Reconciled {count} tasks.")
                except Exception as e:
                    print(f"Error during reconciliation: {e}", file=sys.stderr)
                reconcile_timer = time.time()

            # 2. Launch new tasks
            try:
                task = self.queue.pop()
                if task:
                    self.launch_task(task)
                else:
                    time.sleep(poll_interval)
            except Exception as e:
                print(f"Error in executor loop: {e}", file=sys.stderr)
                time.sleep(5) # Back off if DB is failing

        print("Executor shutting down.")

    def _setup_signals(self):
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)

    def _shutdown(self, *_):
        self.running = False

# --- CLI and Main ---

def main():
    parser = argparse.ArgumentParser(description="Integrated Job Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Add Task
    add_p = subparsers.add_parser("add", help="Add a new task")
    add_p.add_argument("cmd", help="The command to execute (shell string)")
    add_p.add_argument("-p", "--priority", type=int, default=0)
    add_p.add_argument("--delay", type=int, default=0, help="Delay in seconds")
    add_p.add_argument("--deps", help="Comma-separated list of dependency task IDs")
    # Resource controls
    add_p.add_argument("--nice", type=int)
    add_p.add_argument("--cpu-weight", type=int)
    add_p.add_argument("--mem-max-mb", type=int)

    # Run Executor
    run_p = subparsers.add_parser("run", help="Start the task executor/worker")
    run_p.add_argument("--poll", type=float, default=0.5)

    subparsers.add_parser("stats", help="Show queue statistics")

    # Display help if no command is provided
    if len(sys.argv) == 1:
        parser.print_help(sys.stderr)
        sys.exit(1)
        
    args = parser.parse_args()
    q = TaskQueue()

    if args.command == "add":
        try:
            deps = [int(d) for d in args.deps.split(',')] if args.deps else None
        except ValueError:
            sys.exit("Error: Dependencies must be comma-separated integers.")
        
        config = {}
        if args.nice is not None: config['nice'] = args.nice
        if args.cpu_weight: config['cpu_weight'] = args.cpu_weight
        if args.mem_max_mb: config['mem_max_mb'] = args.mem_max_mb

        task_id = q.add(args.cmd, args.priority, args.delay, deps, config)
        print(f"Added Task ID: {task_id}")

    elif args.command == "run":
        if not shutil.which("systemd-run"):
            sys.exit("Error: systemd-run not found. This orchestrator requires systemd.")
        executor = AsyncExecutor(q)
        executor.run_loop(args.poll)

    elif args.command == "stats":
        print(json.dumps(q.stats(), indent=2))

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: geminiStudio.py
================================================================================

#!/usr/bin/env python3
"""
job_orchestrator.py: A unified job orchestration and scheduling system.
Integrates direct process management with optional systemd execution for robustness.
"""
import argparse
import json
import os
import shlex
import sqlite3
import subprocess
import sys
import time
from typing import Any, Dict, List, Optional

# --- Database Setup and Management ---

def get_db_connection(db_path: str = "orchestrator.db") -> sqlite3.Connection:
    """Establishes and configures the SQLite database connection."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    # Optimizations from claudeCodeD
    conn.executescript("""
        PRAGMA journal_mode=WAL;
        PRAGMA synchronous=NORMAL;
        PRAGMA cache_size=-8000;
        PRAGMA temp_store=MEMORY;
        PRAGMA busy_timeout=5000;
    """)
    return conn

def initialize_schema(conn: sqlite3.Connection):
    """Initializes the database schema."""
    conn.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE NOT NULL,
            command TEXT NOT NULL,
            args TEXT DEFAULT '[]',
            status TEXT DEFAULT 'queued',
            priority INTEGER DEFAULT 0,
            scheduled_at INTEGER,
            worker_id TEXT,
            retry_count INTEGER DEFAULT 0,
            error_message TEXT,
            result TEXT,
            created_at INTEGER NOT NULL,
            started_at INTEGER,
            ended_at INTEGER,
            use_systemd BOOLEAN DEFAULT FALSE
        )
    """)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_status_priority ON jobs(status, priority DESC, scheduled_at)")

# --- Core Orchestrator Class ---

class JobOrchestrator:
    """Manages job lifecycle, from creation to execution and completion."""

    def __init__(self, db_path: str = "orchestrator.db"):
        self.conn = get_db_connection(db_path)
        initialize_schema(self.conn)

    def add_job(
        self,
        name: str,
        command: str,
        args: Optional[List[str]] = None,
        priority: int = 0,
        delay_ms: int = 0,
        use_systemd: bool = False,
    ) -> int:
        """Adds a new job to the database."""
        current_time = int(time.time() * 1000)
        scheduled_at = current_time + delay_ms if delay_ms > 0 else current_time
        args_json = json.dumps(args or [])

        cursor = self.conn.cursor()
        cursor.execute(
            "INSERT INTO jobs (name, command, args, priority, scheduled_at, use_systemd, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
            (name, command, args_json, priority, scheduled_at, use_systemd, current_time),
        )
        self.conn.commit()
        return cursor.lastrowid

    def list_jobs(self):
        """Lists all jobs in the database."""
        cursor = self.conn.execute("SELECT id, name, command, status, use_systemd FROM jobs ORDER BY created_at DESC")
        return cursor.fetchall()

    def get_next_job(self, worker_id: str) -> Optional[Dict[str, Any]]:
        """Atomically retrieves and locks the next available job."""
        now = int(time.time() * 1000)
        cursor = self.conn.cursor()

        # Find a suitable job
        cursor.execute(
            """
            SELECT id FROM jobs
            WHERE status = 'queued' AND scheduled_at <= ?
            ORDER BY priority DESC, scheduled_at
            LIMIT 1
            """,
            (now,),
        )
        job_row = cursor.fetchone()
        if not job_row:
            return None

        job_id = job_row['id']

        # Atomically claim it
        cursor.execute(
            """
            UPDATE jobs
            SET status = 'running', worker_id = ?, started_at = ?
            WHERE id = ? AND status = 'queued'
            """,
            (worker_id, now, job_id),
        )
        self.conn.commit()

        if cursor.rowcount > 0:
            job = self.conn.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            return dict(job)
        return None

    def finalize_job(
        self,
        job_id: int,
        success: bool,
        result: Optional[str] = None,
        error: Optional[str] = None,
    ):
        """Marks a job as done or failed."""
        status = 'done' if success else 'failed'
        ended_at = int(time.time() * 1000)
        self.conn.execute(
            "UPDATE jobs SET status = ?, result = ?, error_message = ?, ended_at = ?, worker_id = NULL WHERE id = ?",
            (status, result, error, ended_at, job_id),
        )
        self.conn.commit()

# --- Worker and Execution Logic ---

class Worker:
    """A worker that executes jobs."""

    def __init__(self, orchestrator: JobOrchestrator, worker_id: str):
        self.orchestrator = orchestrator
        self.worker_id = worker_id

    def run_once(self):
        """Fetches and runs a single job."""
        job = self.orchestrator.get_next_job(self.worker_id)
        if not job:
            print("No available jobs.")
            return

        print(f"Executing job {job['name']} (ID: {job['id']})...")
        command = [job['command']] + json.loads(job['args'])

        try:
            if job['use_systemd']:
                self._run_with_systemd(job, command)
            else:
                self._run_with_subprocess(job, command)
        except Exception as e:
            self.orchestrator.finalize_job(job['id'], success=False, error=str(e))
            print(f"An unexpected error occurred while running job {job['name']}: {e}")

    def _run_with_systemd(self, job: Dict[str, Any], command: List[str]):
        """Executes a job using systemd-run."""
        unit_name = f"orchestrator-{job['name']}.service"
        systemd_command = [
            "systemd-run",
            "--user",
            "--collect",
            "--quiet",
            f"--unit={unit_name}",
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
        ] + command

        result = subprocess.run(systemd_command, capture_output=True, text=True)

        if result.returncode == 0:
            self.orchestrator.finalize_job(job['id'], success=True, result=f"Systemd unit '{unit_name}' started.")
        else:
            self.orchestrator.finalize_job(job['id'], success=False, error=result.stderr)

    def _run_with_subprocess(self, job: Dict[str, Any], command: List[str]):
        """Executes a job using a direct subprocess call."""
        try:
            process = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=300,  # 5-minute timeout
                check=True,
            )
            output = {
                "stdout": process.stdout[:1000],
                "stderr": process.stderr[:1000],
            }
            self.orchestrator.finalize_job(job['id'], success=True, result=json.dumps(output))
        except subprocess.CalledProcessError as e:
            error_output = {
                "stdout": e.stdout[:1000],
                "stderr": e.stderr[:1000],
                "returncode": e.returncode,
            }
            self.orchestrator.finalize_job(job['id'], success=False, error=json.dumps(error_output))
        except subprocess.TimeoutExpired:
            self.orchestrator.finalize_job(job['id'], success=False, error="Job timed out after 300 seconds.")

# --- Command-Line Interface ---

def main():
    """Main function to handle command-line arguments."""
    parser = argparse.ArgumentParser(description="A unified job orchestrator.")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # 'add' command
    add_parser = subparsers.add_parser("add", help="Add a new job.")
    add_parser.add_argument("name", help="A unique name for the job.")
    add_parser.add_argument("cmd", help="The command to execute.")
    add_parser.add_argument("args", nargs="*", help="Arguments for the command.")
    add_parser.add_argument("--priority", type=int, default=0, help="Job priority (higher is sooner).")
    add_parser.add_argument("--delay", type=int, default=0, help="Delay execution by N milliseconds.")
    add_parser.add_argument("--systemd", action="store_true", help="Execute the job via systemd-run.")

    # 'list' command
    subparsers.add_parser("list", help="List all jobs.")

    # 'worker' command
    worker_parser = subparsers.add_parser("worker", help="Run a worker to execute a single job.")
    worker_parser.add_argument("--id", default=f"worker-{os.getpid()}", help="A unique ID for the worker.")

    args = parser.parse_args()
    orchestrator = JobOrchestrator()

    if args.command == "add":
        try:
            job_id = orchestrator.add_job(
                name=args.name,
                command=args.cmd,
                args=args.args,
                priority=args.priority,
                delay_ms=args.delay,
                use_systemd=args.systemd,
            )
            print(f"Job '{args.name}' added with ID: {job_id}")
        except sqlite3.IntegrityError:
            print(f"Error: A job with the name '{args.name}' already exists.", file=sys.stderr)
            sys.exit(1)

    elif args.command == "list":
        jobs = orchestrator.list_jobs()
        if not jobs:
            print("No jobs found.")
            return

        print(f"{'ID':<5} {'Name':<20} {'Status':<10} {'Systemd':<8} {'Command'}")
        print("-" * 70)
        for job in jobs:
            print(f"{job['id']:<5} {job['name']:<20} {job['status']:<10} {'Yes' if job['use_systemd'] else 'No':<8} {job['command']}")

    elif args.command == "worker":
        worker = Worker(orchestrator, args.id)
        worker.run_once()

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: geminiWeb.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Orchestrator: Synthesized Production Model

- Uses a high-performance SQLite queue for task state.
- Delegates execution and reaping to systemd via transient units.
- Combines the best patterns from claudeCodeD and aios_systemd_orchestrator.py.
"""
import argparse
import json
import os
import shlex
import signal
import sqlite3
import subprocess
import sys
import threading
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

# --- Configuration ---
DB_PATH = Path.home() / ".aios/orchestrator.db"
UNIT_PREFIX = "aios-task-"

# PRAGMAS from claudeCodeD for high-performance WAL mode
PRAGMAS = """
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA cache_size=-8000;
PRAGMA temp_store=MEMORY;
PRAGMA busy_timeout=5000;
PRAGMA wal_autocheckpoint=1000;
"""


class Orchestrator:
    """Manages task state in SQLite and orchestrates execution via systemd."""

    def __init__(self, db_path=DB_PATH):
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._init_db()

    def _init_db(self):
        """Initializes the database schema."""
        self.conn.executescript(PRAGMAS + """
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                cmd TEXT NOT NULL,
                prio INT DEFAULT 0,
                status TEXT DEFAULT 'pending', -- pending, running, completed, failed
                at INT, -- scheduled_at (ms)
                retries INT DEFAULT 0,
                props TEXT, -- JSON blob for systemd properties (rt, mem, etc.)
                deps TEXT,  -- JSON array of dependency IDs
                unit_name TEXT,
                created_at INT,
                started_at INT,
                completed_at INT,
                result TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_status_prio ON tasks(status, prio DESC, at);
        """)

    def _sh(self, cmd: List[str]) -> subprocess.CompletedProcess:
        """Helper for running shell commands."""
        return subprocess.run(cmd, text=True, capture_output=True, check=False)

    def add(self, name: str, cmd: str, **kwargs) -> Optional[int]:
        """Adds a task to the queue."""
        props = {
            "rtprio": kwargs.get("rtprio"),
            "mem_mb": kwargs.get("mem_mb"),
            "cpu_w": kwargs.get("cpu_w")
        }
        try:
            cursor = self.conn.execute(
                """INSERT INTO tasks (name, cmd, prio, at, deps, props, created_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    name,
                    cmd,
                    kwargs.get("prio", 0),
                    kwargs.get("at", int(time.time() * 1000)),
                    json.dumps(kwargs.get("deps") or []),
                    json.dumps({k: v for k, v in props.items() if v is not None}),
                    int(time.time() * 1000)
                )
            )
            return cursor.lastrowid
        except sqlite3.IntegrityError:
            print(f"Error: Task name '{name}' already exists.", file=sys.stderr)
            return None

    def _get_next(self) -> Optional[sqlite3.Row]:
        """Atomically get the next runnable task."""
        now = int(time.time() * 1000)
        try:
            # Atomically find, update, and return the next task
            cursor = self.conn.execute("""
                UPDATE tasks SET status='running', started_at=? WHERE id = (
                    SELECT id FROM tasks
                    WHERE status='pending' AND at <= ?
                    AND (deps IS NULL OR deps = '[]' OR NOT EXISTS (
                        SELECT 1 FROM json_each(tasks.deps) AS d
                        JOIN tasks AS dt ON dt.id = d.value
                        WHERE dt.status != 'completed'
                    ))
                    ORDER BY prio DESC, at ASC LIMIT 1
                ) RETURNING *
            """, (now, now))
            return cursor.fetchone()
        except sqlite3.OperationalError:  # Fallback for older SQLite versions
            with self.conn:
                row = self.conn.execute(
                    "SELECT id FROM tasks WHERE status='pending' AND at <= ? ORDER BY prio DESC, at ASC LIMIT 1",
                    (now,)
                ).fetchone()
                if not row: return None
                self.conn.execute("UPDATE tasks SET status='running', started_at=? WHERE id=?", (now, row['id']))
                return self.conn.execute("SELECT * FROM tasks WHERE id=?", (row['id'],)).fetchone()

    def _finalize_task(self, task_id: int, success: bool):
        """Mark a task done or requeue for retry."""
        task = self.conn.execute("SELECT retries FROM tasks WHERE id=?", (task_id,)).fetchone()
        if not task: return

        if success:
            self.conn.execute(
                "UPDATE tasks SET status='completed', completed_at=? WHERE id=?",
                (int(time.time() * 1000), task_id)
            )
        else:  # Retry logic
            retries = task['retries']
            if retries < 3:
                delay_ms = 1000 * (2 ** retries)
                self.conn.execute(
                    "UPDATE tasks SET status='pending', retries=retries+1, at=? WHERE id=?",
                    (int(time.time() * 1000) + delay_ms, task_id)
                )
            else:
                self.conn.execute(
                    "UPDATE tasks SET status='failed', completed_at=? WHERE id=?",
                    (int(time.time() * 1000), task_id)
                )

    def _run_with_systemd(self, task: sqlite3.Row) -> Optional[str]:
        """Launch a task using systemd-run."""
        unit_name = f"{UNIT_PREFIX}{task['name']}-{task['id']}.service"
        props = json.loads(task['props'] or '{}')
        
        cmd = ["systemd-run", "--user", "--collect", "--quiet", "--unit", unit_name]
        
        # Add properties
        if props.get("rtprio"):
            cmd += [f"--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={props['rtprio']}"]
        if props.get("mem_mb"):
            cmd += [f"--property=MemoryMax={props['mem_mb']}M"]
        if props.get("cpu_w"):
            cmd += [f"--property=CPUWeight={props['cpu_w']}"]
        
        cmd.extend(["--property=StandardOutput=journal", "--property=StandardError=journal",
                    "--", "/bin/sh", "-c", task['cmd']])

        result = self._sh(cmd)
        if result.returncode == 0:
            self.conn.execute("UPDATE tasks SET unit_name=? WHERE id=?", (unit_name, task['id']))
            return unit_name
        else:
            print(f"Error launching systemd unit for task {task['id']}: {result.stderr}", file=sys.stderr)
            self._finalize_task(task['id'], success=False)
            return None

    def run_worker(self, max_concurrent: int = os.cpu_count()):
        """Main worker loop to manage the task queue and systemd units."""
        print(f"Worker started (PID: {os.getpid()}). Max concurrent jobs: {max_concurrent}")
        running_tasks: Dict[int, str] = {}  # {task_id: unit_name}
        shutdown = threading.Event()

        def _handle_signal(*_):
            print("\nShutdown signal received. Finishing running tasks...")
            shutdown.set()
        signal.signal(signal.SIGINT, _handle_signal)
        signal.signal(signal.SIGTERM, _handle_signal)

        while not shutdown.is_set():
            # 1. Monitor running tasks
            for task_id, unit_name in list(running_tasks.items()):
                res = self._sh(["systemctl", "--user", "show", unit_name, "--property=ActiveState,Result"])
                status = {k: v for k, v in (line.split("=", 1) for line in res.stdout.splitlines() if "=" in line)}
                
                if status.get("ActiveState") in ("inactive", "failed"):
                    success = status.get("Result") == "success"
                    print(f"Task {task_id} ('{unit_name}') finished. Success: {success}")
                    self._finalize_task(task_id, success)
                    del running_tasks[task_id]

            # 2. Launch new tasks if capacity allows
            while len(running_tasks) < max_concurrent:
                if shutdown.is_set(): break
                task = self._get_next()
                if not task:
                    break  # No more pending tasks
                
                print(f"Launching task {task['id']}: {task['name']}")
                unit_name = self._run_with_systemd(task)
                if unit_name:
                    running_tasks[task['id']] = unit_name
                else:  # Failed to launch
                    break  # Avoid rapid-fire launch failures
            
            time.sleep(1)  # Poll interval
        print("Worker shutdown complete.")

    def get_stats(self) -> Dict[str, int]:
        """Returns a count of tasks by status."""
        return {r['status']: r['count'] for r in self.conn.execute(
            "SELECT status, COUNT(*) as count FROM tasks GROUP BY status"
        )}

def generate_systemd_unit() -> str:
    """Generate a systemd service file for the worker daemon."""
    return f"""[Unit]
Description=AIOS Orchestrator Worker
After=network.target

[Service]
Type=simple
ExecStart={sys.executable} {os.path.abspath(__file__)} worker
Restart=always
RestartSec=3
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=default.target
"""

def main():
    """Defines and handles the command-line interface."""
    parser = argparse.ArgumentParser(description="AIOS Systemd Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Worker command
    worker_parser = subparsers.add_parser("worker", help="Run a worker process.")
    worker_parser.add_argument("concurrency", type=int, nargs="?", default=os.cpu_count(),
                               help="Max number of concurrent jobs.")

    # Add command
    add_parser = subparsers.add_parser("add", help="Add a new task.")
    add_parser.add_argument("name", help="Unique name for the task.")
    add_parser.add_argument("command", help="The shell command to execute.")
    add_parser.add_argument("--prio", type=int, default=0, help="Task priority (higher is sooner).")
    add_parser.add_argument("--delay_ms", type=int, default=0, help="Delay execution by milliseconds.")
    add_parser.add_argument("--deps", type=json.loads, help="JSON list of task IDs it depends on, e.g., '[1, 2]'.")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority (1-99).")
    add_parser.add_argument("--mem_mb", type=int, help="Memory limit in MB.")
    add_parser.add_argument("--cpu_w", type=int, help="CPU weight (1-10000).")

    # Stats and Install commands
    subparsers.add_parser("stats", help="Show queue statistics.")
    subparsers.add_parser("install", help="Generate a systemd unit file for the worker.")

    args = parser.parse_args()
    orch = Orchestrator()

    if args.command == "worker":
        orch.run_worker(max_concurrent=args.concurrency)
    elif args.command == "add":
        at = int(time.time() * 1000) + args.delay_ms
        task_id = orch.add(args.name, args.command, prio=args.prio, at=at, deps=args.deps,
                           rtprio=args.rtprio, mem_mb=args.mem_mb, cpu_w=args.cpu_w)
        if task_id:
            print(f"Task '{args.name}' added with ID {task_id}.")
    elif args.command == "stats":
        print(json.dumps(orch.get_stats(), indent=2))
    elif args.command == "install":
        print(generate_systemd_unit())

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: glm.py
================================================================================



```python
#!/usr/bin/env python3
"""
Unified Job Orchestration System
Combines task queue management with systemd orchestration
"""
import argparse, json, os, sqlite3, subprocess, sys, time, threading
from pathlib import Path
from typing import Optional, Dict, Any, List

# Database configuration
DB = Path.home() / ".orchestrator.db"
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA mmap_size=268435456",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

# Systemd commands
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

class Orchestrator:
    """Unified job orchestration system"""
    
    def __init__(self, db_path=DB):
        self.db_path = db_path
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Initialize database with optimized schema"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            # Apply performance pragmas
            for pragma in PRAGMAS:
                conn.execute(pragma)
            
            # Unified schema combining both programs
            conn.execute("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE,
                    cmd TEXT NOT NULL,
                    args TEXT,
                    env TEXT,
                    cwd TEXT,
                    priority INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'queued',
                    created_at INTEGER DEFAULT (strftime('%s','now')*1000),
                    started_at INTEGER,
                    ended_at INTEGER,
                    worker_id TEXT,
                    retry_count INTEGER DEFAULT 0,
                    error_message TEXT,
                    result TEXT,
                    schedule TEXT,
                    unit TEXT,
                    rtprio INTEGER,
                    nice INTEGER,
                    slice TEXT,
                    cpu_weight INTEGER,
                    mem_max_mb INTEGER,
                    dependencies TEXT
                )
            """)
            
            # Optimized index for job selection
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_jobs_status_priority 
                ON jobs(status, priority DESC, created_at, id)
                WHERE status IN ('queued', 'running')
            """)
            
            # Metrics table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS metrics (
                    job_id INTEGER PRIMARY KEY,
                    queue_time REAL,
                    exec_time REAL,
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                )
            """)
    
    def add_job(self, name: str, cmd: str, args: List[str] = None, 
                env: Dict[str, str] = None, cwd: str = None, 
                priority: int = 0, schedule: str = None,
                rtprio: int = None, nice: int = None,
                slice: str = None, cpu_weight: int = None,
                mem_max_mb: int = None, dependencies: List[int] = None) -> int:
        """Add a new job to the system"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute("""
                INSERT INTO jobs (
                    name, cmd, args, env, cwd, priority, schedule,
                    rtprio, nice, slice, cpu_weight, mem_max_mb, dependencies
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                name, cmd, json.dumps(args or []), json.dumps(env or {}), 
                cwd, priority, schedule, rtprio, nice, slice,
                cpu_weight, mem_max_mb, json.dumps(dependencies or [])
            ))
            return conn.execute("SELECT last_insert_rowid()").fetchone()[0]
    
    def get_job(self, job_id: int = None, name: str = None) -> Optional[Dict]:
        """Get job by ID or name"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            if job_id:
                row = conn.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            elif name:
                row = conn.execute("SELECT * FROM jobs WHERE name = ?", (name,)).fetchone()
            else:
                return None
                
            return dict(row) if row else None
    
    def start_job_systemd(self, job_id: int) -> bool:
        """Start a job using systemd"""
        job = self.get_job(job_id)
        if not job:
            return False
        
        unit = f"orch-{job['name'] or job['id']}.service"
        props = [
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",
        ]
        
        # Add resource control properties
        if job.get('rtprio'):
            props.extend([
                "--property=CPUSchedulingPolicy=rr",
                f"--property=CPUSchedulingPriority={job['rtprio']}"
            ])
        if job.get('nice') is not None:
            props.append(f"--property=Nice={job['nice']}")
        if job.get('slice'):
            props.append(f"--slice={job['slice']}")
        if job.get('cpu_weight'):
            props.append(f"--property=CPUWeight={job['cpu_weight']}")
        if job.get('mem_max_mb'):
            props.append(f"--property=MemoryMax={job['mem_max_mb']}M")
        
        # Add environment variables
        env = []
        if job.get('env'):
            for k, v in json.loads(job['env']).items():
                env.extend(["--setenv", f"{k}={v}"])
        
        # Add schedule if provided
        schedule = []
        if job.get('schedule'):
            schedule.extend(["--on-calendar", job['schedule']])
        
        # Add working directory
        if job.get('cwd'):
            props.append(f"--property=WorkingDirectory={job['cwd']}")
        
        # Build command
        cmd = [
            *SYSDRUN, "--unit", unit, *props, *env, *schedule,
            "--", job['cmd'], *json.loads(job.get('args') or '[]')
        ]
        
        # Execute systemd-run
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                # Update job status
                with sqlite3.connect(self.db_path, isolation_level=None) as conn:
                    conn.execute(
                        "UPDATE jobs SET unit=?, status=? WHERE id=?",
                        (unit, "scheduled" if job.get('schedule') else "running", job_id)
                    )
                return True
            return False
        except Exception:
            return False
    
    def pop_job(self, worker_id: str) -> Optional[Dict]:
        """Get next available job for worker execution"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            # Find next eligible job
            row = conn.execute("""
                SELECT id, cmd, args FROM jobs
                WHERE status = 'queued'
                AND (schedule IS NULL OR schedule <= ?)
                AND (dependencies IS NULL OR dependencies = '[]')
                ORDER BY priority DESC, created_at, id
                LIMIT 1
            """, (now,)).fetchone()
            
            if not row:
                return None
            
            # Try to claim the job
            try:
                result = conn.execute("""
                    UPDATE jobs 
                    SET status = 'running', worker_id = ?, started_at = ?
                    WHERE id = ? AND status = 'queued'
                    RETURNING id, cmd, args
                """, (worker_id, now, row['id'])).fetchone()
                
                if result:
                    return {
                        'id': result['id'],
                        'cmd': result['cmd'],
                        'args': json.loads(result['args'] or '[]')
                    }
                return None
            except sqlite3.OperationalError:
                # Fallback for older SQLite
                conn.execute("BEGIN IMMEDIATE")
                updated = conn.execute(
                    "UPDATE jobs SET status = 'running', worker_id = ?, started_at = ? "
                    "WHERE id = ? AND status = 'queued'",
                    (worker_id, now, row['id'])
                ).rowcount
                conn.execute("COMMIT")
                
                if updated:
                    return {
                        'id': row['id'],
                        'cmd': row['cmd'],
                        'args': json.loads(row['args'] or '[]')
                    }
                return None
    
    def complete_job(self, job_id: int, success: bool = True, 
                     result: Any = None, error: str = None) -> bool:
        """Mark a job as completed"""
        now = int(time.time() * 1000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            # Get job details
            job = conn.execute(
                "SELECT * FROM jobs WHERE id = ?", (job_id,)
            ).fetchone()
            
            if not job:
                return False
            
            if success:
                # Update job status
                conn.execute("""
                    UPDATE jobs 
                    SET status = 'completed', ended_at = ?, result = ?, worker_id = NULL
                    WHERE id = ?
                """, (now, json.dumps(result) if result else None, job_id))
                
                # Record metrics
                if job['started_at']:
                    queue_time = (job['started_at'] - job['created_at']) / 1000.0
                    exec_time = (now - job['started_at']) / 1000.0
                    conn.execute(
                        "INSERT OR REPLACE INTO metrics(job_id, queue_time, exec_time) VALUES (?, ?, ?)",
                        (job_id, queue_time, exec_time)
                    )
                return True
            else:
                # Handle failure with retry logic
                if job['retry_count'] < 3:
                    # Exponential backoff: 1s, 2s, 4s
                    delay = 1000 * (2 ** job['retry_count'])
                    conn.execute("""
                        UPDATE jobs 
                        SET status = 'queued', retry_count = retry_count + 1, 
                            error_message = ?, worker_id = NULL
                        WHERE id = ?
                    """, (error, job_id))
                else:
                    # Final failure
                    conn.execute("""
                        UPDATE jobs 
                        SET status = 'failed', ended_at = ?, error_message = ?, worker_id = NULL
                        WHERE id = ?
                    """, (now, error, job_id))
                return True
    
    def stop_job(self, job_id: int = None, name: str = None) -> bool:
        """Stop a running job"""
        job = self.get_job(job_id, name)
        if not job:
            return False
        
        if job.get('unit'):
            # Stop systemd unit
            try:
                subprocess.run(
                    SYSTEMCTL + ["stop", job['unit']], 
                    capture_output=True, check=True
                )
                # Also stop timer if it exists
                subprocess.run(
                    SYSTEMCTL + ["stop", job['unit'].replace('.service', '.timer')], 
                    capture_output=True, check=False
                )
            except subprocess.CalledProcessError:
                pass
        
        # Update status
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.execute(
                "UPDATE jobs SET status = 'stopped' WHERE id = ?",
                (job['id'],)
            )
        return True
    
    def list_jobs(self, status: str = None) -> List[Dict]:
        """List jobs, optionally filtered by status"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            conn.row_factory = sqlite3.Row
            
            if status:
                rows = conn.execute(
                    "SELECT * FROM jobs WHERE status = ? ORDER BY created_at DESC",
                    (status,)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT * FROM jobs ORDER BY created_at DESC"
                ).fetchall()
            
            return [dict(row) for row in rows]
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            # Job counts by status
            counts = {row['status']: row['count'] for row in conn.execute(
                "SELECT status, COUNT(*) as count FROM jobs GROUP BY status"
            )}
            
            # Performance metrics
            perf = conn.execute("""
                SELECT AVG(queue_time) as avg_queue_time, 
                       AVG(exec_time) as avg_exec_time,
                       MAX(queue_time) as max_queue_time,
                       MAX(exec_time) as max_exec_time
                FROM metrics
            """).fetchone()
            
            return {
                'job_counts': counts,
                'performance': dict(perf) if perf else {}
            }
    
    def cleanup(self, days: int = 7) -> int:
        """Clean up old completed/failed jobs"""
        cutoff = int(time.time() * 1000) - (days * 86400000)
        
        with sqlite3.connect(self.db_path, isolation_level=None) as conn:
            deleted = conn.execute("""
                DELETE FROM jobs 
                WHERE status IN ('completed', 'failed') AND ended_at < ?
            """, (cutoff,)).rowcount
            
            # Vacuum if needed
            page_count = conn.execute("PRAGMA page_count").fetchone()[0]
            freelist = conn.execute("PRAGMA freelist_count").fetchone()[0]
            
            if freelist > page_count * 0.3:
                conn.execute("VACUUM")
            
            return deleted

class Worker:
    """Worker process for job execution"""
    
    def __init__(self, orchestrator: Orchestrator, worker_id: str = None):
        self.orchestrator = orchestrator
        self.worker_id = worker_id or f"worker-{os.getpid()}"
        self.running = True
    
    def run(self, batch_size: int = 1):
        """Main worker loop"""
        print(f"Worker {self.worker_id} started (batch_size={batch_size})")
        
        while self.running:
            # Process batch of jobs
            jobs = []
            for _ in range(batch_size):
                job = self.orchestrator.pop_job(self.worker_id)
                if job:
                    jobs.append(job)
            
            if not jobs:
                time.sleep(0.05)
                continue
            
            for job in jobs:
                try:
                    # Execute command
                    result = subprocess.run(
                        [job['cmd'], *job['args']],
                        capture_output=True,
                        text=True,
                        timeout=290
                    )
                    
                    self.orchestrator.complete_job(
                        job['id'],
                        result.returncode == 0,
                        {
                            'stdout': result.stdout[:1000],
                            'stderr': result.stderr[:1000],
                            'returncode': result.returncode
                        },
                        result.stderr if result.returncode != 0 else None
                    )
                except subprocess.TimeoutExpired:
                    self.orchestrator.complete_job(
                        job['id'], False, error="TIMEOUT"
                    )
                except Exception as e:
                    self.orchestrator.complete_job(
                        job['id'], False, error=str(e)
                    )

def main():
    """Command-line interface"""
    parser = argparse.ArgumentParser(description="Job Orchestration System")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # Add command
    add_parser = subparsers.add_parser("add", help="Add a new job")
    add_parser.add_argument("name", help="Job name")
    add_parser.add_argument("command", help="Command to execute")
    add_parser.add_argument("args", nargs="*", help="Command arguments")
    add_parser.add_argument("--env", action="append", help="Environment variables (KEY=VAL)")
    add_parser.add_argument("--cwd", help="Working directory")
    add_parser.add_argument("--priority", type=int, default=0, help="Job priority")
    add_parser.add_argument("--schedule", help="Systemd calendar schedule")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority")
    add_parser.add_argument("--nice", type=int, help="Nice value")
    add_parser.add_argument("--slice", help="Systemd slice")
    add_parser.add_argument("--cpu-weight", type=int, help="CPU weight")
    add_parser.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add_parser.add_argument("--start", action="store_true", help="Start job immediately")
    add_parser.add_argument("--systemd", action="store_true", help="Use systemd for execution")
    
    # List command
    list_parser = subparsers.add_parser("list", help="List jobs")
    list_parser.add_argument("--status", help="Filter by status")
    
    # Start command
    start_parser = subparsers.add_parser("start", help="Start a job")
    start_parser.add_argument("name", help="Job name")
    start_parser.add_argument("--systemd", action="store_true", help="Use systemd for execution")
    
    # Stop command
    stop_parser = subparsers.add_parser("stop", help="Stop a job")
    stop_parser.add_argument("name", help="Job name")
    
    # Status command
    status_parser = subparsers.add_parser("status", help="Show job status")
    status_parser.add_argument("name", help="Job name")
    
    # Worker command
    worker_parser = subparsers.add_parser("worker", help="Run worker process")
    worker_parser.add_argument("--batch", type=int, default=1, help="Batch size")
    
    # Stats command
    subparsers.add_parser("stats", help="Show system statistics")
    
    # Cleanup command
    cleanup_parser = subparsers.add_parser("cleanup", help="Clean up old jobs")
    cleanup_parser.add_argument("--days", type=int, default=7, help="Keep jobs newer than N days")
    
    args = parser.parse_args()
    orchestrator = Orchestrator()
    
    if args.command == "add":
        env = {}
        if args.env:
            for e in args.env:
                if "=" in e:
                    k, v = e.split("=", 1)
                    env[k] = v
        
        job_id = orchestrator.add_job(
            name=args.name,
            cmd=args.command,
            args=args.args,
            env=env,
            cwd=args.cwd,
            priority=args.priority,
            schedule=args.schedule,
            rtprio=args.rtprio,
            nice=args.nice,
            slice=args.slice,
            cpu_weight=args.cpu_weight,
            mem_max_mb=args.mem_max_mb
        )
        
        print(f"Added job {job_id}")
        
        if args.start:
            if args.systemd:
                success = orchestrator.start_job_systemd(job_id)
                print(f"Started with systemd: {'Success' if success else 'Failed'}")
            else:
                # Job will be picked up by a worker
                print("Job queued for worker execution")
    
    elif args.command == "list":
        jobs = orchestrator.list_jobs(args.status)
        for job in jobs:
            print(f"{job['id']}: {job['name']} - {job['status']}")
    
    elif args.command == "start":
        job = orchestrator.get_job(name=args.name)
        if not job:
            print(f"Job '{args.name}' not found")
            return
        
        if args.systemd:
            success = orchestrator.start_job_systemd(job['id'])
            print(f"Started with systemd: {'Success' if success else 'Failed'}")
        else:
            # Reset status to queued so worker can pick it up
            with sqlite3.connect(orchestrator.db_path, isolation_level=None) as conn:
                conn.execute(
                    "UPDATE jobs SET status = 'queued' WHERE id = ?",
                    (job['id'],)
                )
            print("Job queued for worker execution")
    
    elif args.command == "stop":
        success = orchestrator.stop_job(name=args.name)
        print(f"Stopped: {'Success' if success else 'Failed'}")
    
    elif args.command == "status":
        job = orchestrator.get_job(name=args.name)
        if not job:
            print(f"Job '{args.name}' not found")
            return
        
        print(f"Status: {job['status']}")
        if job.get('unit'):
            try:
                result = subprocess.run(
                    SYSTEMCTL + ["show", job['unit'], "--property", "ActiveState"],
                    capture_output=True, text=True, check=True
                )
                print(f"Systemd: {result.stdout.strip()}")
            except subprocess.CalledProcessError:
                print("Systemd: Not found")
    
    elif args.command == "worker":
        worker = Worker(orchestrator)
        worker.run(args.batch)
    
    elif args.command == "stats":
        stats = orchestrator.get_stats()
        print(json.dumps(stats, indent=2))
    
    elif args.command == "cleanup":
        deleted = orchestrator.cleanup(args.days)
        print(f"Deleted {deleted} old jobs")

if __name__ == "__main__":
    main()
```

================================================================================

================================================================================
FILE: grok.py
================================================================================

#!/usr/bin/env python3
"""
Integrated Job Orchestrator: Combines task queue with systemd management.
Uses SQLite for state, systemd for processes, supports deps, retries, scheduling.
"""
import argparse, json, os, shlex, sqlite3, subprocess, sys, time
from pathlib import Path
from typing import Dict, Any, Optional

DB = Path.home() / ".orchestrator_tasks.db"
UNIT_PREFIX = "orch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

def sh(cmd: list[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

def ok(cp): return cp.returncode == 0

def unit_name(name: str) -> str:
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
    return f"{UNIT_PREFIX}{safe}.service"

def db():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    for pragma in PRAGMAS:
        con.execute(pragma)
    con.executescript("""
        CREATE TABLE IF NOT EXISTS jobs(
            name TEXT PRIMARY KEY,
            cmd TEXT NOT NULL,
            args TEXT,
            env TEXT,
            cwd TEXT,
            schedule TEXT,
            scheduled_at INTEGER,
            priority INTEGER DEFAULT 0,
            deps TEXT,
            max_retries INTEGER DEFAULT 3,
            retry_count INTEGER DEFAULT 0,
            timeout_sec INTEGER,
            rtprio INTEGER,
            nice INTEGER,
            slice TEXT,
            cpu_weight INTEGER,
            mem_max_mb INTEGER,
            unit TEXT,
            status TEXT DEFAULT 'added',
            created_at INTEGER,
            started_at INTEGER,
            ended_at INTEGER,
            error TEXT
        );
    """)
    return con

def add_job(**kw):
    kw.setdefault("created_at", int(time.time() * 1000))
    with db() as con:
        con.execute("""INSERT OR REPLACE INTO jobs
            (name,cmd,args,env,cwd,schedule,scheduled_at,priority,deps,max_retries,retry_count,timeout_sec,
             rtprio,nice,slice,cpu_weight,mem_max_mb,unit,status,created_at,started_at,ended_at,error)
            VALUES(:name,:cmd,:args,:env,:cwd,:schedule,:scheduled_at,:priority,:deps,:max_retries,:retry_count,
                   :timeout_sec,:rtprio,:nice,:slice,:cpu_weight,:mem_max_mb,:unit,:status,:created_at,
                   :started_at,:ended_at,:error)""", kw)

def list_jobs():
    with db() as con:
        return con.execute("SELECT * FROM jobs ORDER BY created_at DESC").fetchall()

def show(unit: str, *props: str) -> Dict[str, str]:
    out = sh(SYSTEMCTL + ["show", unit, *(["--property=" + p for p in props] if props else [])]).stdout
    return {k: v for k, v in (line.split("=", 1) for line in out.splitlines() if "=" in line)}

def start_transient(job: sqlite3.Row) -> tuple[bool, str, str]:
    unit = unit_name(job["name"])
    props = [
        "--property=StandardOutput=journal",
        "--property=StandardError=journal",
        "--property=KillMode=control-group",
    ]
    if job["timeout_sec"]:
        props += [f"--property=TimeoutSec={job['timeout_sec']}"]
    if job["rtprio"]:
        props += ["--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={job['rtprio']}"]
    calc_nice = job["nice"]
    if calc_nice is None and job["priority"] is not None:
        calc_nice = 10 - job["priority"]  # Higher priority -> lower nice
    if calc_nice is not None:
        props += [f"--property=Nice={int(calc_nice)}"]
    if job["slice"]:
        props += [f"--slice={job['slice']}"]
    if job["cpu_weight"]:
        props += [f"--property=CPUWeight={job['cpu_weight']}"]
    if job["mem_max_mb"]:
        props += [f"--property=MemoryMax={int(job['mem_max_mb'])}M"]
    if job["cwd"]:
        props += [f"--property=WorkingDirectory={job['cwd']}"]
    env = []
    if job["env"]:
        for k, v in json.loads(job["env"]).items():
            env += ["--setenv", f"{k}={v}"]
    when = []
    persistent = False
    now_s = time.time()
    if job["schedule"]:
        when = ["--on-calendar", job["schedule"]]
        persistent = True
    elif job["scheduled_at"]:
        sched_s = job["scheduled_at"] / 1000.0
        delay_s = max(0, sched_s - now_s)
        if delay_s > 0:
            when = ["--on-active", f"{int(delay_s + 0.5)}s"]  # Round up
            persistent = True
    if persistent:
        props += ["--property=Persistent=true"]
    cmd_args = [job["cmd"], *json.loads(job["args"] or "[]")]
    cp = sh([*SYSDRUN, "--unit", unit, *props, *env, *when, "--", *cmd_args])
    with db() as con:
        new_status = "scheduled" if when else "started"
        con.execute("UPDATE jobs SET unit=?, status=? WHERE name=?", (unit, new_status, job["name"]))
    return ok(cp), unit, cp.stderr.strip() or cp.stdout.strip()

def stop(name: str):
    with db() as con:
        row = con.execute("SELECT unit FROM jobs WHERE name=?", (name,)).fetchone()
        if not row or not row["unit"]:
            return
    unit = row["unit"]
    sh(SYSTEMCTL + ["stop", unit])
    sh(SYSTEMCTL + ["stop", unit.replace(".service", ".timer")])
    with db() as con:
        con.execute("UPDATE jobs SET status='stopped', unit=NULL WHERE name=?", (name,))

def status(name: str) -> Dict[str, Any]:
    with db() as con:
        row = con.execute("SELECT unit, status FROM jobs WHERE name=?", (name,)).fetchone()
        if not row:
            return {"active": "unknown"}
    if not row["unit"]:
        return {"unit": None, "active": row["status"]}
    info = show(row["unit"], "ActiveState", "Result", "MainPID")
    return {"unit": row["unit"], "active": info.get("ActiveState"), "result": info.get("Result"), "pid": info.get("MainPID")}

def reconcile():
    now_ms = int(time.time() * 1000)
    with db() as con:
        jobs = con.execute("SELECT * FROM jobs").fetchall()
        for job in jobs:
            if job["unit"]:
                unit = job["unit"]
                info = show(unit, "ActiveState", "Result", "ExecMainStartTimestamp", "ExecMainExitTimestamp")
                active = info.get("ActiveState")
                if active in ("inactive", "failed"):
                    result = info.get("Result")
                    if active == "failed" or result != "success":
                        new_status = "failed"
                        err = f"see journalctl --user -u {unit}"
                        con.execute("UPDATE jobs SET status=?, error=?, ended_at=? WHERE name=?",
                                    (new_status, err, now_ms, job["name"]))
                    else:
                        new_status = "completed"
                        con.execute("UPDATE jobs SET status=?, ended_at=? WHERE name=?",
                                    (new_status, now_ms, job["name"]))
                elif active == "active":
                    if job["status"] != "running":
                        con.execute("UPDATE jobs SET status='running' WHERE name=?", (job["name"],))
                    start_ts = info.get("ExecMainStartTimestamp")
                    if start_ts and start_ts.isdigit() and not job["started_at"]:
                        started_at = int(int(start_ts) / 1000)
                        con.execute("UPDATE jobs SET started_at=? WHERE name=?", (started_at, job["name"]))
            if job["status"] in ("added", "failed"):
                if job["status"] == "failed" and job["retry_count"] >= job["max_retries"]:
                    continue
                deps_ok = True
                if job["deps"]:
                    deps = json.loads(job["deps"])
                    for dname in deps:
                        drow = con.execute("SELECT status FROM jobs WHERE name=?", (dname,)).fetchone()
                        if not drow or drow["status"] != "completed":
                            deps_ok = False
                            break
                if not deps_ok:
                    continue
                if job["scheduled_at"] and job["scheduled_at"] > now_ms:
                    continue
                if job["status"] == "failed":
                    backoff_ms = 1000 * (2 ** job["retry_count"])
                    if now_ms < (job["ended_at"] or 0) + backoff_ms:
                        continue
                if job["unit"]:
                    stop(job["name"])
                if job["status"] == "failed":
                    con.execute("UPDATE jobs SET retry_count=retry_count+1 WHERE name=?", (job["name"],))
                ok_, unit, msg = start_transient(job)
                if not ok_:
                    continue
                con.execute("UPDATE jobs SET started_at=? WHERE name=? AND started_at IS NULL", (now_ms, job["name"]))

def stats() -> Dict[str, Any]:
    with db() as con:
        counts = {r["status"]: r["c"] for r in con.execute("SELECT status, COUNT(*) c FROM jobs GROUP BY status")}
        perf = con.execute("""
            SELECT AVG(started_at - created_at)/1000.0 avg_qt,
                   AVG(ended_at - started_at)/1000.0 avg_et
            FROM jobs WHERE ended_at IS NOT NULL AND started_at IS NOT NULL
        """).fetchone()
        return {"tasks": counts, "perf": dict(perf) if perf else {}}

def cleanup(days: int = 7) -> int:
    cutoff = int(time.time() * 1000) - (days * 86400000)
    with db() as con:
        deleted = con.execute("DELETE FROM jobs WHERE status IN ('completed','failed') AND ended_at < ?", (cutoff,)).rowcount
        page_count = con.execute("PRAGMA page_count").fetchone()[0]
        freelist = con.execute("PRAGMA freelist_count").fetchone()[0]
        if freelist > page_count * 0.3:
            con.execute("VACUUM")
        return deleted

def systemd_unit():
    return f"""[Unit]
Description=Job Orchestrator Reconciler
After=network.target
[Service]
Type=simple
ExecStart=/usr/bin/python3 {os.path.abspath(__file__)} run
Restart=always
RestartSec=3
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=10
LimitNOFILE=65536
Nice=-5
PrivateTmp=yes
StandardOutput=journal
StandardError=journal
[Install]
WantedBy=multi-user.target"""

def main():
    ap = argparse.ArgumentParser(description="Job Orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)
    a = sub.add_parser("add", help="add job")
    a.add_argument("name")
    a.add_argument("command")
    a.add_argument("args", nargs="*")
    a.add_argument("--env", action="append", default=[])
    a.add_argument("--cwd")
    a.add_argument("--schedule")
    a.add_argument("--delay-ms", type=int)
    a.add_argument("--priority", type=int)
    a.add_argument("--deps")
    a.add_argument("--max-retries", type=int)
    a.add_argument("--timeout-sec", type=int)
    a.add_argument("--rtprio", type=int)
    a.add_argument("--nice", type=int)
    a.add_argument("--slice")
    a.add_argument("--cpu-weight", type=int)
    a.add_argument("--mem-max-mb", type=int)
    a.add_argument("--start", action="store_true")
    sub.add_parser("list")
    s = sub.add_parser("start")
    s.add_argument("name")
    t = sub.add_parser("stop")
    t.add_argument("name")
    u = sub.add_parser("status")
    u.add_argument("name")
    sub.add_parser("reconcile")
    sub.add_parser("run", help="run reconciler loop")
    sub.add_parser("stats")
    c = sub.add_parser("cleanup")
    c.add_argument("days", type=int, nargs="?", default=7)
    sub.add_parser("systemd", help="generate systemd unit")
    args = ap.parse_args()
    if args.cmd == "add":
        env_d = dict(e.split("=", 1) for e in args.env) if args.env else {}
        deps_l = args.deps.split(",") if args.deps else []
        add_job(
            name=args.name, cmd=args.command, args=json.dumps(args.args),
            env=json.dumps(env_d) if env_d else None, cwd=args.cwd, schedule=args.schedule,
            scheduled_at=args.delay_ms + int(time.time() * 1000) if args.delay_ms else None,
            priority=args.priority, deps=json.dumps(deps_l), max_retries=args.max_retries,
            timeout_sec=args.timeout_sec, rtprio=args.rtprio, nice=args.nice, slice=args.slice,
            cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb, unit=None, status="added",
            started_at=None, ended_at=None, error=None
        )
        if args.start:
            with db() as con:
                row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
            ok_, unit, msg = start_transient(row)
            print(unit if ok_ else f"ERROR: {msg}")
        else:
            print("added")
    elif args.cmd == "list":
        reconcile()
        for r in list_jobs():
            print(f"{r['name']}: {r['status']}  unit={r['unit'] or '-'}  sched={r['schedule'] or r['scheduled_at'] or '-'}")
    elif args.cmd == "start":
        with db() as con:
            row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
        if not row:
            sys.exit("unknown job")
        ok_, unit, msg = start_transient(row)
        print(unit if ok_ else f"ERROR: {msg}")
    elif args.cmd == "stop":
        stop(args.name)
        print("stopped")
    elif args.cmd == "status":
        print(json.dumps(status(args.name), indent=2))
    elif args.cmd == "reconcile":
        reconcile()
        print("ok")
    elif args.cmd == "run":
        print("Orchestrator running...")
        while True:
            reconcile()
            time.sleep(0.5)
    elif args.cmd == "stats":
        print(json.dumps(stats(), indent=2))
    elif args.cmd == "cleanup":
        deleted = cleanup(args.days)
        print(f"Deleted {deleted} old jobs")
    elif args.cmd == "systemd":
        print(systemd_unit())

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: kimi.py
================================================================================

#!/usr/bin/env python3
"""
aiosq â€“ tiny unified job-queue + systemd orchestrator
One SQLite file (~/.aiosq.db) + systemd-run for cgroups, auto cleanup, no zombies.
"""
from __future__ import annotations
import argparse, json, os, shlex, sqlite3, subprocess, sys, time, signal, threading
from pathlib import Path
from typing import Dict, Any, Optional, List

DB_PATH   = Path.home() / ".aiosq.db"
UNIT_PREF = "aiosq-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN   = ["systemd-run", "--user", "--collect", "--quiet"]

# ---------- SQLite helpers ---------------------------------------------------
def conn() -> sqlite3.Connection:
    c = sqlite3.connect(DB_PATH, isolation_level=None, check_same_thread=False)
    c.row_factory = sqlite3.Row
    c.executescript("""
        PRAGMA journal_mode=WAL;  PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS t(
            id INTEGER PRIMARY KEY,
            cmd TEXT NOT NULL,
            p  INTEGER DEFAULT 0,               -- priority
            s  TEXT DEFAULT 'q',                -- status
            at INTEGER DEFAULT 0,               -- scheduled_at ms
            w  TEXT,                            -- worker
            r  INTEGER DEFAULT 0,               -- retry
            e  TEXT,                            -- error
            res TEXT,                           -- result
            ct INTEGER DEFAULT (unixepoch()*1000),
            st INTEGER, et INTEGER,             -- start/end
            dep TEXT                            -- json list
        );
        CREATE INDEX IF NOT EXISTS ix ON t(s,p DESC,at,id) WHERE s IN ('q','r');
    """)
    return c

LOCK = threading.RLock()

# ---------- Core queue ops ---------------------------------------------------
def add(cmd:str, p:int=0, at:int=0, dep:Optional[List[int]]=None) -> int:
    with LOCK:
        return conn().execute(
            "INSERT INTO t(cmd,p,at,dep) VALUES(?,?,?,?)",
            (cmd, p, at or int(time.time()*1000), json.dumps(dep) if dep else None)
        ).lastrowid

def pop(worker:str) -> Optional[Dict[str,Any]]:
    now = int(time.time()*1000)
    with LOCK:
        row = conn().execute("""
            SELECT id,cmd FROM t
            WHERE s='q' AND at<=? AND
                  (dep IS NULL OR NOT EXISTS(
                      SELECT 1 FROM json_each(t.dep) j JOIN t d ON d.id=j.value WHERE d.s!='d'))
            ORDER BY p DESC,at,id LIMIT 1
        """, (now,)).fetchone()
        if not row: return None
        conn().execute("UPDATE t SET s='r',w=?,st=? WHERE id=? AND s='q'", (worker,now,row['id']))
        return dict(row)

def done(task_id:int, ok:bool=True, worker:str="", err:str=""):
    now = int(time.time()*1000)
    with LOCK:
        if ok:
            conn().execute("UPDATE t SET s='d',et=?,w=NULL,res=? WHERE id=? AND w=?",
                           (now, json.dumps(err) if err else None, task_id, worker))
        else:
            row = conn().execute("SELECT r FROM t WHERE id=? AND w=?", (task_id,worker)).fetchone()
            if row and row['r']<3:
                delay = 1000*(2**row['r'])
                conn().execute("UPDATE t SET s='q',at=?,r=r+1,e=?,w=NULL WHERE id=?",
                               (now+delay,err,task_id))
            else:
                conn().execute("UPDATE t SET s='f',et=?,e=?,w=NULL WHERE id=?", (now,err,task_id))

# ---------- systemd helpers --------------------------------------------------
def unit(name:str) -> str: return f"{UNIT_PREF}{name.replace('/','_')}.service"
def sh(cmd): return subprocess.run(cmd,text=True,capture_output=True)

def start_transient(name:str, cmdline:str, env:dict=None, nice:int=None, rtprio:int=None,
                    slice:str=None, cpuw:int=None, mem:int=None, sched:str=None):
    props = ["--property=StandardOutput=journal", "--property=StandardError=journal"]
    if nice is not None: props.append(f"--property=Nice={nice}")
    if rtprio: props += ["--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={rtprio}"]
    if slice: props.append(f"--slice={slice}")
    if cpuw: props.append(f"--property=CPUWeight={cpuw}")
    if mem: props.append(f"--property=MemoryMax={mem}M")
    eargs = []
    if env:
        for k,v in env.items(): eargs += ["--setenv",f"{k}={v}"]
    when = []
    if sched: when += ["--on-calendar", sched]
    u = unit(name)
    full = [*SYSDRUN, "--unit", u, *props, *eargs, *when, "--", *shlex.split(cmdline)]
    cp = sh(full)
    return cp.returncode==0, u, cp.stderr or cp.stdout

# ---------- Worker loop ------------------------------------------------------
class Worker:
    def __init__(self, batch:int=1):
        self.batch = batch
        self.running = True
        signal.signal(signal.SIGTERM, lambda *_: setattr(self,'running',False))
        signal.signal(signal.SIGINT,  lambda *_: setattr(self,'running',False))

    def run(self):
        wid = f"w{os.getpid()}"
        print(f"Worker {wid} started (batch={self.batch})")
        while self.running:
            tasks = []
            for _ in range(self.batch):
                t = pop(wid)
                if t: tasks.append(t)
            if not tasks:
                time.sleep(.05); continue
            for t in tasks:
                if not self.running: break
                try:
                    res = subprocess.run(t['cmd'], shell=True, capture_output=True,
                                         text=True, timeout=290)
                    done(t['id'], res.returncode==0, wid,
                         err=res.stderr[:500] if res.returncode else None)
                except Exception as e:
                    done(t['id'], False, wid, err=str(e))

# ---------- CLI --------------------------------------------------------------
def stats():
    c = conn()
    counts = {r['s']:r['c'] for r in c.execute("SELECT s,COUNT(*) c FROM t GROUP BY s")}
    return {"tasks":counts}

def cleanup(days:int=7):
    cutoff = int(time.time()*1000)-days*86400*1000
    with LOCK:
        deleted = conn().execute("DELETE FROM t WHERE s IN ('d','f') AND et<?",(cutoff,)).rowcount
    return deleted

def cli():
    ap = argparse.ArgumentParser(description="aiosq â€“ queue + systemd orchestrator")
    sp = ap.add_subparsers(dest="cmd",required=True)

    a = sp.add_parser("add", help="add task")
    a.add_argument("cmd"); a.add_argument("-p","--priority",type=int,default=0)
    a.add_argument("-d","--delay",type=int,default=0)
    a.add_argument("--dep",type=int,nargs="+")

    sp.add_parser("worker").add_argument("-b","--batch",type=int,default=1)
    sp.add_parser("stats")
    c = sp.add_parser("cleanup"); c.add_argument("days",type=int,nargs="?",default=7)

    r = sp.add_parser("run", help="run via systemd")
    r.add_argument("name"); r.add_argument("cmdline")
    r.add_argument("--nice",type=int); r.add_argument("--rtprio",type=int)
    r.add_argument("--slice"); r.add_argument("--cpu-weight",type=int)
    r.add_argument("--mem-max",type=int)
    r.add_argument("--on-calendar"); r.add_argument("--env",action="append",default=[])

    s = sp.add_parser("stop"); s.add_argument("name")
    st = sp.add_parser("status"); st.add_argument("name")

    args = ap.parse_args()

    if args.cmd == "add":
        at = int(time.time()*1000)+args.delay if args.delay else 0
        tid = add(args.cmd, args.priority, at, args.dep)
        print(tid)

    elif args.cmd == "worker":
        Worker(args.batch).run()

    elif args.cmd == "stats":
        print(json.dumps(stats(),indent=2))

    elif args.cmd == "cleanup":
        print(cleanup(args.days))

    elif args.cmd == "run":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        ok,u,msg = start_transient(args.name, args.cmdline, env, args.nice, args.rtprio,
                                   args.slice, args.cpu_weight, args.mem_max, args.on_calendar)
        print(u if ok else f"error: {msg}")

    elif args.cmd == "stop":
        u = unit(args.name)
        sh(SYSTEMCTL + ["stop", u])
        sh(SYSTEMCTL + ["stop", u.replace(".service",".timer")])
        print("stopped")

    elif args.cmd == "status":
        info = sh(SYSTEMCTL + ["show", unit(args.name), "--property=ActiveState,Result,MainPID"]).stdout
        print(info or "unknown")

if __name__ == "__main__":
    cli()

================================================================================

================================================================================
FILE: minimax.py
================================================================================

#!/usr/bin/env python3
"""
Integrated Job Orchestrator: Combines task queue with systemd process management
"""
import sqlite3, subprocess, json, time, sys, os, signal, threading, argparse
from pathlib import Path

PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA mmap_size=268435456",
    "PRAGMA busy_timeout=5000",
    "PRAGMA wal_autocheckpoint=1000",
]

SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

class JobManager:
    def __init__(self, db="jobs.db"):
        self.db = db
        self.lock = threading.RLock()
        self._init_db()

    def _init_db(self):
        with self.lock, sqlite3.connect(self.db) as conn:
            conn.row_factory = sqlite3.Row
            for pragma in PRAGMAS:
                conn.execute(pragma)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY,
                    cmd TEXT NOT NULL,
                    args TEXT,
                    env TEXT,
                    cwd TEXT,
                    p INT DEFAULT 0,
                    s TEXT DEFAULT 'q',
                    at INT DEFAULT 0,
                    w TEXT,
                    r INT DEFAULT 0,
                    e TEXT,
                    res TEXT,
                    ct INT DEFAULT (strftime('%s','now')*1000),
                    st INT,
                    et INT,
                    dep TEXT,
                    schedule TEXT,
                    rtprio INTEGER,
                    nice INTEGER,
                    slice TEXT,
                    cpu_weight INTEGER,
                    mem_max_mb INTEGER,
                    unit TEXT
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS ix ON jobs(s,p DESC,at,id)
                WHERE s IN ('q','r')
            """)

    def add_job(self, cmd, args=None, env=None, cwd=None, priority=0, 
                scheduled_at=None, deps=None, schedule=None, rtprio=None,
                nice=None, slice=None, cpu_weight=None, mem_max_mb=None):
        now = int(time.time() * 1000)
        at = int(scheduled_at * 1000) if scheduled_at else now
        dep_json = json.dumps(deps) if deps else None
        
        with self.lock, sqlite3.connect(self.db) as conn:
            return conn.execute("""
                INSERT INTO jobs 
                (cmd, args, env, cwd, p, at, dep, schedule, rtprio, nice, slice, cpu_weight, mem_max_mb)
                VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)
            """, (
                cmd, 
                json.dumps(args) if args else None,
                json.dumps(env) if env else None,
                cwd,
                priority,
                at,
                dep_json,
                schedule,
                rtprio,
                nice,
                slice,
                cpu_weight,
                mem_max_mb
            )).lastrowid

    def get_ready_jobs(self):
        now = int(time.time() * 1000)
        with self.lock, sqlite3.connect(self.db) as conn:
            return [dict(row) for row in conn.execute("""
                SELECT * FROM jobs
                WHERE s='q' AND at <= ?
                AND (dep IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(dep) AS d
                    JOIN jobs AS j ON j.id = d.value
                    WHERE j.s != 'd'
                ))
                ORDER BY p DESC, at, id
            """, (now,))]

    def start_job(self, job_id):
        with self.lock, sqlite3.connect(self.db) as conn:
            job = conn.execute("SELECT * FROM jobs WHERE id=?", (job_id,)).fetchone()
            if not job or job['s'] != 'q':
                return False
            
            unit = f"job-{job_id}.service"
            props = [
                "--property=StandardOutput=journal",
                "--property=StandardError=journal",
                "--property=KillMode=control-group"
            ]
            
            if job['rtprio']:
                props += [
                    "--property=CPUSchedulingPolicy=rr",
                    f"--property=CPUSchedulingPriority={job['rtprio']}"
                ]
            if job['nice'] is not None:
                props.append(f"--property=Nice={job['nice']}")
            if job['slice']:
                props.append(f"--slice={job['slice']}")
            if job['cpu_weight']:
                props.append(f"--property=CPUWeight={job['cpu_weight']}")
            if job['mem_max_mb']:
                props.append(f"--property=MemoryMax={job['mem_max_mb']}M")
            if job['cwd']:
                props.append(f"--property=WorkingDirectory={job['cwd']}")
            
            cmd = [*SYSDRUN, "--unit", unit, *props]
            
            if job['env']:
                for k, v in json.loads(job['env']).items():
                    cmd += ["--setenv", f"{k}={v}"]
            
            if job['schedule']:
                cmd += ["--on-calendar", job['schedule']]
            
            cmd += ["--", job['cmd']]
            if job['args']:
                cmd += json.loads(job['args'])
            
            if subprocess.run(cmd, capture_output=True).returncode == 0:
                conn.execute("UPDATE jobs SET s='r', unit=?, w=? WHERE id=?", 
                           (unit, os.getpid(), job_id))
                return True
            return False

    def reconcile(self):
        with self.lock, sqlite3.connect(self.db) as conn:
            for job in conn.execute("SELECT id, unit FROM jobs WHERE s='r' AND unit IS NOT NULL"):
                cp = subprocess.run(
                    [*SYSTEMCTL, "show", job['unit'], 
                     "--property=ActiveState", "--property=Result", 
                     "--property=ExecMainExitCode"],
                    capture_output=True, text=True
                )
                if cp.returncode != 0:
                    continue
                
                props = dict(line.split('=', 1) for line in cp.stdout.splitlines() if '=' in line)
                active = props.get("ActiveState", "")
                result = props.get("Result", "")
                
                if active in ("inactive", "failed"):
                    if result == "success":
                        conn.execute("""
                            UPDATE jobs SET s='d', et=?, res=? 
                            WHERE id=?
                        """, (int(time.time()*1000), json.dumps({'exit': props.get('ExecMainExitCode')}), job['id']))
                    else:
                        conn.execute("""
                            UPDATE jobs SET s='f', e=?, et=? 
                            WHERE id=?
                        """, (f"Exit {props.get('ExecMainExitCode')}", 
                              int(time.time()*1000), job['id']))

class Worker:
    def __init__(self, jm):
        self.jm = jm
        self.running = True
        signal.signal(signal.SIGTERM, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGINT, lambda *_: setattr(self, 'running', False))

    def run(self):
        while self.running:
            self.jm.reconcile()
            for job in self.jm.get_ready_jobs():
                if not self.running:
                    break
                if self.jm.start_job(job['id']):
                    print(f"Started job {job['id']}")
            time.sleep(0.1)

def main():
    parser = argparse.ArgumentParser(description="Integrated Job Orchestrator")
    sub = parser.add_subparsers(dest="cmd", required=True)
    
    add = sub.add_parser("add", help="Add new job")
    add.add_argument("cmd", help="Command to execute")
    add.add_argument("--args", type=json.loads, default=[])
    add.add_argument("--env", type=json.loads, default={})
    add.add_argument("--cwd", help="Working directory")
    add.add_argument("--priority", type=int, default=0)
    add.add_argument("--schedule", help="Systemd calendar schedule")
    add.add_argument("--rtprio", type=int, help="Real-time priority")
    add.add_argument("--nice", type=int, help="Nice value")
    add.add_argument("--slice", help="Systemd slice")
    add.add_argument("--cpu-weight", type=int, help="CPU weight")
    add.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    add.add_argument("--deps", type=json.loads, default=[], help="Dependency IDs")
    
    sub.add_parser("worker", help="Start worker process")
    
    args = parser.parse_args()
    
    jm = JobManager()
    
    if args.cmd == "add":
        job_id = jm.add_job(
            cmd=args.cmd,
            args=args.args,
            env=args.env,
            cwd=args.cwd,
            priority=args.priority,
            schedule=args.schedule,
            rtprio=args.rtprio,
            nice=args.nice,
            slice=args.slice,
            cpu_weight=args.cpu_weight,
            mem_max_mb=args.mem_max_mb,
            deps=args.deps
        )
        print(f"Added job {job_id}")
    
    elif args.cmd == "worker":
        Worker(jm).run()

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: mistral.py
================================================================================

import sqlite3, subprocess, json, time, sys, os
from pathlib import Path
from typing import Optional, Dict, Any

DB_PATH = Path.home() / ".job_orchestrator.db"
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA busy_timeout=5000",
]

class JobOrchestrator:
    def __init__(self, db_path=DB_PATH):
        self.c = sqlite3.connect(db_path, isolation_level=None, check_same_thread=False)
        self.c.row_factory = sqlite3.Row
        self._init_db()

    def _init_db(self):
        for pragma in PRAGMAS:
            self.c.execute(pragma)
        self.c.execute("""
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE,
                cmd TEXT NOT NULL,
                args TEXT,
                env TEXT,
                cwd TEXT,
                schedule TEXT,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'queued',
                created_at INTEGER DEFAULT (strftime('%s','now')),
                started_at INTEGER,
                ended_at INTEGER,
                error_message TEXT,
                result TEXT,
                systemd_unit TEXT,
                systemd_properties TEXT,
                dependencies TEXT,
                retry_count INTEGER DEFAULT 0,
                retry_delay INTEGER DEFAULT 1000
            )
        """)

    def add_job(self, name, cmd, args=None, env=None, cwd=None, schedule=None,
                priority=0, dependencies=None, systemd_properties=None):
        args_json = json.dumps(args) if args else None
        env_json = json.dumps(env) if env else None
        dependencies_json = json.dumps(dependencies) if dependencies else None
        systemd_properties_json = json.dumps(systemd_properties) if systemd_properties else None
        cursor = self.c.execute("""
            INSERT INTO jobs (name, cmd, args, env, cwd, schedule, priority, dependencies, systemd_properties)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (name, cmd, args_json, env_json, cwd, schedule, priority, dependencies_json, systemd_properties_json))
        return cursor.lastrowid

    def start_job(self, job_id):
        with self.c:
            job = self.c.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            if not job:
                return False, "Job not found"
            if job['status'] != 'queued':
                return False, "Job is not queued"

            if job['dependencies']:
                deps = json.loads(job['dependencies'])
                for dep_id in deps:
                    dep_job = self.c.execute("SELECT status FROM jobs WHERE id = ?", (dep_id,)).fetchone()
                    if not dep_job or dep_job['status'] != 'completed':
                        return False, f"Dependency {dep_id} not satisfied"

            self.c.execute("UPDATE jobs SET status = 'starting' WHERE id = ?", (job_id,))

            unit_name = f"job-{job_id}.service"
            cmd = ['systemd-run', '--user', '--collect', '--quiet', '--unit', unit_name]

            if job['systemd_properties']:
                properties = json.loads(job['systemd_properties'])
                for key, value in properties.items():
                    cmd.extend(['--property', f"{key}={value}"])

            cmd.extend(['--', job['cmd']])
            if job['args']:
                args = json.loads(job['args'])
                cmd.extend(args)

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                retry_count = job['retry_count'] + 1
                if retry_count < 3:
                    retry_delay = job['retry_delay'] * 2
                    self.c.execute("""
                        UPDATE jobs SET status = 'queued', retry_count = ?, retry_delay = ?, error_message = ?
                        WHERE id = ?
                    """, (retry_count, retry_delay, result.stderr, job_id))
                    return False, f"Job failed, will retry. Attempt {retry_count}/3"
                else:
                    self.c.execute("""
                        UPDATE jobs SET status = 'failed', error_message = ?, ended_at = ?
                        WHERE id = ?
                    """, (result.stderr, int(time.time()), job_id))
                    return False, result.stderr

            self.c.execute("""
                UPDATE jobs SET status = 'running', systemd_unit = ?, started_at = ?, retry_count = 0
                WHERE id = ?
            """, (unit_name, int(time.time()), job_id))
            return True, unit_name

    def stop_job(self, job_id):
        with self.c:
            job = self.c.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            if not job:
                return False, "Job not found"
            if job['status'] not in ('running', 'starting'):
                return False, "Job is not running"

            unit_name = job['systemd_unit']
            stop_cmd = ['systemctl', '--user', 'stop', unit_name]
            result = subprocess.run(stop_cmd, capture_output=True, text=True)
            if result.returncode != 0:
                return False, result.stderr

            self.c.execute("""
                UPDATE jobs SET status = 'stopped', ended_at = ?
                WHERE id = ?
            """, (int(time.time()), job_id))
            return True, "Job stopped"

    def reclaim_stalled_jobs(self, timeout=300):
        cutoff = int(time.time()) - timeout
        with self.c:
            stalled = self.c.execute("""
                SELECT id FROM jobs WHERE status = 'starting' AND started_at < ?
            """, (cutoff,)).fetchall()
            for job in stalled:
                self.c.execute("""
                    UPDATE jobs SET status = 'queued', started_at = NULL, retry_count = retry_count + 1
                    WHERE id = ?
                """, (job['id'],))
            return len(stalled)

    def cleanup(self, days=7):
        cutoff = int(time.time()) - (days * 24 * 60 * 60)
        with self.c:
            self.c.execute("""
                DELETE FROM jobs WHERE status IN ('completed', 'failed', 'stopped') AND ended_at < ?
            """, (cutoff,))
            return self.c.total_changes

    def reconcile(self):
        with self.c:
            jobs = self.c.execute("SELECT id, systemd_unit FROM jobs WHERE systemd_unit IS NOT NULL").fetchall()
            for job in jobs:
                unit_name = job['systemd_unit']
                check_cmd = ['systemctl', '--user', 'show', unit_name, '--property=ActiveState,Result']
                result = subprocess.run(check_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    info = {}
                    for line in result.stdout.splitlines():
                        if '=' in line:
                            key, value = line.split('=', 1)
                            info[key] = value
                    if info.get('ActiveState') == 'inactive':
                        new_status = 'completed' if info.get('Result') == 'success' else 'failed'
                        self.c.execute("""
                            UPDATE jobs SET status = ? WHERE id = ?
                        """, (new_status, job['id']))
            return len(jobs)

    def list_jobs(self, status_filter=None):
        query = "SELECT * FROM jobs ORDER BY priority DESC, created_at DESC"
        if status_filter:
            query = f"SELECT * FROM jobs WHERE status = ? ORDER BY priority DESC, created_at DESC"
            return self.c.execute(query, (status_filter,)).fetchall()
        return self.c.execute(query).fetchall()

    def get_job_status(self, job_id):
        with self.c:
            job = self.c.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
            if not job:
                return None
            return dict(job)

    def stats(self):
        with self.c:
            stats = {
                'total': self.c.execute("SELECT COUNT(*) FROM jobs").fetchone()[0],
                'by_status': dict(self.c.execute("SELECT status, COUNT(*) FROM jobs GROUP BY status").fetchall()),
                'retry_counts': dict(self.c.execute("SELECT retry_count, COUNT(*) FROM jobs GROUP BY retry_count").fetchall())
            }
            return stats

def main():
    import argparse
    ap = argparse.ArgumentParser(description="Job Orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)

    a = sub.add_parser("add", help="Add a job")
    a.add_argument("name")
    a.add_argument("command")
    a.add_argument("args", nargs='*')
    a.add_argument("--env", action="append", default=[])
    a.add_argument("--cwd")
    a.add_argument("--schedule")
    a.add_argument("--priority", type=int, default=0)
    a.add_argument("--dependencies", nargs='*', type=int)
    a.add_argument("--systemd-properties", type=json.loads)

    s = sub.add_parser("start", help="Start a job")
    s.add_argument("job_id", type=int)

    t = sub.add_parser("stop", help="Stop a job")
    t.add_argument("job_id", type=int)

    l = sub.add_parser("list", help="List jobs")
    l.add_argument("--status", help="Filter by status")

    u = sub.add_parser("status", help="Get job status")
    u.add_argument("job_id", type=int)

    r = sub.add_parser("reclaim", help="Reclaim stalled jobs")
    r.add_argument("--timeout", type=int, default=300)

    c = sub.add_parser("cleanup", help="Clean up old jobs")
    c.add_argument("--days", type=int, default=7)

    sub.add_parser("reconcile", help="Reconcile job statuses with systemd")
    sub.add_parser("stats", help="Show job statistics")

    args = ap.parse_args()
    jo = JobOrchestrator()

    if args.cmd == "add":
        env = dict(e.split("=", 1) for e in args.env) if args.env else {}
        job_id = jo.add_job(
            name=args.name,
            cmd=args.command,
            args=args.args,
            env=env,
            cwd=args.cwd,
            schedule=args.schedule,
            priority=args.priority,
            dependencies=args.dependencies,
            systemd_properties=args.systemd_properties
        )
        print(f"Added job with ID: {job_id}")

    elif args.cmd == "start":
        success, msg = jo.start_job(args.job_id)
        if success:
            print(f"Started job: {msg}")
        else:
            print(f"Error: {msg}")

    elif args.cmd == "stop":
        success, msg = jo.stop_job(args.job_id)
        if success:
            print(f"Stopped job: {msg}")
        else:
            print(f"Error: {msg}")

    elif args.cmd == "list":
        jobs = jo.list_jobs(args.status)
        for job in jobs:
            print(f"{job['id']}: {job['name']} - {job['status']} (Priority: {job['priority']})")

    elif args.cmd == "status":
        status = jo.get_job_status(args.job_id)
        if status:
            print(json.dumps(status, indent=2))
        else:
            print("Job not found")

    elif args.cmd == "reclaim":
        count = jo.reclaim_stalled_jobs(args.timeout)
        print(f"Reclaimed {count} stalled jobs")

    elif args.cmd == "cleanup":
        count = jo.cleanup(args.days)
        print(f"Cleaned up {count} old jobs")

    elif args.cmd == "reconcile":
        count = jo.reconcile()
        print(f"Reconciled {count} jobs")

    elif args.cmd == "stats":
        stats = jo.stats()
        print(json.dumps(stats, indent=2))

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: perplexity.py
================================================================================

#!/usr/bin/env python3
"""
Unified Job Orchestration + Management System
- SQLite-backed job store
- Dependencies, priorities, retry, scheduling
- Optional systemd transient units for managed jobs (if available)
- Clean process control; thread-safe
"""
import argparse, json, os, sys, time, sqlite3, threading, subprocess, shlex
from pathlib import Path

# ==== GLOBAL CONSTANTS ====
DB_PATH = os.environ.get("JOB_DB", str(Path.home() / ".job_tasks.db"))
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000"
]

# ==== DATABASE LAYER ====
class DB:
    def __init__(self, db_path=DB_PATH):
        self.lock = threading.RLock()
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        for p in PRAGMAS: self.conn.execute(p)
        self._init_schema()
    def _init_schema(self):
        self.conn.executescript("""
        CREATE TABLE IF NOT EXISTS jobs(
            id INTEGER PRIMARY KEY,
            name TEXT,
            cmd TEXT,
            args TEXT,
            env TEXT,
            priority INTEGER DEFAULT 0,
            status TEXT DEFAULT 'queued',
            schedule TEXT,
            at INTEGER DEFAULT (strftime('%s','now')*1000), -- epoch ms
            worker TEXT,
            result TEXT,
            error TEXT,
            dep TEXT,
            retry INTEGER DEFAULT 0,
            created_at INTEGER DEFAULT (strftime('%s','now')*1000),
            started_at INTEGER,
            ended_at INTEGER,
            rtprio INTEGER,
            nice INTEGER,
            slice TEXT,
            cpu_weight INTEGER,
            mem_max_mb INTEGER,
            unit TEXT
        );
        CREATE INDEX IF NOT EXISTS idx_sched ON jobs(status,priority DESC,at,id);
        """)
    def add_job(self, **kw):
        with self.lock:
            dep = json.dumps(kw.get("dep")) if kw.get("dep") else None
            now = int(time.time()*1000)
            r = self.conn.execute("""
            INSERT INTO jobs (name,cmd,args,env,priority,schedule,at,dep,rtprio,nice,slice,cpu_weight,mem_max_mb,created_at) 
            VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?)
            """, (kw.get("name"), kw.get("cmd"), json.dumps(kw.get("args") or []), json.dumps(kw.get("env") or {}),
                  kw.get("priority",0), kw.get("schedule"), kw.get("at", now), dep, kw.get("rtprio"),
                  kw.get("nice"), kw.get("slice"), kw.get("cpu_weight"), kw.get("mem_max_mb"), now))
            return r.lastrowid
    def list_jobs(self):
        with self.lock:
            return self.conn.execute("SELECT * FROM jobs ORDER BY created_at DESC").fetchall()
    def pop_job(self, worker):
        with self.lock:
            now = int(time.time() * 1000)
            row = self.conn.execute("""
                SELECT * FROM jobs
                WHERE status='queued' AND at<=?
                AND (dep IS NULL OR NOT EXISTS (
                    SELECT 1 FROM json_each(jobs.dep) d
                    JOIN jobs AS jb ON jb.id=d.value WHERE jb.status!='done'))
                ORDER BY priority DESC, at, id LIMIT 1
            """, (now,)).fetchone()
            if not row: return None
            r = self.conn.execute("""
                UPDATE jobs SET status='running', worker=?, started_at=?
                WHERE id=? AND status='queued'
                RETURNING id, name, cmd, args, env
            """, (worker, now, row['id'])).fetchone()
            return dict(r) if r else None
    def complete_job(self, job_id, ok=True, result=None, error=None, worker=None):
        with self.lock:
            now = int(time.time()*1000)
            st = self.conn.execute("SELECT started_at, created_at, retry FROM jobs WHERE id=?", (job_id,)).fetchone()
            if not st: return
            if ok:
                self.conn.execute("""
                    UPDATE jobs SET status='done', ended_at=?, result=?, worker=NULL WHERE id=?
                """, (now, json.dumps(result) if result else None, job_id))
            elif st['retry'] < 3:
                delay = 1000 * (2 ** st['retry'])
                self.conn.execute("""
                    UPDATE jobs SET status='queued', at=?, retry=retry+1, error=?, worker=NULL WHERE id=?
                """, (now + delay, error, job_id))
            else:
                self.conn.execute("""
                    UPDATE jobs SET status='failed', ended_at=?, error=?, worker=NULL WHERE id=?
                """, (now, error, job_id))
    def cleanup_jobs(self, days=7):
        threshold = int(time.time()*1000) - days*86400000
        with self.lock:
            delcount = self.conn.execute(
                "DELETE FROM jobs WHERE status IN ('done','failed') AND ended_at < ?", (threshold,)).rowcount
            return delcount
    def update_status(self, job_id, status, unit=None):
        with self.lock:
            self.conn.execute("UPDATE jobs SET status=?, unit=? WHERE id=?", (status, unit, job_id))
    def get_job(self, job_id):
        with self.lock:
            return self.conn.execute("SELECT * FROM jobs WHERE id=?", (job_id,)).fetchone()
    def get_job_by_name(self, name):
        with self.lock:
            return self.conn.execute("SELECT * FROM jobs WHERE name=?", (name,)).fetchone()
    def update_unit(self, name, unit, status):
        with self.lock:
            self.conn.execute("UPDATE jobs SET unit=?, status=? WHERE name=?", (unit, status, name))

# ==== SYSTEMD LAYER (optional) ====
def systemd_run_args(job):
    # Generates args for systemd-run
    props = [
        "--property=StandardOutput=journal",
        "--property=StandardError=journal",
        "--property=KillMode=control-group"
    ]
    if job["rtprio"]: props += [f"--property=CPUSchedulingPolicy=rr", f"--property=CPUSchedulingPriority={job['rtprio']}"]
    if job["nice"]: props += [f"--property=Nice={job['nice']}"]
    if job["slice"]: props += [f"--slice={job['slice']}"]
    if job["cpu_weight"]: props += [f"--property=CPUWeight={job['cpu_weight']}"]
    if job["mem_max_mb"]: props += [f"--property=MemoryMax={job['mem_max_mb']}M"]
    env = []
    if job["env"]:
        try:
            for k,v in json.loads(job["env"]).items(): env += ["--setenv", f"{k}={v}"]
        except: pass
    when = []
    if job["schedule"]: when += ["--on-calendar", job["schedule"]]
    if job["cwd"]: props += [f"--property=WorkingDirectory={job['cwd']}"]
    args = []
    if job.get("args"): args += json.loads(job["args"])
    unit = "job-" + str(job['id']) + ".service"
    return ["systemd-run", "--user", "--collect", "--quiet", "--unit", unit] + props + env + when + ["--", job['cmd']] + args, unit

def run_job(job, db):
    # Native execution (if not systemd)
    try:
        p = subprocess.run([job['cmd']] + json.loads(job.get('args') or "[]"),
                           env={**os.environ, **json.loads(job['env'] or "{}")},
                           text=True, capture_output=True, timeout=290)
        db.complete_job(job['id'], p.returncode == 0,
                        {'stdout': p.stdout[:1000],'stderr':p.stderr[:1000]},
                        p.stderr if p.returncode != 0 else None)
        print(f"Job {job['id']} {job['name']} finished. Code={p.returncode}")
    except subprocess.TimeoutExpired:
        db.complete_job(job['id'], False, error="TIMEOUT")
    except Exception as e:
        db.complete_job(job['id'], False, error=str(e))

def start_systemd(job, db):
    import subprocess
    args, unit = systemd_run_args(job)
    cp = subprocess.run(args, text=True, capture_output=True)
    status = "scheduled" if job["schedule"] else "started"
    db.update_unit(job["name"], unit, status)
    if cp.returncode == 0:
        print(f"{unit}: scheduled/started")
    else:
        print(f"ERROR [{unit}]: {cp.stderr.strip()}")

# ==== WORKER LOOP ====
def worker_loop(db, batch=1, use_systemd=False):
    wid = f"w{os.getpid()}"
    running=True
    def sig(_1,_2): nonlocal running; running=False
    try: import signal; signal.signal(signal.SIGTERM,sig); signal.signal(signal.SIGINT,sig)
    except: pass
    print(f"Worker {wid} started; batch={batch} systemd={use_systemd}")
    while running:
        tasks = []
        for _ in range(batch):
            t = db.pop_job(wid)
            if t: tasks.append(t)
        if not tasks:
            time.sleep(0.05)
            continue
        for task in tasks:
            if use_systemd: start_systemd(task, db)
            else: run_job(task, db)

# ==== CLI HANDLER ====
def main():
    ap = argparse.ArgumentParser(description="Unified Job Orchestrator")
    sp = ap.add_subparsers(dest="cmd", required=True)
    a = sp.add_parser("add", help="Add new job")
    a.add_argument("name"); a.add_argument("command")
    a.add_argument("args", nargs="*")
    a.add_argument("--env", action="append", default=[], help="KEY=VAL")
    a.add_argument("--priority", type=int, default=0)
    a.add_argument("--delay", type=int, default=0)
    a.add_argument("--deps", default=None)
    a.add_argument("--schedule")
    a.add_argument("--rtprio", type=int)
    a.add_argument("--nice", type=int)
    a.add_argument("--slice")
    a.add_argument("--cpu-weight", type=int)
    a.add_argument("--mem-max-mb", type=int)
    a.add_argument("--start", action="store_true")
    w = sp.add_parser("worker", help="Run worker loop")
    w.add_argument("--batch", type=int, default=1)
    w.add_argument("--systemd", action="store_true")
    sp.add_parser("list", help="List all jobs")
    cln = sp.add_parser("cleanup", help="Clean old jobs")
    cln.add_argument("--days", type=int, default=7)
    st = sp.add_parser("status", help="Status of named job"); st.add_argument("name")
    sp.add_parser("bench", help="Benchmark inserts/perf")

    args = ap.parse_args()
    db = DB()
    if args.cmd == "add":
        env = {e.split('=',1)[0]:e.split('=',1)[1] for e in args.env} if args.env else {}
        deps = json.loads(args.deps) if args.deps else None
        at = int(time.time()*1000) + args.delay if args.delay else None
        job_id = db.add_job(name=args.name, cmd=args.command, args=args.args,
                            env=env, priority=args.priority, schedule=args.schedule,
                            at=at, dep=deps, rtprio=args.rtprio, nice=args.nice,
                            slice=args.slice, cpu_weight=args.cpu_weight,
                            mem_max_mb=args.mem_max_mb)
        print(f"Job {job_id} added")
        if args.start:
            job = db.get_job(job_id)
            run_job(job, db)
    elif args.cmd == "worker":
        worker_loop(db, batch=args.batch, use_systemd=args.systemd)
    elif args.cmd == "list":
        for r in db.list_jobs():
            print(f"{r['id']} {r['name']} [{r['status']}] unit={r['unit'] or '-'} sched={r['schedule'] or '-'}")
    elif args.cmd == "cleanup":
        deleted = db.cleanup_jobs(args.days)
        print(f"Deleted {deleted} old jobs")
    elif args.cmd == "status":
        job = db.get_job_by_name(args.name)
        print(json.dumps(dict(job) if job else {"error":"not found"}, indent=2))
    elif args.cmd == "bench":
        start = time.time()
        for i in range(1000):
            db.add_job(name=f"test{i}", cmd="echo", args=[str(i)])
        print(f"1000 inserts: {(time.time()-start)*1000:.2f}ms")
        print(f"List count: {len(db.list_jobs())}")

if __name__ == "__main__":
    main()

================================================================================

================================================================================
FILE: qwen.py
================================================================================

#!/usr/bin/env python3
"""
Unified Job Orchestrator: SQLite + systemd integration (<500 lines)
Combines task queue scheduling with systemd unit management for robust process control.
"""
import argparse, json, os, shlex, sqlite3, subprocess, sys, time, signal, threading
from pathlib import Path
from typing import Optional, Dict, Any

# Configuration
DB_PATH = Path.home() / ".job_orchestrator.db"
UNIT_PREFIX = "joborch-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

# Database setup with optimized pragmas
PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA cache_size=-8000",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA busy_timeout=5000",
]

class JobOrchestrator:
    """Unified job scheduler and systemd unit manager"""
    
    def __init__(self, db_path: str = DB_PATH):
        self.db_path = db_path
        self.lock = threading.RLock()
        self._init_db()
    
    def _init_db(self):
        """Initialize database with optimized schema"""
        with self._get_connection() as conn:
            for pragma in PRAGMAS:
                conn.execute(pragma)
            
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE,
                    cmd TEXT NOT NULL,
                    args TEXT,
                    env TEXT,
                    cwd TEXT,
                    schedule TEXT,
                    priority INTEGER DEFAULT 0,
                    rtprio INTEGER,
                    nice INTEGER,
                    slice TEXT,
                    cpu_weight INTEGER,
                    mem_max_mb INTEGER,
                    unit TEXT,
                    status TEXT DEFAULT 'pending',
                    created_at INTEGER DEFAULT (strftime('%s','now')),
                    started_at INTEGER,
                    ended_at INTEGER,
                    retries INTEGER DEFAULT 0,
                    last_error TEXT,
                    result TEXT
                );
                
                CREATE INDEX IF NOT EXISTS idx_status_priority 
                ON jobs(status, priority DESC, created_at) 
                WHERE status IN ('pending', 'running');
                
                CREATE TABLE IF NOT EXISTS metrics (
                    job_id INTEGER PRIMARY KEY,
                    queue_time REAL,
                    exec_time REAL,
                    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
                );
            """)
    
    def _get_connection(self):
        """Get database connection with row factory"""
        conn = sqlite3.connect(self.db_path, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        return conn
    
    def add_job(self, name: str, cmd: str, **kwargs) -> int:
        """Add a new job to the queue"""
        with self.lock, self._get_connection() as conn:
            # Convert complex types to JSON
            if 'args' in kwargs and kwargs['args'] is not None:
                kwargs['args'] = json.dumps(kwargs['args'])
            if 'env' in kwargs and kwargs['env'] is not None:
                kwargs['env'] = json.dumps(kwargs['env'])
            
            # Insert or replace job
            cursor = conn.execute("""
                INSERT OR REPLACE INTO jobs 
                (name, cmd, args, env, cwd, schedule, priority, rtprio, nice, 
                 slice, cpu_weight, mem_max_mb, unit, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'pending', strftime('%s','now'))
                """, (
                    name, cmd, 
                    kwargs.get('args'), 
                    kwargs.get('env'), 
                    kwargs.get('cwd'), 
                    kwargs.get('schedule'), 
                    kwargs.get('priority', 0),
                    kwargs.get('rtprio'), 
                    kwargs.get('nice'), 
                    kwargs.get('slice'), 
                    kwargs.get('cpu_weight'), 
                    kwargs.get('mem_max_mb'), 
                    None
                ))
            return cursor.lastrowid
    
    def get_next_job(self) -> Optional[Dict[str, Any]]:
        """Get the next eligible job to run"""
        with self.lock, self._get_connection() as conn:
            # Get next pending job (considering schedule if present)
            now = int(time.time())
            row = conn.execute("""
                SELECT * FROM jobs 
                WHERE status = 'pending' 
                AND (schedule IS NULL OR strftime('%s', 'now') >= strftime('%s', schedule))
                ORDER BY priority DESC, created_at
                LIMIT 1
            """).fetchone()
            
            if not row:
                return None
            
            # Mark as running
            conn.execute(
                "UPDATE jobs SET status = 'running', started_at = ? WHERE id = ?",
                (now, row['id'])
            )
            
            # Convert JSON strings back to objects
            job = dict(row)
            if job['args']:
                job['args'] = json.loads(job['args'])
            if job['env']:
                job['env'] = json.loads(job['env'])
            
            return job
    
    def start_job_as_systemd_unit(self, job: Dict[str, Any]) -> bool:
        """Start a job as a transient systemd unit"""
        try:
            unit_name = self._get_unit_name(job['name'])
            props = [
                "--property=StandardOutput=journal",
                "--property=StandardError=journal",
                "--property=KillMode=control-group",
            ]
            
            # Add resource and scheduling properties
            if job['rtprio']:
                props += ["--property=CPUSchedulingPolicy=rr",
                         f"--property=CPUSchedulingPriority={job['rtprio']}"]
            if job['nice'] is not None:
                props += [f"--property=Nice={int(job['nice'])}"]
            if job['slice']:
                props += [f"--slice={job['slice']}"]
            if job['cpu_weight']:
                props += [f"--property=CPUWeight={job['cpu_weight']}"]
            if job['mem_max_mb']:
                props += [f"--property=MemoryMax={int(job['mem_max_mb'])}M"]
            
            # Add environment variables
            env = []
            if job['env']:
                for k, v in job['env'].items():
                    env += ["--setenv", f"{k}={v}"]
            
            # Add working directory
            if job['cwd']:
                props += [f"--property=WorkingDirectory={job['cwd']}"]
            
            # Build command
            cmd_args = job['args'] if job['args'] else []
            full_cmd = [job['cmd']] + cmd_args
            
            # Execute systemd-run
            systemd_cmd = [*SYSDRUN, "--unit", unit_name, *props, *env, "--", *full_cmd]
            result = subprocess.run(systemd_cmd, capture_output=True, text=True)
            
            # Update job record
            with self._get_connection() as conn:
                if result.returncode == 0:
                    status = "running_systemd"
                    conn.execute(
                        "UPDATE jobs SET unit = ?, status = ? WHERE id = ?",
                        (unit_name, status, job['id'])
                    )
                    return True
                else:
                    error_msg = result.stderr.strip() or result.stdout.strip()
                    conn.execute(
                        "UPDATE jobs SET status = 'failed', last_error = ? WHERE id = ?",
                        (error_msg, job['id'])
                    )
                    return False
                    
        except Exception as e:
            with self._get_connection() as conn:
                conn.execute(
                    "UPDATE jobs SET status = 'failed', last_error = ? WHERE id = ?",
                    (str(e), job['id'])
                )
            return False
    
    def complete_job(self, job_id: int, success: bool = True, result: Any = None, error: str = None):
        """Mark a job as completed (success or failure)"""
        with self.lock, self._get_connection() as conn:
            now = int(time.time())
            
            if success:
                # Record success
                conn.execute("""
                    UPDATE jobs SET status = 'completed', ended_at = ?, result = ?
                    WHERE id = ?
                """, (now, json.dumps(result) if result else None, job_id))
                
                # Record metrics
                row = conn.execute(
                    "SELECT created_at, started_at FROM jobs WHERE id = ?", 
                    (job_id,)
                ).fetchone()
                
                if row and row['started_at']:
                    queue_time = row['started_at'] - row['created_at']
                    exec_time = now - row['started_at']
                    conn.execute(
                        "INSERT OR REPLACE INTO metrics(job_id, queue_time, exec_time) VALUES(?, ?, ?)",
                        (job_id, queue_time, exec_time)
                    )
            else:
                # Handle failure with retry logic
                row = conn.execute(
                    "SELECT retries FROM jobs WHERE id = ?", 
                    (job_id,)
                ).fetchone()
                
                if row and row['retries'] < 3:
                    # Exponential backoff retry
                    delay = 60 * (2 ** row['retries'])  # 1m, 2m, 4m
                    new_schedule = now + delay
                    conn.execute("""
                        UPDATE jobs SET status = 'pending', retries = retries + 1, 
                        last_error = ?, schedule = datetime(?, 'unixepoch')
                        WHERE id = ?
                    """, (error or "Unknown error", new_schedule, job_id))
                else:
                    # Final failure
                    conn.execute("""
                        UPDATE jobs SET status = 'failed', ended_at = ?, last_error = ?
                        WHERE id = ?
                    """, (now, error or "Exceeded retry limit", job_id))
    
    def stop_job(self, name: str):
        """Stop a running job"""
        unit_name = self._get_unit_name(name)
        
        # Stop the systemd unit if it exists
        subprocess.run(SYSTEMCTL + ["stop", unit_name], capture_output=True)
        subprocess.run(SYSTEMCTL + ["stop", unit_name.replace(".service", ".timer")], capture_output=True)
        
        # Update database
        with self._get_connection() as conn:
            conn.execute(
                "UPDATE jobs SET status = 'stopped', ended_at = ? WHERE name = ?",
                (int(time.time()), name)
            )
    
    def get_job_status(self, name: str) -> Dict[str, Any]:
        """Get the status of a job"""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM jobs WHERE name = ?", 
                (name,)
            ).fetchone()
            
            if not row:
                return {"error": "Job not found"}
            
            job = dict(row)
            
            # Get systemd unit status if applicable
            if job['unit']:
                unit_info = self._get_unit_info(job['unit'])
                job.update({
                    "unit_active_state": unit_info.get("ActiveState"),
                    "unit_result": unit_info.get("Result"),
                    "unit_pid": unit_info.get("MainPID")
                })
            
            return job
    
    def list_jobs(self, status: Optional[str] = None) -> list:
        """List all jobs, optionally filtered by status"""
        with self._get_connection() as conn:
            if status:
                rows = conn.execute(
                    "SELECT * FROM jobs WHERE status = ? ORDER BY created_at DESC",
                    (status,)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT * FROM jobs ORDER BY created_at DESC"
                ).fetchall()
            
            jobs = []
            for row in rows:
                job = dict(row)
                # Convert JSON fields
                if job['args']:
                    job['args'] = json.loads(job['args'])
                if job['env']:
                    job['env'] = json.loads(job['env'])
                jobs.append(job)
            
            return jobs
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        with self._get_connection() as conn:
            # Job counts by status
            counts = {row['status']: row['count'] for row in conn.execute(
                "SELECT status, COUNT(*) as count FROM jobs GROUP BY status"
            )}
            
            # Performance metrics
            metrics = conn.execute("""
                SELECT 
                    AVG(queue_time) as avg_queue_time,
                    AVG(exec_time) as avg_exec_time,
                    MAX(queue_time) as max_queue_time,
                    MAX(exec_time) as max_exec_time
                FROM metrics
            """).fetchone()
            
            return {
                "job_counts": counts,
                "metrics": dict(metrics) if metrics else {}
            }
    
    def cleanup_old_jobs(self, days: int = 7):
        """Clean up completed/failed jobs older than specified days"""
        cutoff = int(time.time()) - (days * 86400)
        with self._get_connection() as conn:
            deleted = conn.execute(
                "DELETE FROM jobs WHERE status IN ('completed', 'failed') AND ended_at < ?",
                (cutoff,)
            ).rowcount
            return deleted
    
    def reconcile_systemd_units(self):
        """Reconcile job statuses with systemd unit states"""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT id, name, unit, status FROM jobs WHERE unit IS NOT NULL AND status IN ('running_systemd', 'running')"
            ).fetchall()
            
            for row in rows:
                unit_info = self._get_unit_info(row['unit'])
                if unit_info and unit_info.get("ActiveState") in ("inactive", "failed"):
                    new_status = "completed" if unit_info.get("Result") == "success" else "failed"
                    conn.execute(
                        "UPDATE jobs SET status = ?, ended_at = ? WHERE id = ?",
                        (new_status, int(time.time()), row['id'])
                    )
    
    def _get_unit_name(self, name: str) -> str:
        """Generate a safe systemd unit name"""
        safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
        return f"{UNIT_PREFIX}{safe}.service"
    
    def _get_unit_info(self, unit: str) -> Dict[str, str]:
        """Get information about a systemd unit"""
        try:
            result = subprocess.run(
                SYSTEMCTL + ["show", unit, "--property=ActiveState,Result,MainPID"],
                capture_output=True, text=True
            )
            
            if result.returncode != 0:
                return {}
            
            info = {}
            for line in result.stdout.strip().splitlines():
                if "=" in line:
                    key, value = line.split("=", 1)
                    info[key] = value
            return info
        except Exception:
            return {}


class Worker:
    """Background worker that processes jobs"""
    
    def __init__(self, orchestrator: JobOrchestrator):
        self.orchestrator = orchestrator
        self.running = True
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        """Setup graceful shutdown handlers"""
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)
    
    def _shutdown(self, signum, frame):
        """Handle shutdown signals"""
        print(f"Received signal {signum}, shutting down gracefully...")
        self.running = False
    
    def run(self):
        """Main worker loop"""
        print("Job Orchestrator Worker started")
        counter = 0
        
        while self.running:
            counter += 1
            
            # Periodic maintenance
            if counter % 30 == 0:  # Every ~30 seconds
                self.orchestrator.reconcile_systemd_units()
                print("Performed systemd unit reconciliation")
            
            # Get and process next job
            job = self.orchestrator.get_next_job()
            if job:
                print(f"Starting job: {job['name']}")
                success = self.orchestrator.start_job_as_systemd_unit(job)
                if not success:
                    self.orchestrator.complete_job(
                        job['id'], False, 
                        error="Failed to start systemd unit"
                    )
            else:
                # Sleep briefly if no jobs available
                time.sleep(1)


def main():
    """Command line interface"""
    parser = argparse.ArgumentParser(description="Unified Job Orchestrator")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # Add job command
    add_parser = subparsers.add_parser("add", help="Add a new job")
    add_parser.add_argument("name", help="Job name")
    add_parser.add_argument("command", help="Command to execute")
    add_parser.add_argument("args", nargs="*", help="Command arguments")
    add_parser.add_argument("--priority", type=int, default=0, help="Job priority")
    add_parser.add_argument("--env", action="append", help="Environment variables (KEY=VALUE)")
    add_parser.add_argument("--cwd", help="Working directory")
    add_parser.add_argument("--schedule", help="Schedule in systemd calendar format")
    add_parser.add_argument("--rtprio", type=int, help="Real-time priority")
    add_parser.add_argument("--nice", type=int, help="Nice value")
    add_parser.add_argument("--slice", help="Systemd slice")
    add_parser.add_argument("--cpu-weight", type=int, help="CPU weight")
    add_parser.add_argument("--mem-max-mb", type=int, help="Memory limit in MB")
    
    # List jobs command
    list_parser = subparsers.add_parser("list", help="List jobs")
    list_parser.add_argument("--status", help="Filter by status")
    
    # Start worker command
    subparsers.add_parser("worker", help="Start background worker")
    
    # Stop job command
    stop_parser = subparsers.add_parser("stop", help="Stop a job")
    stop_parser.add_argument("name", help="Job name")
    
    # Status command
    status_parser = subparsers.add_parser("status", help="Get job status")
    status_parser.add_argument("name", help="Job name")
    
    # Stats command
    subparsers.add_parser("stats", help="Show system statistics")
    
    # Cleanup command
    cleanup_parser = subparsers.add_parser("cleanup", help="Clean up old jobs")
    cleanup_parser.add_argument("--days", type=int, default=7, help="Days to keep completed jobs")
    
    # Reconcile command
    subparsers.add_parser("reconcile", help="Reconcile job statuses with systemd")
    
    args = parser.parse_args()
    orchestrator = JobOrchestrator()
    
    if args.command == "add":
        env = {}
        if args.env:
            for e in args.env:
                if "=" in e:
                    key, value = e.split("=", 1)
                    env[key] = value
        
        job_id = orchestrator.add_job(
            name=args.name,
            cmd=args.command,
            args=args.args if args.args else [],
            env=env if env else None,
            cwd=args.cwd,
            schedule=args.schedule,
            priority=args.priority,
            rtprio=args.rtprio,
            nice=args.nice,
            slice=args.slice,
            cpu_weight=args.cpu_weight,
            mem_max_mb=args.mem_max_mb
        )
        print(f"Job added with ID: {job_id}")
    
    elif args.command == "list":
        jobs = orchestrator.list_jobs(status=args.status)
        for job in jobs:
            schedule = job['schedule'] if job['schedule'] else "-"
            print(f"{job['name']}: {job['status']} (priority: {job['priority']}, schedule: {schedule})")
    
    elif args.command == "worker":
        worker = Worker(orchestrator)
        worker.run()
    
    elif args.command == "stop":
        orchestrator.stop_job(args.name)
        print(f"Job '{args.name}' stopped")
    
    elif args.command == "status":
        status = orchestrator.get_job_status(args.name)
        print(json.dumps(status, indent=2))
    
    elif args.command == "stats":
        stats = orchestrator.get_stats()
        print(json.dumps(stats, indent=2))
    
    elif args.command == "cleanup":
        deleted = orchestrator.cleanup_old_jobs(args.days)
        print(f"Cleaned up {deleted} old jobs")
    
    elif args.command == "reconcile":
        orchestrator.reconcile_systemd_units()
        print("Reconciliation completed")


if __name__ == "__main__":
    main()

================================================================================
