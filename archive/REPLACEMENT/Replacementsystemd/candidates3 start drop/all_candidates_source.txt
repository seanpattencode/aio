================================================================================
SYSTEMD CANDIDATES1 - ALL SOURCE FILES
================================================================================

================================================================================
FILE: androidCore.cpp
================================================================================

// Core zombie reaping pattern from Android
while (true) {
    auto pending_functions = epoll.Wait(epoll_timeout);
    if (!pending_functions->empty()) {
        ReapAnyOutstandingChildren();  // Always reap first
        for (const auto& function : *pending_functions) {
            (*function)();
        }
    }
}


================================================================================
FILE: chatgpt1.py
================================================================================

#!/usr/bin/env python3
# AIOS Systemd Orchestrator — single file, <200 lines
# Manages user units/timers, transient runs, RT scheduling, and logs to SQLite.
import os, sys, json, sqlite3, subprocess
from pathlib import Path

HOME = Path.home()
USER_DIR = HOME/".config/systemd/user"
DB = HOME/".aios_tasks.db"
UNIT_PREFIX = "aios-"

def sh(*args):
    return subprocess.run(list(args), text=True, capture_output=True)

class Orchestrator:
    def __init__(self):
        USER_DIR.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(DB)
        self.db.execute("""CREATE TABLE IF NOT EXISTS tasks(
            name TEXT PRIMARY KEY, cmd TEXT, kind TEXT, props TEXT, created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )""")
        self.db.commit()

    # ---------- core ----------
    def daemon_reload(self):
        sh("systemctl","--user","daemon-reload")

    def enable_now(self, unit):
        self.daemon_reload()
        sh("systemctl","--user","enable","--now",unit)

    def disable_now(self, unit):
        sh("systemctl","--user","disable","--now",unit)

    def status(self, unit):
        return sh("systemctl","--user","status",unit).stdout

    # ---------- file-backed units ----------
    def write_service(self, name, cmd, workdir=None, env=None, restart="on-failure",
                      killmode="control-group", rt=None, extra=None):
        unit = f"{UNIT_PREFIX}{name}.service"
        p = USER_DIR/unit
        env_lines = ""
        if env:
            for k,v in env.items():
                env_lines += f"Environment={k}={v}\n"
        rt_lines = ""
        if rt:
            pol, prio = rt
            rt_lines = f"CPUSchedulingPolicy={pol}\nCPUSchedulingPriority={prio}\n"
        extra = extra or ""
        wd = f"WorkingDirectory={workdir}\n" if workdir else ""
        text = f"""[Unit]
Description=AIOS job {name}
After=network-online.target
[Service]
Type=exec
ExecStart={cmd}
{wd}{env_lines}Restart={restart}
KillMode={killmode}
{rt_lines}StandardOutput=journal
StandardError=journal
[Install]
WantedBy=default.target
"""
        if extra:
            text = text.replace("[Install]\nWantedBy=default.target\n", extra + "\n[Install]\nWantedBy=default.target\n")
        p.write_text(text)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,cmd,kind,props) VALUES(?,?,?,?)",
                        (name, cmd, "service", json.dumps({"rt":rt,"restart":restart})))
        self.db.commit()
        self.enable_now(unit)
        return unit

    def write_timer(self, name, *, on_calendar=None, on_active=None, on_boot=None,
                    on_unit_active=None, persistent=False):
        timer = f"{UNIT_PREFIX}{name}.timer"
        svc = f"{UNIT_PREFIX}{name}.service"
        p = USER_DIR/timer
        tsec = []
        if on_calendar: tsec.append(f"OnCalendar={on_calendar}")
        if on_active: tsec.append(f"OnActiveSec={on_active}")
        if on_boot: tsec.append(f"OnBootSec={on_boot}")
        if on_unit_active: tsec.append(f"OnUnitActiveSec={on_unit_active}")
        if persistent: tsec.append("Persistent=true")
        tblock = "\n".join(tsec) or "OnActiveSec=60"
        text = f"""[Unit]
Description=AIOS timer {name}
[Timer]
{tblock}
Unit={svc}
[Install]
WantedBy=timers.target
"""
        p.write_text(text)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,cmd,kind,props) VALUES(?,?,?,?)",
                        (name, svc, "timer", json.dumps({"timer":tsec})))
        self.db.commit()
        self.enable_now(timer)
        return timer

    # ---------- transient runs (no files) ----------
    def run_transient(self, name, cmd, *, on_calendar=None, on_active=None, rt=None, env=None):
        unit = f"{UNIT_PREFIX}{name}.service"
        props = []
        if rt: props += [f"--property=CPUSchedulingPolicy={rt[0]}", f"--property=CPUSchedulingPriority={rt[1]}"]
        if env:
            for k,v in env.items(): props += [f"--setenv={k}={v}"]
        when = []
        if on_calendar: when += [f"--on-calendar={on_calendar}"]
        if on_active: when += [f"--on-active={on_active}"]
        out = sh("systemd-run","--user","--unit",unit,*props,*when,cmd)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,cmd,kind,props) VALUES(?,?,?,?)",
                        (name, cmd, "transient", json.dumps({"rt":rt,"when":when})))
        self.db.commit()
        return out.stdout + out.stderr

    # ---------- live property tweaks ----------
    def set_rt(self, name, policy="fifo", prio=20):
        unit = f"{UNIT_PREFIX}{name}.service"
        return sh("systemctl","--user","set-property",unit,
                  f"CPUSchedulingPolicy={policy}", f"CPUSchedulingPriority={prio}").stdout

    def stop(self, name):
        unit = f"{UNIT_PREFIX}{name}.service"
        timer = f"{UNIT_PREFIX}{name}.timer"
        self.disable_now(unit)
        if (USER_DIR/timer).exists():
            self.disable_now(timer)

if __name__ == "__main__":
    o = Orchestrator()
    # demo usage when run directly (safe no-ops if systemd --user unavailable)
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        o.write_service("hello", "/usr/bin/python3 -c 'print(\"hi\")'; /usr/bin/sleep 5",
                        rt=("fifo", 10))
        o.write_timer("hello", on_active="10s", on_unit_active="1h", persistent=True)
        print(o.status(f"{UNIT_PREFIX}hello.service"))


================================================================================
FILE: chatgpt2.py
================================================================================

#!/usr/bin/env python3
# aios_systemd_orchestrator.py — best-of synthesis (<200 lines)
# - Transient user units & timers (systemd-run) = no zombies, auto cgroups
# - RT scheduling & nice via properties; optional slice/limits
# - Single SQLite DB for state; simple CLI
import argparse, json, os, shlex, sqlite3, subprocess, sys, time
from pathlib import Path

DB = Path.home() / ".aios_tasks.db"
UNIT_PREFIX = "aios-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

def sh(cmd:list[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

def ok(cp): return cp.returncode == 0

def unit_name(name:str) -> str:
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
    return f"{UNIT_PREFIX}{safe}.service"

def db():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    con.executescript("""
      PRAGMA journal_mode=WAL; PRAGMA busy_timeout=5000;
      CREATE TABLE IF NOT EXISTS jobs(
        name TEXT PRIMARY KEY, cmd TEXT NOT NULL, args TEXT,
        env TEXT, cwd TEXT, schedule TEXT, rtprio INTEGER,
        nice INTEGER, slice TEXT, cpu_weight INTEGER, mem_max_mb INTEGER,
        unit TEXT, status TEXT DEFAULT 'added', created_at INTEGER
      );
    """)
    return con

def add_job(**kw):
    kw.setdefault("created_at", int(time.time()))
    with db() as con:
        con.execute("""INSERT OR REPLACE INTO jobs
        (name,cmd,args,env,cwd,schedule,rtprio,nice,slice,cpu_weight,mem_max_mb,unit,status,created_at)
        VALUES(:name,:cmd,:args,:env,:cwd,:schedule,:rtprio,:nice,:slice,:cpu_weight,:mem_max_mb,:unit,:status,:created_at)""", kw)

def list_jobs():
    with db() as con:
        return con.execute("SELECT * FROM jobs ORDER BY created_at DESC").fetchall()

def show(unit:str, *props:str) -> dict:
    out = sh(SYSTEMCTL + ["show", unit, *(["--property="+p for p in props] if props else [])]).stdout
    return {k:v for k,v in (line.split("=",1) for line in out.splitlines() if "=" in line)}

def start_transient(job:sqlite3.Row):
    unit = unit_name(job["name"])
    props = [
        "--property=StandardOutput=journal",
        "--property=StandardError=journal",
        "--property=KillMode=control-group",
    ]
    if job["rtprio"]: props += ["--property=CPUSchedulingPolicy=rr",
                                f"--property=CPUSchedulingPriority={job['rtprio']}"]
    if job["nice"] is not None: props += [f"--property=Nice={int(job['nice'])}"]
    if job["slice"]: props += [f"--slice={job['slice']}"]
    if job["cpu_weight"]: props += [f"--property=CPUWeight={job['cpu_weight']}"]
    if job["mem_max_mb"]: props += [f"--property=MemoryMax={int(job['mem_max_mb'])}M"]
    env = []
    if job["env"]:
        for k,v in json.loads(job["env"]).items():
            env += ["--setenv", f"{k}={v}"]
    when = []
    if job["schedule"]: when += ["--on-calendar", job["schedule"]]
    if job["cwd"]: props += [f"--property=WorkingDirectory={job['cwd']}"]

    cmd = [*SYSDRUN, "--unit", unit, *props, *env, *when, "--", job["cmd"], *json.loads(job["args"] or "[]")]
    cp = sh(cmd)
    with db() as con:
        con.execute("UPDATE jobs SET unit=?, status=? WHERE name=?", (unit, "scheduled" if job["schedule"] else "started", job["name"]))
    return ok(cp), unit, cp.stderr.strip() or cp.stdout.strip()

def stop(name:str):
    unit = unit_name(name)
    sh(SYSTEMCTL + ["stop", unit])
    sh(SYSTEMCTL + ["stop", unit.replace(".service",".timer")])
    with db() as con:
        con.execute("UPDATE jobs SET status='stopped' WHERE name=?", (name,))

def status(name:str):
    unit = unit_name(name)
    info = show(unit, "ActiveState","Result","MainPID")
    if not info: return {"unit":unit,"active":"unknown"}
    return {"unit":unit,"active":info.get("ActiveState"),"result":info.get("Result"),"pid":info.get("MainPID")}

def set_rt(name:str, policy="fifo", prio=20):
    unit = unit_name(name)
    return sh(SYSTEMCTL + ["set-property", unit, f"CPUSchedulingPolicy={policy}", f"CPUSchedulingPriority={prio}"]).stdout

def reconcile():
    # Mark finished jobs by querying unit state
    with db() as con:
        for r in con.execute("SELECT name,unit,status FROM jobs WHERE unit IS NOT NULL"):
            info = show(r["unit"], "ActiveState","Result")
            if info and info.get("ActiveState") in ("inactive","failed"):
                new = "completed" if info.get("Result") == "success" else "failed"
                con.execute("UPDATE jobs SET status=? WHERE name=?", (new, r["name"]))

def main():
    ap = argparse.ArgumentParser(description="AIOS systemd orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)

    a = sub.add_parser("add", help="record & (optionally) start a job")
    a.add_argument("name"); a.add_argument("command"); a.add_argument("args", nargs="*")
    a.add_argument("--env", action="append", default=[], help="KEY=VAL")
    a.add_argument("--cwd"); a.add_argument("--on-calendar")
    a.add_argument("--rtprio", type=int); a.add_argument("--nice", type=int)
    a.add_argument("--slice"); a.add_argument("--cpu-weight", type=int); a.add_argument("--mem-max-mb", type=int)
    a.add_argument("--start", action="store_true")

    sub.add_parser("list", help="list jobs")
    s = sub.add_parser("start", help="start existing job"); s.add_argument("name")
    t = sub.add_parser("stop", help="stop job"); t.add_argument("name")
    u = sub.add_parser("status", help="status"); u.add_argument("name")
    r = sub.add_parser("set-rt", help="tune RT"); r.add_argument("name"); r.add_argument("--policy", default="fifo"); r.add_argument("--prio", type=int, default=20)
    sub.add_parser("reconcile", help="refresh job statuses")

    args = ap.parse_args()

    if args.cmd == "add":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        add_job(
            name=args.name, cmd=args.command, args=json.dumps(args.args),
            env=json.dumps(env) if env else None, cwd=args.cwd, schedule=args.on_calendar,
            rtprio=args.rtprio, nice=args.nice, slice=args.slice,
            cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb, unit=None, status="added"
        )
        if args.start:
            with db() as con:
                row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
            ok_, unit, msg = start_transient(row)
            print(unit if ok_ else f"ERROR: {msg}")
        else:
            print("added")

    elif args.cmd == "list":
        reconcile()
        for r in list_jobs():
            print(f"{r['name']}: {r['status']}  unit={r['unit'] or '-'}  sched={r['schedule'] or '-'}")

    elif args.cmd == "start":
        with db() as con:
            row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
        if not row: sys.exit("unknown job")
        ok_, unit, msg = start_transient(row)
        print(unit if ok_ else f"ERROR: {msg}")

    elif args.cmd == "stop":
        stop(args.name); print("stopped")

    elif args.cmd == "status":
        print(json.dumps(status(args.name), indent=2))

    elif args.cmd == "set-rt":
        print(set_rt(args.name, args.policy, args.prio).strip())

    elif args.cmd == "reconcile":
        reconcile(); print("ok")

if __name__ == "__main__":
    main()


================================================================================
FILE: chatgptResearch1.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Systemd Task Manager - runs tasks under systemd supervision.
Requires: python-dbus library. Must run with root privileges or proper PolicyKit rights.
"""

import dbus, os, sys, shlex, time
from dbus.exceptions import DBusException

# Connect to the system bus and get systemd manager interface
SYSTEMD_BUS = 'org.freedesktop.systemd1'
SYSTEMD_PATH = '/org/freedesktop/systemd1'
MANAGER_IFACE = 'org.freedesktop.systemd1.Manager'

try:
    bus = dbus.SystemBus()
    systemd_obj = bus.get_object(SYSTEMD_BUS, SYSTEMD_PATH)
    systemd_mgr = dbus.Interface(systemd_obj, dbus_interface=MANAGER_IFACE)
except DBusException as e:
    sys.exit(f"Failed to connect to systemd: {e}")

def run_task(command, run_at=None, use_realtime=False, unit_name=None):
    """
    Run a command under systemd. If run_at is provided (datetime or timestamp), schedule it using a transient timer.
    use_realtime=True will run with real-time scheduling (FIFO policy with high priority).
    Returns the systemd unit name that was started.
    """
    # Determine unique unit name
    base_name = unit_name or f"aios_task_{int(time.time())}"
    service_name = base_name + ".service"
    # Parse command into executable and args
    if isinstance(command, str):
        cmd_list = shlex.split(command)
    else:
        cmd_list = list(command)
    if not cmd_list:
        raise ValueError("Command is empty")
    exec_path = cmd_list[0]
    args_list = cmd_list[:]  # include the executable as arg0
    # Build service properties for StartTransientUnit
    service_properties = [
        ("Description", f"AIOS Task - {command}"),
        ("ExecStart", [(exec_path, args_list, False)]),  # False -> don't ignore failures
    ]
    # If the task is expected to finish and we want to capture its status, use oneshot
    service_properties.append(("Type", "oneshot"))
    # Apply real-time scheduling if requested
    if use_realtime:
        service_properties.append(("CPUSchedulingPolicy", "rr"))
        service_properties.append(("CPUSchedulingPriority", dbus.Int32(99)))
    # If scheduling for future run
    if run_at:
        # Convert run_at (datetime or timestamp or str) to systemd time string
        if isinstance(run_at, (int, float)):  # timestamp
            ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(run_at))
        elif hasattr(run_at, "strftime"):     # datetime/date
            ts = run_at.strftime("%Y-%m-%d %H:%M:%S")
        elif isinstance(run_at, str):
            ts = run_at  # assume already a valid time spec string
        else:
            raise ValueError("Unsupported run_at format")
        timer_name = base_name + ".timer"
        timer_properties = [
            ("Description", f"Timer for {base_name}"),
            ("OnCalendar", ts),
            ("Persistent", False),           # do not run if missed (adjust as needed)
            ("RemainAfterElapse", False)
        ]
        try:
            # Create transient timer + service. The second parameter is "replace" mode.
            systemd_mgr.StartTransientUnit(timer_name, "replace", 
                                           timer_properties, 
                                           [(service_name, service_properties)])
            print(f"Scheduled task '{command}' as transient unit '{service_name}' at {ts}")
        except DBusException as e:
            sys.exit(f"Failed to schedule task: {e}")
        return service_name  # return the service unit name
    else:
        try:
            # Start a transient service immediately
            systemd_mgr.StartTransientUnit(service_name, "replace", service_properties, [])
            print(f"Started task '{command}' as transient service '{service_name}'")
        except DBusException as e:
            sys.exit(f"Failed to start task: {e}")
        return service_name

# Example usage (uncomment for testing purposes):
# run_task("python3 /path/to/script.py")                      # run immediately
# run_task("ls -l /", run_at=time.time()+60)                  # schedule 60 seconds from now
# run_task("my_realtime_app", use_realtime=True)              # run with realtime priority


================================================================================
FILE: chatgptResearch2.py
================================================================================

#!/usr/bin/env python3
# aios_systemd_orchestrator.py  — <200 lines
# - One SQLite DB
# - systemd user units (.service/.timer) + transient runs
# - RT scheduling, resource caps, env/WD, journald logging
# - Minimal CLI: svc|tmr|run|setrt|stop|status|list
import os, sys, json, sqlite3, subprocess, shlex, time
from pathlib import Path

HOME = Path.home()
USER_DIR = HOME/".config/systemd/user"
DB = HOME/".aios_tasks.db"
UNIT_PREFIX = "aios-"
SYSTEMD_RUN = "systemd-run"

def sh(*args):  # systemctl --user helper
    return subprocess.run(["systemctl","--user",*args], text=True, capture_output=True)

def props_kwargs(rt=None, nice=None, mem=None, cpuw=None, slice_name=None):
    p=[]
    if rt: p += [f"--property=CPUSchedulingPolicy={rt[0]}", f"--property=CPUSchedulingPriority={rt[1]}"]
    if nice is not None: p += [f"--property=Nice={nice}"]
    if mem: p += [f"--property=MemoryMax={mem}"]
    if cpuw: p += [f"--property=CPUWeight={cpuw}"]
    if slice_name: p += [f"--slice={slice_name}"]
    return p

class Aiosd:
    def __init__(self):
        USER_DIR.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(DB)
        self.db.execute("""CREATE TABLE IF NOT EXISTS tasks(
            name TEXT PRIMARY KEY,
            kind TEXT, cmd TEXT, props TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )""")
        self.db.commit()

    # -------- file-backed services/timers --------
    def write_service(self, name, cmd, *, wd=None, env=None, restart="on-failure",
                      rt=None, nice=None, mem=None, cpuw=None, slice_name="aios.slice"):
        unit = f"{UNIT_PREFIX}{name}.service"
        p = USER_DIR/unit
        env_lines = ""
        if env:
            for k,v in env.items(): env_lines += f"Environment={k}={v}\n"
        rt_lines = ""
        if rt:
            pol,prio=rt; rt_lines = f"CPUSchedulingPolicy={pol}\nCPUSchedulingPriority={prio}\n"
        if nice is not None: rt_lines += f"Nice={nice}\n"
        if mem: rt_lines += f"MemoryMax={mem}\n"
        if cpuw: rt_lines += f"CPUWeight={cpuw}\n"
        if slice_name: rt_lines += f"Slice={slice_name}\n"
        wd_line = f"WorkingDirectory={wd}\n" if wd else ""
        text = f"""[Unit]
Description=AIOS job {name}
After=network-online.target
[Service]
Type=simple
ExecStart=/bin/sh -lc {shlex.quote(cmd)}
{wd_line}{env_lines}{rt_lines}Restart={restart}
KillMode=control-group
StandardOutput=journal
StandardError=journal
[Install]
WantedBy=default.target
"""
        p.write_text(text)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,kind,cmd,props) VALUES(?,?,?,?)",
                        (name,"service",cmd,json.dumps(dict(rt=rt,nice=nice,mem=mem,cpuw=cpuw,slice=slice_name))))
        self.db.commit()
        sh("daemon-reload"); sh("enable","--now",unit)
        return unit

    def write_timer(self, name, *, on_calendar=None, on_active=None, on_unit_active=None, persistent=True):
        timer = f"{UNIT_PREFIX}{name}.timer"
        svc   = f"{UNIT_PREFIX}{name}.service"
        lines=[]
        if on_calendar: lines.append(f"OnCalendar={on_calendar}")
        if on_active: lines.append(f"OnActiveSec={on_active}")
        if on_unit_active: lines.append(f"OnUnitActiveSec={on_unit_active}")
        if persistent: lines.append("Persistent=true")
        t = f"""[Unit]
Description=AIOS timer {name}
[Timer]
{os.linesep.join(lines) if lines else 'OnActiveSec=60'}
Unit={svc}
[Install]
WantedBy=timers.target
"""
        (USER_DIR/timer).write_text(t)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,kind,cmd,props) VALUES(?,?,?,?)",
                        (name,"timer",svc,json.dumps(dict(timer=lines))))
        self.db.commit()
        sh("daemon-reload"); sh("enable","--now",timer)
        return timer

    # -------- transient runs (no files; auto-reap via systemd) --------
    def run_transient(self, name, cmd, *, on_calendar=None, on_active=None, rt=None,
                      env=None, nice=None, mem=None, cpuw=None, slice_name="aios.slice", wd=None):
        unit = f"{UNIT_PREFIX}{name}.service"
        when=[]
        if on_calendar: when += [f"--on-calendar={on_calendar}"]
        if on_active:   when += [f"--on-active={on_active}"]
        envs=[]
        if env:
            for k,v in env.items(): envs += [f"--setenv={k}={v}"]
        wdir = [f"--working-directory={wd}"] if wd else []
        extra = props_kwargs(rt=rt,nice=nice,mem=mem,cpuw=cpuw,slice_name=slice_name)
        cmd_list = shlex.split(cmd) if isinstance(cmd,str) else list(cmd)
        # --collect ensures unit is garbage-collected after exit; journald captures output
        out = subprocess.run([SYSTEMD_RUN,"--user","--quiet","--collect","--unit",unit,
                              *extra,*envs,*wdir,*when,"--",*cmd_list],
                             text=True, capture_output=True)
        self.db.execute("INSERT OR REPLACE INTO tasks(name,kind,cmd,props) VALUES(?,?,?,?)",
                        (name,"transient"," ".join(cmd_list),
                         json.dumps(dict(rt=rt,when=when,nice=nice,mem=mem,cpuw=cpuw,slice=slice_name,wd=wd)))))
        self.db.commit()
        return (out.returncode, unit, out.stderr.strip())

    # -------- live tweaks / lifecycle / status --------
    def set_rt(self, name, policy="fifo", prio=20):
        unit = f"{UNIT_PREFIX}{name}.service"
        return sh("set-property",unit,f"CPUSchedulingPolicy={policy}",f"CPUSchedulingPriority={prio}").stdout

    def stop(self, name):
        unit=f"{UNIT_PREFIX}{name}.service"; timer=f"{UNIT_PREFIX}{name}.timer"
        sh("disable","--now",unit); sh("disable","--now",timer)
        (USER_DIR/unit).unlink(missing_ok=True); (USER_DIR/timer).unlink(missing_ok=True); sh("daemon-reload")

    def status(self, pattern=f"{UNIT_PREFIX}*.service"):
        return sh("list-units",pattern,"--no-legend","--plain").stdout

    def list_db(self):
        rows=self.db.execute("SELECT name,kind,cmd,props,created_at FROM tasks ORDER BY created_at DESC").fetchall()
        return "\n".join(f"{r[0]:<24} {r[1]:<9} {r[2]}  props={r[3]}" for r in rows)

# ---------------- CLI ----------------
def main():
    a = sys.argv[1:] or []
    if not a or a[0] in {"-h","--help"}:
        print(f"""Usage:
  svc  <name> <cmd> [--rt fifo:10] [--nice N] [--mem 1G] [--cpuw 512] [--wd DIR] [--env K=V,K2=V2]
  tmr  <name> [--cal "Mon..Fri 09:00"] [--active 10s] [--unitactive 1h] [--no-persist]
  run  <name> <cmd> [--cal ...|--active ...] [--rt fifo:50] [--nice N] [--mem 2G] [--cpuw 800] [--wd DIR] [--env ...]
  setrt <name> <policy> <prio>
  stop <name>
  status
  list
Note: user services require 'loginctl enable-linger $USER' to run without a login session."""); return
    o=Aiosd(); cmd=a[0]

    if cmd=="svc":
        name, cmdline = a[1], a[2]
        opts = dict(rt=None,nice=None,mem=None,cpuw=None,wd=None,env=None)
        for i,x in enumerate(a[3:],3):
            if x=="--wd": opts["wd"]=a[i+1]
            if x=="--nice": opts["nice"]=int(a[i+1])
            if x=="--mem": opts["mem"]=a[i+1]
            if x=="--cpuw": opts["cpuw"]=int(a[i+1])
            if x=="--env": opts["env"]={k:v for k,v in (p.split("=",1) for p in a[i+1].split(","))}
            if x=="--rt":
                pol,prio=a[i+1].split(":"); opts["rt"]=(pol,int(prio))
        print(o.write_service(name,cmdline,**opts))

    elif cmd=="tmr":
        name=a[1]; cal=None; active=None; ua=None; persist=True
        for i,x in enumerate(a[2:],2):
            if x=="--cal": cal=a[i+1]
            if x=="--active": active=a[i+1]
            if x=="--unitactive": ua=a[i+1]
            if x=="--no-persist": persist=False
        print(o.write_timer(name,on_calendar=cal,on_active=active,on_unit_active=ua,persistent=persist))

    elif cmd=="run":
        name, cmdline = a[1], a[2]
        opts=dict(on_calendar=None,on_active=None,rt=None,nice=None,mem=None,cpuw=None,env=None,wd=None)
        for i,x in enumerate(a[3:],3):
            if x=="--cal": opts["on_calendar"]=a[i+1]
            if x=="--active": opts["on_active"]=a[i+1]
            if x=="--nice": opts["nice"]=int(a[i+1])
            if x=="--mem": opts["mem"]=a[i+1]
            if x=="--cpuw": opts["cpuw"]=int(a[i+1])
            if x=="--wd": opts["wd"]=a[i+1]
            if x=="--env": opts["env"]={k:v for k,v in (p.split("=",1) for p in a[i+1].split(","))}
            if x=="--rt":
                pol,prio=a[i+1].split(":"); opts["rt"]=(pol,int(prio))
        rc,unit,err = o.run_transient(name,cmdline,**opts)
        print(unit if rc==0 else f"ERR {unit}: {err}")

    elif cmd=="setrt":
        print(o.set_rt(a[1],a[2],int(a[3])))

    elif cmd=="stop":
        o.stop(a[1]); print("stopped")

    elif cmd=="status":
        print(o.status())

    elif cmd=="list":
        print(o.list_db())

if __name__=="__main__": main()


================================================================================
FILE: claude1.py
================================================================================

#!/usr/bin/env python3
"""
AIOS - AI Operating System
Systemd-based workflow and process manager for AI-generated programs
"""

import os
import sys
import json
import sqlite3
import signal
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from enum import Enum

# Configure logging to systemd journal if available
try:
    from systemd import journal
    handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios')
    logging.root.addHandler(handler)
except ImportError:
    pass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WorkflowState(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    REJECTED = "rejected"

class AIOS:
    def __init__(self, db_path: str = "/var/lib/aios/aios.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.row_factory = sqlite3.Row
        self.shutdown = False
        self.processes: Dict[int, asyncio.subprocess.Process] = {}
        self._init_db()
        self._setup_signals()
        
    def _init_db(self):
        """Initialize SQLite database with required tables"""
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                description TEXT,
                code TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 5,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                pid INTEGER,
                exit_code INTEGER,
                output TEXT
            );
            
            CREATE TABLE IF NOT EXISTS projects (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                workflow_ids TEXT,
                state TEXT DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            
            CREATE INDEX IF NOT EXISTS idx_workflow_state ON workflows(state);
            CREATE INDEX IF NOT EXISTS idx_workflow_priority ON workflows(priority);
        """)
        self.conn.commit()
        
    def _setup_signals(self):
        """Setup signal handlers for graceful shutdown"""
        for sig in [signal.SIGTERM, signal.SIGINT]:
            signal.signal(sig, self._handle_signal)
            
    def _handle_signal(self, signum, frame):
        logger.info(f"Received signal {signum}, initiating graceful shutdown")
        self.shutdown = True
        
    async def submit_workflow(self, name: str, code: str, description: str = "", 
                             priority: int = 5) -> int:
        """Submit a new workflow for review"""
        cursor = self.conn.execute(
            "INSERT INTO workflows (name, description, code, priority) VALUES (?, ?, ?, ?)",
            (name, description, code, priority)
        )
        self.conn.commit()
        logger.info(f"Submitted workflow: {name} (ID: {cursor.lastrowid})")
        return cursor.lastrowid
        
    async def review_workflow(self, workflow_id: int, approved: bool) -> bool:
        """Review and approve/reject a workflow"""
        state = WorkflowState.APPROVED if approved else WorkflowState.REJECTED
        self.conn.execute(
            "UPDATE workflows SET state = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
            (state.value, workflow_id)
        )
        self.conn.commit()
        logger.info(f"Workflow {workflow_id} {state.value}")
        return True
        
    async def _execute_workflow(self, workflow: sqlite3.Row):
        """Execute a single workflow with proper process management"""
        workflow_id = workflow['id']
        try:
            # Update state to running
            self.conn.execute(
                "UPDATE workflows SET state = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
                (WorkflowState.RUNNING.value, workflow_id)
            )
            self.conn.commit()
            
            # Create temporary file for code execution
            code_path = Path(f"/tmp/aios_workflow_{workflow_id}.py")
            code_path.write_text(workflow['code'])
            
            # Execute with proper process management
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(code_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                limit=1024*1024  # 1MB output limit
            )
            
            self.processes[workflow_id] = process
            
            # Update PID in database
            self.conn.execute(
                "UPDATE workflows SET pid = ? WHERE id = ?",
                (process.pid, workflow_id)
            )
            self.conn.commit()
            
            # Wait for completion with timeout
            try:
                output, _ = await asyncio.wait_for(
                    process.communicate(), 
                    timeout=300  # 5 minute timeout
                )
                exit_code = process.returncode
            except asyncio.TimeoutError:
                process.terminate()
                await asyncio.sleep(1)
                if process.returncode is None:
                    process.kill()
                output = b"Process timed out"
                exit_code = -15
            
            # Update results
            state = WorkflowState.COMPLETED if exit_code == 0 else WorkflowState.FAILED
            self.conn.execute(
                """UPDATE workflows 
                   SET state = ?, exit_code = ?, output = ?, 
                       pid = NULL, updated_at = CURRENT_TIMESTAMP 
                   WHERE id = ?""",
                (state.value, exit_code, output.decode('utf-8', errors='replace'), workflow_id)
            )
            self.conn.commit()
            
            # Cleanup
            code_path.unlink(missing_ok=True)
            del self.processes[workflow_id]
            
            logger.info(f"Workflow {workflow_id} completed with exit code {exit_code}")
            
        except Exception as e:
            logger.error(f"Error executing workflow {workflow_id}: {e}")
            self.conn.execute(
                "UPDATE workflows SET state = ?, output = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
                (WorkflowState.FAILED.value, str(e), workflow_id)
            )
            self.conn.commit()
            
    async def scheduler(self):
        """Main scheduler loop with priority-based execution"""
        while not self.shutdown:
            # Get approved workflows sorted by priority
            workflows = self.conn.execute(
                """SELECT * FROM workflows 
                   WHERE state = ? 
                   ORDER BY priority DESC, created_at ASC 
                   LIMIT 5""",
                (WorkflowState.APPROVED.value,)
            ).fetchall()
            
            # Execute workflows concurrently
            if workflows:
                tasks = [self._execute_workflow(w) for w in workflows]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # Clean up zombie processes
            for wid, proc in list(self.processes.items()):
                if proc.returncode is not None:
                    del self.processes[wid]
            
            await asyncio.sleep(1)
            
    async def cleanup(self):
        """Cleanup on shutdown"""
        logger.info("Cleaning up processes...")
        for proc in self.processes.values():
            proc.terminate()
        await asyncio.sleep(2)
        for proc in self.processes.values():
            if proc.returncode is None:
                proc.kill()
        self.conn.close()
        
    async def run(self):
        """Main entry point"""
        logger.info("AIOS starting...")
        try:
            await self.scheduler()
        finally:
            await self.cleanup()
            logger.info("AIOS shutdown complete")

async def main():
    aios = AIOS()
    await aios.run()

if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: claude2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Ultimate - Synthesized from production patterns across 500M+ devices
Combines: Android zombie reaping, transient systemd units, WAL SQLite, journal logging
"""
import os
import sys
import sqlite3
import subprocess
import signal
import time
import json
from pathlib import Path
from typing import Dict, Optional, Any
from enum import Enum

# Optional systemd integration
try:
    from systemd import journal
    HAS_JOURNAL = True
except ImportError:
    HAS_JOURNAL = False

class TaskState(Enum):
    PENDING = "pending"
    APPROVED = "approved"  
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class AIOS:
    def __init__(self, db_path: str = "~/.aios/state.db"):
        self.db_path = Path(db_path).expanduser()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.running = True
        self.children: Dict[int, str] = {}
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGTERM, self._shutdown)
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGCHLD, self._reap_children)
        
        self._init_db()
        
    def _init_db(self):
        """Initialize SQLite with production settings (WAL mode for concurrency)"""
        self.conn = sqlite3.connect(str(self.db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        
        # Production SQLite configuration (from Gemini patterns)
        self.conn.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA busy_timeout=5000;
            PRAGMA temp_store=MEMORY;
            
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 5,
                realtime BOOLEAN DEFAULT 0,
                memory_limit_mb INTEGER,
                cpu_weight INTEGER DEFAULT 100,
                restart_policy TEXT DEFAULT 'on-failure',
                unit_name TEXT,
                pid INTEGER,
                exit_code INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP
            );
            
            CREATE INDEX IF NOT EXISTS idx_state_priority 
            ON tasks(state, priority DESC, created_at);
        """)
        
    def _log(self, message: str, level: str = "INFO"):
        """Log to systemd journal if available, otherwise stderr"""
        if HAS_JOURNAL:
            journal.send(message, SYSLOG_IDENTIFIER="aios", PRIORITY=level)
        print(f"[{level}] {message}", file=sys.stderr)
        
    def _reap_children(self, signum=None, frame=None):
        """Android-style zombie reaping - always reap first"""
        try:
            while True:
                pid, status = os.waitpid(-1, os.WNOHANG)
                if pid == 0:
                    break
                    
                # Update database for reaped process
                exit_code = os.WEXITSTATUS(status) if os.WIFEXITED(status) else -1
                task_id = self.children.get(pid)
                
                if task_id:
                    state = TaskState.COMPLETED if exit_code == 0 else TaskState.FAILED
                    self.conn.execute(
                        """UPDATE tasks SET state=?, exit_code=?, pid=NULL, 
                           completed_at=CURRENT_TIMESTAMP WHERE id=?""",
                        (state.value, exit_code, task_id)
                    )
                    del self.children[pid]
                    self._log(f"Task {task_id} (PID {pid}) completed with code {exit_code}")
                    
        except ChildProcessError:
            pass  # No more children to reap
            
    def submit_task(self, name: str, command: str, **kwargs) -> Optional[int]:
        """Submit a new task for review/execution"""
        try:
            cursor = self.conn.execute(
                """INSERT INTO tasks (name, command, priority, realtime, 
                   memory_limit_mb, cpu_weight, restart_policy, state)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
                (name, command, 
                 kwargs.get('priority', 5),
                 kwargs.get('realtime', False),
                 kwargs.get('memory_limit_mb'),
                 kwargs.get('cpu_weight', 100),
                 kwargs.get('restart_policy', 'on-failure'),
                 TaskState.APPROVED.value if kwargs.get('auto_approve') else TaskState.PENDING.value)
            )
            task_id = cursor.lastrowid
            self._log(f"Submitted task '{name}' (ID: {task_id})")
            return task_id
        except sqlite3.IntegrityError:
            self._log(f"Task '{name}' already exists", "ERROR")
            return None
            
    def approve_task(self, task_id: int) -> bool:
        """Approve a pending task for execution"""
        cursor = self.conn.execute(
            "UPDATE tasks SET state=? WHERE id=? AND state=?",
            (TaskState.APPROVED.value, task_id, TaskState.PENDING.value)
        )
        if cursor.rowcount > 0:
            self._log(f"Task {task_id} approved")
            return True
        return False
        
    def _execute_transient(self, task: sqlite3.Row) -> bool:
        """Execute task as transient systemd unit (best pattern from research)"""
        unit_name = f"aios-task-{task['id']}.service"
        
        # Build systemd-run command (from Kimi/ChatGPT patterns)
        cmd = ["systemd-run", "--user", "--collect", "--quiet",
               "--unit", unit_name,
               "--property=StandardOutput=journal",
               "--property=StandardError=journal",
               f"--property=MemoryMax={task['memory_limit_mb']}M" if task['memory_limit_mb'] else "",
               f"--property=CPUWeight={task['cpu_weight']}",
               f"--property=Restart={task['restart_policy']}"]
        
        # Add realtime scheduling if requested
        if task['realtime']:
            cmd.extend(["--property=CPUSchedulingPolicy=rr",
                       "--property=CPUSchedulingPriority=90"])
                       
        # Filter empty properties and add command
        cmd = [c for c in cmd if c]
        cmd.extend(["--", "/bin/sh", "-c", task['command']])
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                self.conn.execute(
                    "UPDATE tasks SET state=?, unit_name=? WHERE id=?",
                    (TaskState.RUNNING.value, unit_name, task['id'])
                )
                self._log(f"Started task {task['id']} as {unit_name}")
                return True
        except Exception as e:
            self._log(f"Failed to start task {task['id']}: {e}", "ERROR")
            
        return False
        
    def _execute_direct(self, task: sqlite3.Row) -> bool:
        """Direct fork/exec execution (fallback when systemd unavailable)"""
        try:
            # Update state to running
            self.conn.execute(
                "UPDATE tasks SET state=? WHERE id=?",
                (TaskState.RUNNING.value, task['id'])
            )
            
            # Fork and exec (from Copilot/DeepSeek patterns)
            pid = os.fork()
            
            if pid == 0:  # Child process
                try:
                    # Set realtime priority if requested
                    if task['realtime']:
                        try:
                            os.sched_setscheduler(0, os.SCHED_FIFO, os.sched_param(90))
                        except PermissionError:
                            pass
                            
                    # Execute command
                    os.execvp("/bin/sh", ["/bin/sh", "-c", task['command']])
                except Exception:
                    os._exit(1)
            else:  # Parent process
                self.children[pid] = task['id']
                self.conn.execute(
                    "UPDATE tasks SET pid=? WHERE id=?",
                    (pid, task['id'])
                )
                self._log(f"Started task {task['id']} with PID {pid}")
                return True
                
        except Exception as e:
            self._log(f"Failed to execute task {task['id']}: {e}", "ERROR")
            self.conn.execute(
                "UPDATE tasks SET state=? WHERE id=?",
                (TaskState.FAILED.value, task['id'])
            )
            return False
            
    def process_queue(self):
        """Process approved tasks using atomic queue operations"""
        # Atomic fetch and update (from GeminiWeb pattern)
        cursor = self.conn.execute("""
            UPDATE tasks SET state='running' WHERE id IN (
                SELECT id FROM tasks WHERE state='approved'
                ORDER BY priority DESC, created_at ASC LIMIT 1
            ) RETURNING *
        """)
        
        task = cursor.fetchone()
        if not task:
            return
            
        # Try transient systemd unit first, fallback to direct execution
        if not self._has_systemd() or not self._execute_transient(task):
            self._execute_direct(task)
            
    def monitor_systemd_tasks(self):
        """Monitor systemd-managed tasks"""
        cursor = self.conn.execute(
            "SELECT id, unit_name FROM tasks WHERE state='running' AND unit_name IS NOT NULL"
        )
        
        for row in cursor.fetchall():
            result = subprocess.run(
                ["systemctl", "--user", "show", row['unit_name'], "--property=ActiveState"],
                capture_output=True, text=True
            )
            
            if "inactive" in result.stdout or "failed" in result.stdout:
                state = TaskState.FAILED if "failed" in result.stdout else TaskState.COMPLETED
                self.conn.execute(
                    """UPDATE tasks SET state=?, completed_at=CURRENT_TIMESTAMP 
                       WHERE id=?""",
                    (state.value, row['id'])
                )
                self._log(f"Task {row['id']} {state.value}")
                
    def _has_systemd(self) -> bool:
        """Check if systemd --user is available"""
        try:
            result = subprocess.run(
                ["systemctl", "--user", "status"],
                capture_output=True, timeout=1
            )
            return result.returncode != 127  # Command not found
        except:
            return False
            
    def get_status(self) -> Dict[str, Any]:
        """Get system status"""
        cursor = self.conn.execute(
            "SELECT state, COUNT(*) as count FROM tasks GROUP BY state"
        )
        
        status = {row['state']: row['count'] for row in cursor}
        status['children'] = len(self.children)
        status['systemd'] = self._has_systemd()
        return status
        
    def _shutdown(self, signum=None, frame=None):
        """Graceful shutdown"""
        self._log("Shutting down...")
        self.running = False
        
        # Stop all children
        for pid in list(self.children.keys()):
            try:
                os.kill(pid, signal.SIGTERM)
            except ProcessLookupError:
                pass
                
        # Give them time to exit
        time.sleep(1)
        
        # Force kill remaining
        for pid in list(self.children.keys()):
            try:
                os.kill(pid, signal.SIGKILL)
            except ProcessLookupError:
                pass
                
    def run(self):
        """Main event loop"""
        self._log("AIOS started")
        
        # Notify systemd if available
        if HAS_JOURNAL:
            try:
                subprocess.run(["systemd-notify", "--ready"], check=False)
            except:
                pass
                
        while self.running:
            # Always reap children first (Android pattern)
            self._reap_children()
            
            # Process task queue
            self.process_queue()
            
            # Monitor systemd tasks
            if self._has_systemd():
                self.monitor_systemd_tasks()
                
            # Brief sleep to prevent CPU spinning
            time.sleep(0.5)
            
        self._log("AIOS stopped")
        self.conn.close()

# CLI Interface
def main():
    if len(sys.argv) < 2:
        print("Usage: aios [run|submit|approve|status]")
        sys.exit(1)
        
    aios = AIOS()
    cmd = sys.argv[1]
    
    if cmd == "run":
        try:
            aios.run()
        except KeyboardInterrupt:
            pass
    elif cmd == "submit" and len(sys.argv) >= 4:
        task_id = aios.submit_task(sys.argv[2], sys.argv[3], auto_approve=True)
        print(f"Task ID: {task_id}")
    elif cmd == "approve" and len(sys.argv) >= 3:
        if aios.approve_task(int(sys.argv[2])):
            print("Task approved")
        else:
            print("Task not found or already processed")
    elif cmd == "status":
        print(json.dumps(aios.get_status(), indent=2))
    else:
        print("Invalid command")
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeCode1.py
================================================================================

#!/usr/bin/env python3
"""
claudeCode1: Ultra-Minimal Systemd Orchestrator (<100 lines)
Core pattern: systemd-run for transient units + subprocess
Best for: Quick prototypes, simple workflows
"""
import subprocess
import sys
import json
from pathlib import Path

UNIT_PREFIX = "aios-"

class MinimalOrchestrator:
    """Absolute minimum viable systemd orchestrator"""

    def __init__(self):
        self.units = {}

    def run(self, name: str, command: str, realtime: bool = False) -> bool:
        """Execute command as transient systemd unit"""
        unit = f"{UNIT_PREFIX}{name}"
        args = ["systemd-run", "--user", "--unit", unit, "--collect"]

        if realtime:
            args.extend(["--property=CPUSchedulingPolicy=rr",
                        "--property=CPUSchedulingPriority=80"])

        args.extend(["--", "sh", "-c", command])

        result = subprocess.run(args, capture_output=True, text=True)
        if result.returncode == 0:
            self.units[name] = unit
            return True
        return False

    def status(self, name: str) -> str:
        """Check unit status"""
        if name not in self.units:
            return "unknown"

        result = subprocess.run(
            ["systemctl", "--user", "is-active", self.units[name]],
            capture_output=True, text=True
        )
        return result.stdout.strip()

    def stop(self, name: str) -> bool:
        """Stop a unit"""
        if name not in self.units:
            return False

        result = subprocess.run(
            ["systemctl", "--user", "stop", self.units[name]],
            capture_output=True
        )
        return result.returncode == 0

    def list_active(self) -> dict:
        """List all active AIOS units"""
        result = subprocess.run(
            ["systemctl", "--user", "list-units", f"{UNIT_PREFIX}*",
             "--no-legend", "--plain"],
            capture_output=True, text=True
        )

        active = {}
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                if parts:
                    name = parts[0].replace('.service', '').replace(UNIT_PREFIX, '')
                    active[name] = parts[3]  # active/inactive
        return active

def main():
    """CLI interface"""
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <run|status|stop|list> [args...]")
        sys.exit(1)

    orch = MinimalOrchestrator()
    cmd = sys.argv[1]

    if cmd == "run" and len(sys.argv) >= 4:
        name, command = sys.argv[2], sys.argv[3]
        realtime = "--realtime" in sys.argv
        if orch.run(name, command, realtime):
            print(f"Started: {name}")
        else:
            print(f"Failed to start: {name}")

    elif cmd == "status" and len(sys.argv) >= 3:
        print(orch.status(sys.argv[2]))

    elif cmd == "stop" and len(sys.argv) >= 3:
        if orch.stop(sys.argv[2]):
            print(f"Stopped: {sys.argv[2]}")

    elif cmd == "list":
        for name, state in orch.list_active().items():
            print(f"{name}: {state}")

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeCode2.py
================================================================================

#!/usr/bin/env python3
"""
claudeCode2: Enhanced Features with SQLite State (<150 lines)
Combines: chatgpt1 + kimi1 + systemdOrchestrator patterns
Best for: Stateful workflows, scheduling, persistence
"""
import sqlite3
import subprocess
import json
import sys
import time
from pathlib import Path
from typing import Optional, Dict, List

DB_PATH = Path.home() / ".aios_state.db"
USER_DIR = Path.home() / ".config/systemd/user"
UNIT_PREFIX = "aios-"

class EnhancedOrchestrator:
    """Systemd orchestrator with SQLite state management"""

    def __init__(self):
        USER_DIR.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(DB_PATH)
        self._init_db()

    def _init_db(self):
        """Initialize database schema"""
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                name TEXT PRIMARY KEY,
                command TEXT NOT NULL,
                type TEXT DEFAULT 'service',
                schedule TEXT,
                realtime INTEGER DEFAULT 0,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'inactive',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.db.commit()

    def _systemctl(self, *args) -> subprocess.CompletedProcess:
        """Execute systemctl --user command"""
        return subprocess.run(["systemctl", "--user"] + list(args),
                            capture_output=True, text=True)

    def create_service(self, name: str, command: str,
                      realtime: bool = False, priority: int = 0) -> bool:
        """Create persistent systemd service"""
        unit_file = USER_DIR / f"{UNIT_PREFIX}{name}.service"

        # Generate service content
        content = f"""[Unit]
Description=AIOS Workflow: {name}
After=network.target

[Service]
Type=simple
ExecStart=/bin/sh -c '{command}'
Restart=on-failure
RestartSec=5
StandardOutput=journal
StandardError=journal
"""
        if realtime:
            content += f"""CPUSchedulingPolicy=rr
CPUSchedulingPriority={min(99, max(1, priority))}
"""

        content += "\n[Install]\nWantedBy=default.target\n"

        # Write unit file
        unit_file.write_text(content)

        # Store in database
        self.db.execute("""
            INSERT OR REPLACE INTO workflows
            (name, command, type, realtime, priority)
            VALUES (?, ?, 'service', ?, ?)
        """, (name, command, int(realtime), priority))
        self.db.commit()

        # Reload and enable
        self._systemctl("daemon-reload")
        result = self._systemctl("enable", "--now", f"{UNIT_PREFIX}{name}.service")

        if result.returncode == 0:
            self.db.execute("UPDATE workflows SET status='active' WHERE name=?", (name,))
            self.db.commit()
            return True
        return False

    def create_timer(self, name: str, command: str, schedule: str) -> bool:
        """Create scheduled task with systemd timer"""
        # First create the service
        self.create_service(name, command)

        # Create timer
        timer_file = USER_DIR / f"{UNIT_PREFIX}{name}.timer"
        timer_content = f"""[Unit]
Description=Timer for AIOS Workflow: {name}

[Timer]
OnCalendar={schedule}
Persistent=true

[Install]
WantedBy=timers.target
"""
        timer_file.write_text(timer_content)

        # Update database
        self.db.execute("""
            UPDATE workflows SET type='timer', schedule=? WHERE name=?
        """, (schedule, name))
        self.db.commit()

        # Enable timer
        self._systemctl("daemon-reload")
        result = self._systemctl("enable", "--now", f"{UNIT_PREFIX}{name}.timer")
        return result.returncode == 0

    def run_transient(self, name: str, command: str, delay_sec: int = 0) -> bool:
        """Run transient unit (no files created)"""
        unit = f"{UNIT_PREFIX}transient-{name}"
        args = ["systemd-run", "--user", "--unit", unit, "--collect"]

        if delay_sec > 0:
            args.append(f"--on-active={delay_sec}")

        args.extend(["--", "sh", "-c", command])

        result = subprocess.run(args, capture_output=True)
        return result.returncode == 0

    def list_workflows(self) -> List[Dict]:
        """List all workflows with current status"""
        cursor = self.db.execute("""
            SELECT name, command, type, schedule, status FROM workflows
            ORDER BY created_at DESC
        """)

        workflows = []
        for row in cursor:
            name = row[0]
            # Get live status
            unit = f"{UNIT_PREFIX}{name}.{'timer' if row[2] == 'timer' else 'service'}"
            result = self._systemctl("is-active", unit)
            live_status = result.stdout.strip()

            workflows.append({
                'name': name,
                'command': row[1],
                'type': row[2],
                'schedule': row[3],
                'status': live_status
            })
        return workflows

    def remove(self, name: str) -> bool:
        """Remove workflow completely"""
        # Stop and disable units
        for suffix in ['.service', '.timer']:
            unit = f"{UNIT_PREFIX}{name}{suffix}"
            self._systemctl("stop", unit)
            self._systemctl("disable", unit)
            unit_file = USER_DIR / unit
            unit_file.unlink(missing_ok=True)

        # Remove from database
        self.db.execute("DELETE FROM workflows WHERE name=?", (name,))
        self.db.commit()

        self._systemctl("daemon-reload")
        return True

def main():
    """CLI interface"""
    if len(sys.argv) < 2:
        print("Usage: claudeCode2.py <service|timer|transient|list|remove> [args...]")
        sys.exit(1)

    orch = EnhancedOrchestrator()
    cmd = sys.argv[1]

    if cmd == "service" and len(sys.argv) >= 4:
        if orch.create_service(sys.argv[2], sys.argv[3]):
            print(f"Service created: {sys.argv[2]}")

    elif cmd == "timer" and len(sys.argv) >= 5:
        if orch.create_timer(sys.argv[2], sys.argv[3], sys.argv[4]):
            print(f"Timer created: {sys.argv[2]}")

    elif cmd == "transient" and len(sys.argv) >= 4:
        if orch.run_transient(sys.argv[2], sys.argv[3]):
            print(f"Transient unit started: {sys.argv[2]}")

    elif cmd == "list":
        for wf in orch.list_workflows():
            print(json.dumps(wf))

    elif cmd == "remove" and len(sys.argv) >= 3:
        if orch.remove(sys.argv[2]):
            print(f"Removed: {sys.argv[2]}")

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeCode3.py
================================================================================

#!/usr/bin/env python3
"""
claudeCode3: Production-Ready Orchestrator (<200 lines)
Synthesizes: Android reaping + Chrome WAL + Firefox journal patterns
Best for: Production deployments, high reliability
"""
import sqlite3
import subprocess
import signal
import logging
import json
import sys
import os
import time
from pathlib import Path
from typing import Dict, Optional, List
from enum import Enum

# Production configuration
DB_PATH = Path("/var/lib/aios/orchestrator.db")
UNIT_PREFIX = "aios-"
WATCHDOG_INTERVAL = 30

class WorkflowState(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class ProductionOrchestrator:
    """Production-grade systemd orchestrator with reliability patterns"""

    def __init__(self):
        self.running = True
        self.children = {}

        # Setup logging to systemd journal
        try:
            from systemd import journal
            handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios')
            logging.root.addHandler(handler)
        except ImportError:
            pass

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        # Initialize database with production settings
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(str(DB_PATH), isolation_level=None)
        self._init_db()

        # Signal handlers for graceful shutdown
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGCHLD, self._reap_children)

    def _init_db(self):
        """Initialize SQLite with production optimizations"""
        # Chrome/Firefox WAL mode patterns
        self.db.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA cache_size=-8000;
            PRAGMA busy_timeout=5000;
            PRAGMA wal_autocheckpoint=1000;

            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                retry_count INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                started_at DATETIME,
                completed_at DATETIME,
                pid INTEGER,
                exit_code INTEGER,
                unit_name TEXT
            );

            CREATE INDEX IF NOT EXISTS idx_state_priority
            ON workflows(state, priority DESC, created_at);
        """)

    def _handle_signal(self, signum, frame):
        """Graceful shutdown handler"""
        self.logger.info(f"Received signal {signum}, shutting down")
        self.running = False
        try:
            from systemd import daemon
            daemon.notify("STOPPING=1")
        except ImportError:
            pass

    def _reap_children(self, signum, frame):
        """Android-style zombie reaping pattern"""
        while True:
            try:
                pid, status = os.waitpid(-1, os.WNOHANG)
                if pid == 0:
                    break

                exit_code = os.WEXITSTATUS(status) if os.WIFEXITED(status) else -1

                # Update database
                self.db.execute("""
                    UPDATE workflows
                    SET state = ?, exit_code = ?, completed_at = CURRENT_TIMESTAMP
                    WHERE pid = ?
                """, ('completed' if exit_code == 0 else 'failed', exit_code, pid))

                self.logger.info(f"Reaped child {pid} with exit code {exit_code}")

            except ChildProcessError:
                break

    def submit(self, name: str, command: str, priority: int = 0) -> int:
        """Submit new workflow"""
        cursor = self.db.execute("""
            INSERT INTO workflows (name, command, priority)
            VALUES (?, ?, ?)
        """, (name, command, priority))

        workflow_id = cursor.lastrowid
        self.logger.info(f"Submitted workflow {workflow_id}: {name}")
        return workflow_id

    def deploy_workflow(self, workflow_id: int) -> bool:
        """Deploy workflow as systemd service with production settings"""
        row = self.db.execute("""
            SELECT name, command, priority FROM workflows WHERE id = ?
        """, (workflow_id,)).fetchone()

        if not row:
            return False

        name, command, priority = row
        unit_name = f"{UNIT_PREFIX}{workflow_id}.service"

        # Create transient unit with production properties
        properties = [
            "--collect",  # Clean up automatically
            "--property=Type=exec",
            "--property=Restart=on-failure",
            "--property=RestartSec=10",
            f"--property=Nice={-priority}",  # Higher priority = lower nice
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",  # Kill all children
            "--property=TimeoutStopSec=30",
            # Resource limits
            "--property=MemoryMax=4G",
            "--property=CPUQuota=200%",  # 2 cores max
            "--property=TasksMax=1000",
            # Security
            "--property=PrivateTmp=yes",
            "--property=ProtectSystem=strict",
            "--property=NoNewPrivileges=yes",
        ]

        # Use systemd-run for transient units
        result = subprocess.run(
            ["systemd-run", "--user", "--unit", unit_name] + properties +
            ["--", "sh", "-c", command],
            capture_output=True, text=True
        )

        if result.returncode == 0:
            # Get PID from systemd
            pid_result = subprocess.run(
                ["systemctl", "--user", "show", unit_name, "--property=MainPID"],
                capture_output=True, text=True
            )

            pid = 0
            if pid_result.returncode == 0 and "=" in pid_result.stdout:
                pid = int(pid_result.stdout.split("=")[1].strip())

            # Update database
            self.db.execute("""
                UPDATE workflows
                SET state = 'running', started_at = CURRENT_TIMESTAMP,
                    pid = ?, unit_name = ?
                WHERE id = ?
            """, (pid, unit_name, workflow_id))

            self.logger.info(f"Deployed workflow {workflow_id} as {unit_name} (PID: {pid})")
            return True

        return False

    def monitor(self):
        """Monitor and process workflows"""
        # Deploy pending workflows
        pending = self.db.execute("""
            SELECT id FROM workflows
            WHERE state = 'pending'
            ORDER BY priority DESC, created_at
            LIMIT 5
        """).fetchall()

        for row in pending:
            self.deploy_workflow(row[0])

        # Check running workflows
        running = self.db.execute("""
            SELECT id, unit_name, retry_count, max_retries
            FROM workflows WHERE state = 'running'
        """).fetchall()

        for workflow_id, unit_name, retry_count, max_retries in running:
            result = subprocess.run(
                ["systemctl", "--user", "is-active", unit_name],
                capture_output=True, text=True
            )

            if result.stdout.strip() == "inactive":
                # Check if should retry
                if retry_count < max_retries:
                    self.db.execute("""
                        UPDATE workflows
                        SET state = 'pending', retry_count = retry_count + 1
                        WHERE id = ?
                    """, (workflow_id,))
                    self.logger.info(f"Retrying workflow {workflow_id} (attempt {retry_count + 1})")

        # Periodic WAL checkpoint (Firefox pattern)
        self.db.execute("PRAGMA wal_checkpoint(PASSIVE)")

    def run(self):
        """Main service loop"""
        try:
            from systemd import daemon
            daemon.notify("READY=1")
        except ImportError:
            pass

        self.logger.info("Production orchestrator started")

        while self.running:
            try:
                self.monitor()

                # Update systemd watchdog
                try:
                    from systemd import daemon
                    daemon.notify("WATCHDOG=1")
                except ImportError:
                    pass

                time.sleep(5)

            except Exception as e:
                self.logger.error(f"Monitor error: {e}")
                time.sleep(10)

        self.logger.info("Orchestrator shutdown complete")

def main():
    """CLI interface"""
    if len(sys.argv) < 2:
        print("Usage: claudeCode3.py <submit|status|run>")
        sys.exit(1)

    cmd = sys.argv[1]

    if cmd == "submit" and len(sys.argv) >= 4:
        orch = ProductionOrchestrator()
        workflow_id = orch.submit(sys.argv[2], sys.argv[3])
        print(f"Workflow ID: {workflow_id}")

    elif cmd == "status":
        db = sqlite3.connect(str(DB_PATH))
        for row in db.execute("SELECT id, name, state FROM workflows"):
            print(f"{row[0]}: {row[1]} [{row[2]}]")

    elif cmd == "run":
        orch = ProductionOrchestrator()
        orch.run()

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeCode4.py
================================================================================

#!/usr/bin/env python3
"""
claudeCode4: Enterprise Scale with Full Features
Synthesizes: Kubernetes patterns + Android WorkManager + Chrome scheduling
Best for: Large-scale deployments, complex workflows, full monitoring
"""
import sqlite3
import subprocess
import asyncio
import signal
import logging
import json
import time
from pathlib import Path
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime, timedelta

# Enterprise configuration
DB_PATH = Path("/var/lib/aios/enterprise.db")
UNIT_PREFIX = "aios-enterprise-"

class WorkflowState(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    SCHEDULED = "scheduled"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class Priority(Enum):
    USER_BLOCKING = 100
    USER_VISIBLE = 50
    BACKGROUND = 0

@dataclass
class Workflow:
    """Enterprise workflow model with full metadata"""
    id: Optional[int] = None
    name: str = ""
    command: str = ""
    state: WorkflowState = WorkflowState.PENDING
    priority: int = Priority.BACKGROUND.value
    scheduled_at: Optional[float] = None
    dependencies: List[int] = None
    max_retries: int = 3
    retry_count: int = 0
    backoff_policy: str = "exponential"
    cpu_limit: float = 2.0
    memory_limit_gb: float = 4.0
    requires_approval: bool = True
    created_at: Optional[float] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.metadata is None:
            self.metadata = {}
        if self.created_at is None:
            self.created_at = time.time()

class EnterpriseOrchestrator:
    """Enterprise-grade orchestrator with advanced features"""

    def __init__(self):
        self.running = True
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)

        # Production SQLite with all optimizations
        self.db = sqlite3.connect(str(DB_PATH), isolation_level=None, check_same_thread=False)
        self.db.row_factory = sqlite3.Row
        self._init_db()

        # Setup comprehensive logging
        self._setup_logging()

        # Signal handlers
        signal.signal(signal.SIGTERM, self._handle_shutdown)
        signal.signal(signal.SIGINT, self._handle_shutdown)

        # Try to import systemd for integration
        try:
            from systemd import daemon
            self.systemd_available = True
        except ImportError:
            self.systemd_available = False

    def _setup_logging(self):
        """Setup enterprise logging with multiple handlers"""
        self.logger = logging.getLogger("aios-enterprise")
        self.logger.setLevel(logging.INFO)

        # Try systemd journal
        try:
            from systemd import journal
            handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios-enterprise')
            self.logger.addHandler(handler)
        except ImportError:
            pass

        # File handler for audit trail
        log_path = Path("/var/log/aios-enterprise.log")
        if log_path.parent.exists():
            file_handler = logging.FileHandler(log_path)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

    def _init_db(self):
        """Initialize enterprise database schema"""
        self.db.executescript("""
            -- Chrome/Firefox optimizations
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA cache_size=-16000;
            PRAGMA mmap_size=536870912;
            PRAGMA busy_timeout=10000;
            PRAGMA wal_autocheckpoint=1000;

            -- Main workflows table with all enterprise fields
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                scheduled_at REAL,
                max_retries INTEGER DEFAULT 3,
                retry_count INTEGER DEFAULT 0,
                backoff_policy TEXT DEFAULT 'exponential',
                cpu_limit REAL DEFAULT 2.0,
                memory_limit_gb REAL DEFAULT 4.0,
                requires_approval BOOLEAN DEFAULT 1,
                created_at REAL DEFAULT (julianday('now')),
                started_at REAL,
                completed_at REAL,
                pid INTEGER,
                exit_code INTEGER,
                unit_name TEXT,
                metadata TEXT
            );

            -- Dependencies (Android WorkManager pattern)
            CREATE TABLE IF NOT EXISTS workflow_dependencies (
                workflow_id INTEGER,
                depends_on_id INTEGER,
                PRIMARY KEY (workflow_id, depends_on_id),
                FOREIGN KEY (workflow_id) REFERENCES workflows(id) ON DELETE CASCADE,
                FOREIGN KEY (depends_on_id) REFERENCES workflows(id) ON DELETE CASCADE
            );

            -- Metrics table (Chrome pattern)
            CREATE TABLE IF NOT EXISTS workflow_metrics (
                workflow_id INTEGER PRIMARY KEY,
                queue_time REAL,
                execution_time REAL,
                cpu_usage_percent REAL,
                memory_usage_mb REAL,
                FOREIGN KEY (workflow_id) REFERENCES workflows(id) ON DELETE CASCADE
            );

            -- Audit log
            CREATE TABLE IF NOT EXISTS audit_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                workflow_id INTEGER,
                action TEXT,
                user TEXT,
                timestamp REAL DEFAULT (julianday('now')),
                details TEXT
            );

            -- Optimized indexes
            CREATE INDEX IF NOT EXISTS idx_state_priority_scheduled
                ON workflows(state, priority DESC, scheduled_at);
            CREATE INDEX IF NOT EXISTS idx_dependencies
                ON workflow_dependencies(workflow_id, depends_on_id);
            CREATE INDEX IF NOT EXISTS idx_audit_workflow
                ON audit_log(workflow_id, timestamp DESC);
        """)

    def submit(self, workflow: Workflow) -> int:
        """Submit workflow with full validation"""
        # Serialize metadata
        metadata_json = json.dumps(workflow.metadata) if workflow.metadata else None

        # Insert workflow
        cursor = self.db.execute("""
            INSERT INTO workflows (
                name, command, state, priority, scheduled_at,
                max_retries, backoff_policy, cpu_limit, memory_limit_gb,
                requires_approval, metadata
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (workflow.name, workflow.command, workflow.state.value,
              workflow.priority, workflow.scheduled_at, workflow.max_retries,
              workflow.backoff_policy, workflow.cpu_limit, workflow.memory_limit_gb,
              int(workflow.requires_approval), metadata_json))

        workflow_id = cursor.lastrowid

        # Add dependencies
        for dep_id in workflow.dependencies:
            self.db.execute("""
                INSERT INTO workflow_dependencies (workflow_id, depends_on_id)
                VALUES (?, ?)
            """, (workflow_id, dep_id))

        # Audit log
        self._audit(workflow_id, "SUBMITTED", {"name": workflow.name})

        self.logger.info(f"Submitted workflow {workflow_id}: {workflow.name}")
        return workflow_id

    def approve(self, workflow_id: int, approver: str = "system") -> bool:
        """Approve workflow for execution"""
        self.db.execute("""
            UPDATE workflows SET state = ? WHERE id = ? AND state = 'pending'
        """, (WorkflowState.APPROVED.value, workflow_id))

        if self.db.total_changes > 0:
            self._audit(workflow_id, "APPROVED", {"approver": approver})
            self.logger.info(f"Workflow {workflow_id} approved by {approver}")
            return True
        return False

    async def deploy(self, workflow_id: int) -> bool:
        """Deploy workflow with enterprise features"""
        row = self.db.execute("""
            SELECT * FROM workflows WHERE id = ?
        """, (workflow_id,)).fetchone()

        if not row:
            return False

        # Check dependencies
        deps = self.db.execute("""
            SELECT d.depends_on_id, w.state
            FROM workflow_dependencies d
            JOIN workflows w ON d.depends_on_id = w.id
            WHERE d.workflow_id = ?
        """, (workflow_id,)).fetchall()

        if any(dep['state'] != WorkflowState.COMPLETED.value for dep in deps):
            return False  # Dependencies not satisfied

        unit_name = f"{UNIT_PREFIX}{workflow_id}.service"

        # Build systemd-run command with enterprise properties
        cmd = [
            "systemd-run", "--user",
            "--unit", unit_name,
            "--collect",
            "--property=Type=exec",
            "--property=Restart=on-failure",
            f"--property=RestartSec={10 * (2 ** row['retry_count'])}",  # Exponential backoff
            f"--property=CPUQuota={int(row['cpu_limit'] * 100)}%",
            f"--property=MemoryMax={row['memory_limit_gb']}G",
            "--property=TasksMax=1000",
            # Security hardening
            "--property=PrivateTmp=yes",
            "--property=ProtectSystem=strict",
            "--property=ProtectHome=yes",
            "--property=NoNewPrivileges=yes",
            "--property=RestrictSUIDSGID=yes",
            # Logging
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            f"--property=SyslogIdentifier={unit_name}",
        ]

        # Add real-time scheduling for high priority
        if row['priority'] >= Priority.USER_VISIBLE.value:
            cmd.extend([
                "--property=CPUSchedulingPolicy=rr",
                f"--property=CPUSchedulingPriority={min(99, row['priority'])}"
            ])

        cmd.extend(["--", "sh", "-c", row['command']])

        # Execute deployment
        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode == 0:
            # Update state
            self.db.execute("""
                UPDATE workflows
                SET state = 'running', started_at = julianday('now'), unit_name = ?
                WHERE id = ?
            """, (unit_name, workflow_id))

            self._audit(workflow_id, "DEPLOYED", {"unit": unit_name})
            self.logger.info(f"Deployed workflow {workflow_id} as {unit_name}")
            return True

        self.logger.error(f"Failed to deploy workflow {workflow_id}: {stderr.decode()}")
        return False

    async def monitor(self):
        """Advanced monitoring with metrics collection"""
        # Process scheduled workflows
        current_time = datetime.now().toordinal() + 1721425.5
        scheduled = self.db.execute("""
            SELECT id FROM workflows
            WHERE state = 'approved' AND
            (scheduled_at IS NULL OR scheduled_at <= ?)
            ORDER BY priority DESC, created_at
            LIMIT 10
        """, (current_time,)).fetchall()

        for row in scheduled:
            await self.deploy(row['id'])

        # Monitor running workflows
        running = self.db.execute("""
            SELECT id, unit_name, started_at FROM workflows
            WHERE state = 'running'
        """).fetchall()

        for row in running:
            # Check systemd unit status
            result = subprocess.run(
                ["systemctl", "--user", "show", row['unit_name'],
                 "--property=ActiveState,MainPID,CPUUsageNSec,MemoryCurrent"],
                capture_output=True, text=True
            )

            props = {}
            for line in result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    props[k] = v

            if props.get('ActiveState') in ['inactive', 'failed']:
                # Calculate metrics
                exec_time = (current_time - row['started_at']) * 86400 if row['started_at'] else 0

                # Record metrics
                self.db.execute("""
                    INSERT OR REPLACE INTO workflow_metrics
                    (workflow_id, execution_time, cpu_usage_percent, memory_usage_mb)
                    VALUES (?, ?, ?, ?)
                """, (row['id'], exec_time,
                      float(props.get('CPUUsageNSec', 0)) / 1e9,
                      float(props.get('MemoryCurrent', 0)) / 1e6))

                # Update workflow state
                state = WorkflowState.COMPLETED if props.get('ActiveState') == 'inactive' else WorkflowState.FAILED
                self.db.execute("""
                    UPDATE workflows
                    SET state = ?, completed_at = julianday('now')
                    WHERE id = ?
                """, (state.value, row['id']))

                self._audit(row['id'], state.value, {"exec_time": exec_time})

    def _audit(self, workflow_id: int, action: str, details: Dict = None):
        """Record audit log entry"""
        self.db.execute("""
            INSERT INTO audit_log (workflow_id, action, user, details)
            VALUES (?, ?, ?, ?)
        """, (workflow_id, action, os.getenv('USER', 'system'),
              json.dumps(details) if details else None))

    def _handle_shutdown(self, signum, frame):
        """Graceful shutdown"""
        self.logger.info("Initiating graceful shutdown")
        self.running = False
        if self.systemd_available:
            from systemd import daemon
            daemon.notify("STOPPING=1")

    async def run(self):
        """Main enterprise loop"""
        if self.systemd_available:
            from systemd import daemon
            daemon.notify("READY=1")

        self.logger.info("Enterprise orchestrator started")

        while self.running:
            try:
                await self.monitor()

                # Periodic maintenance
                if int(time.time()) % 300 == 0:  # Every 5 minutes
                    self.db.execute("PRAGMA wal_checkpoint(PASSIVE)")
                    self.db.execute("PRAGMA optimize")

                if self.systemd_available:
                    from systemd import daemon
                    daemon.notify("WATCHDOG=1")

                await asyncio.sleep(5)

            except Exception as e:
                self.logger.error(f"Monitor error: {e}", exc_info=True)
                await asyncio.sleep(10)

def main():
    """Enterprise CLI"""
    import argparse
    parser = argparse.ArgumentParser(description="Enterprise AIOS Orchestrator")
    parser.add_argument("command", choices=["submit", "approve", "status", "run"])
    parser.add_argument("--name", help="Workflow name")
    parser.add_argument("--command", help="Command to execute")
    parser.add_argument("--priority", type=int, default=0)
    parser.add_argument("--id", type=int, help="Workflow ID")

    args = parser.parse_args()

    orch = EnterpriseOrchestrator()

    if args.command == "submit":
        workflow = Workflow(
            name=args.name,
            command=args.command,
            priority=args.priority
        )
        wf_id = orch.submit(workflow)
        print(f"Workflow submitted: {wf_id}")

    elif args.command == "approve" and args.id:
        if orch.approve(args.id):
            print(f"Workflow {args.id} approved")

    elif args.command == "status":
        for row in orch.db.execute("SELECT id, name, state FROM workflows ORDER BY id DESC LIMIT 20"):
            print(f"{row['id']}: {row['name']} [{row['state']}]")

    elif args.command == "run":
        asyncio.run(orch.run())

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeResearch1.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Process Manager - SystemD-based workload orchestration
Synthesized from Android, Kubernetes, Cloud, and IoT best practices
Under 200 lines, production-ready for 500M+ scale
"""
import json
import sqlite3
import logging
import signal
import subprocess
import time
from pathlib import Path
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, asdict
from enum import Enum

try:
    from pystemd.systemd1 import Unit, Manager
    from systemd import journal, daemon
except ImportError:
    print("Install: pip install pystemd systemd-python")
    raise

class WorkflowState(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class AIWorkflow:
    id: str
    name: str
    command: str
    state: WorkflowState
    memory_limit_gb: float = 4.0
    cpu_quota_percent: int = 100
    restart_policy: str = "on-failure"
    requires_approval: bool = True
    created_at: float = 0
    pid: Optional[int] = None

class AIOSProcessManager:
    def __init__(self, db_path: str = "/var/lib/aios/state.db"):
        self.db_path = db_path
        self.manager = Manager()
        self.manager.load()
        self.running = True
        
        # Setup logging to systemd journal
        self.logger = logging.getLogger('aios')
        self.logger.addHandler(journal.JournalHandler(SYSLOG_IDENTIFIER='aios'))
        self.logger.setLevel(logging.INFO)
        
        # Initialize database
        self._init_db()
        
        # Signal handlers for clean shutdown
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
    
    def _init_db(self):
        """Initialize SQLite database for state management"""
        conn = sqlite3.connect(self.db_path)
        conn.execute('''CREATE TABLE IF NOT EXISTS workflows (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            command TEXT NOT NULL,
            state TEXT NOT NULL,
            memory_limit_gb REAL,
            cpu_quota_percent INTEGER,
            restart_policy TEXT,
            requires_approval BOOLEAN,
            created_at REAL,
            pid INTEGER,
            result TEXT
        )''')
        conn.commit()
        conn.close()
    
    def create_workflow(self, workflow: AIWorkflow) -> str:
        """Create new AI workflow pending approval"""
        workflow.created_at = time.time()
        workflow.state = WorkflowState.PENDING if workflow.requires_approval else WorkflowState.APPROVED
        
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT INTO workflows VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (workflow.id, workflow.name, workflow.command, workflow.state.value,
             workflow.memory_limit_gb, workflow.cpu_quota_percent, 
             workflow.restart_policy, workflow.requires_approval,
             workflow.created_at, workflow.pid, None)
        )
        conn.commit()
        conn.close()
        
        self.logger.info(f"Created workflow {workflow.id}: {workflow.name}")
        
        if not workflow.requires_approval:
            self._deploy_workflow(workflow)
        
        return workflow.id
    
    def approve_workflow(self, workflow_id: str) -> bool:
        """Approve pending workflow for execution"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute(
            "SELECT * FROM workflows WHERE id = ? AND state = ?",
            (workflow_id, WorkflowState.PENDING.value)
        )
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return False
        
        workflow = self._row_to_workflow(row)
        workflow.state = WorkflowState.APPROVED
        
        conn.execute(
            "UPDATE workflows SET state = ? WHERE id = ?",
            (WorkflowState.APPROVED.value, workflow_id)
        )
        conn.commit()
        conn.close()
        
        self._deploy_workflow(workflow)
        return True
    
    def _deploy_workflow(self, workflow: AIWorkflow):
        """Deploy workflow as systemd service with resource limits"""
        service_name = f"aios-{workflow.id}.service"
        service_path = f"/etc/systemd/system/{service_name}"
        
        # Generate systemd service file - pattern from cloud-init/snapd
        service_content = f"""[Unit]
Description=AIOS Workflow: {workflow.name}
After=network.target aios.service
PartOf=aios.target

[Service]
Type=simple
ExecStart={workflow.command}
Restart={workflow.restart_policy}
RestartSec=10

# Resource limits (Kubernetes/Android pattern)
MemoryMax={workflow.memory_limit_gb}G
CPUQuota={workflow.cpu_quota_percent}%
TasksMax=1000

# Process management (Android init pattern)
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=30

# Security (snapd/ChromeOS pattern)
PrivateTmp=yes
ProtectSystem=strict
ProtectHome=yes
NoNewPrivileges=yes
ReadWritePaths=/var/lib/aios/workflows/{workflow.id}

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=aios-{workflow.id}

[Install]
WantedBy=aios.target"""
        
        Path(service_path).write_text(service_content)
        
        # Reload and start service
        subprocess.run(['systemctl', 'daemon-reload'], check=True)
        
        unit = Unit(service_name.encode())
        unit.load()
        unit.Unit.Start(b'replace')
        
        # Update database
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "UPDATE workflows SET state = ?, pid = ? WHERE id = ?",
            (WorkflowState.RUNNING.value, unit.Service.MainPID, workflow.id)
        )
        conn.commit()
        conn.close()
        
        self.logger.info(f"Deployed workflow {workflow.id} as {service_name}")
    
    def monitor_workflows(self):
        """Monitor running workflows - implements Android's reaping pattern"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute(
            "SELECT * FROM workflows WHERE state = ?",
            (WorkflowState.RUNNING.value,)
        )
        
        for row in cursor.fetchall():
            workflow = self._row_to_workflow(row)
            service_name = f"aios-{workflow.id}.service"
            
            try:
                unit = Unit(service_name.encode())
                unit.load()
                
                state = unit.Unit.ActiveState.decode()
                
                if state in ['inactive', 'failed']:
                    # Workflow completed or failed
                    new_state = WorkflowState.FAILED if state == 'failed' else WorkflowState.COMPLETED
                    
                    conn.execute(
                        "UPDATE workflows SET state = ? WHERE id = ?",
                        (new_state.value, workflow.id)
                    )
                    
                    # Cleanup service file
                    Path(f"/etc/systemd/system/{service_name}").unlink(missing_ok=True)
                    subprocess.run(['systemctl', 'daemon-reload'])
                    
                    self.logger.info(f"Workflow {workflow.id} {new_state.value}")
                
                elif state == 'active':
                    # Log resource usage (cloud provider pattern)
                    memory = unit.Service.MemoryCurrent / (1024**3)
                    cpu_ns = unit.Service.CPUUsageNSec
                    
                    if memory > workflow.memory_limit_gb * 0.9:
                        self.logger.warning(
                            f"Workflow {workflow.id} approaching memory limit: {memory:.2f}GB"
                        )
                        
            except Exception as e:
                self.logger.error(f"Error monitoring workflow {workflow.id}: {e}")
        
        conn.commit()
        conn.close()
    
    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """Get comprehensive workflow status"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("SELECT * FROM workflows WHERE id = ?", (workflow_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return None
        
        workflow = self._row_to_workflow(row)
        status = asdict(workflow)
        
        if workflow.state == WorkflowState.RUNNING:
            try:
                unit = Unit(f"aios-{workflow_id}.service".encode())
                unit.load()
                status['memory_usage_gb'] = unit.Service.MemoryCurrent / (1024**3)
                status['cpu_time_sec'] = unit.Service.CPUUsageNSec / 1e9
                status['restart_count'] = unit.Service.NRestarts
            except:
                pass
        
        return status
    
    def _row_to_workflow(self, row) -> AIWorkflow:
        """Convert database row to workflow object"""
        return AIWorkflow(
            id=row[0], name=row[1], command=row[2],
            state=WorkflowState(row[3]), memory_limit_gb=row[4],
            cpu_quota_percent=row[5], restart_policy=row[6],
            requires_approval=row[7], created_at=row[8], pid=row[9]
        )
    
    def _handle_signal(self, signum, frame):
        """Clean shutdown handler"""
        self.logger.info(f"Received signal {signum}, shutting down")
        self.running = False
        daemon.notify('STOPPING=1')
    
    def run(self):
        """Main service loop with systemd integration"""
        daemon.notify('READY=1')
        self.logger.info("AIOS Process Manager started")
        
        while self.running:
            try:
                self.monitor_workflows()
                
                # Update systemd status
                conn = sqlite3.connect(self.db_path)
                active = conn.execute(
                    "SELECT COUNT(*) FROM workflows WHERE state = ?",
                    (WorkflowState.RUNNING.value,)
                ).fetchone()[0]
                conn.close()
                
                daemon.notify(f'STATUS=Managing {active} workflows')
                time.sleep(10)
                
            except Exception as e:
                self.logger.error(f"Error in main loop: {e}")
                time.sleep(30)
        
        self.logger.info("AIOS Process Manager stopped")

# Example usage
if __name__ == "__main__":
    manager = AIOSProcessManager()
    
    # Create AI workflow
    workflow = AIWorkflow(
        id="ml-train-001",
        name="Model Training",
        command="/opt/ai/train.py --model gpt --dataset custom",
        memory_limit_gb=8.0,
        cpu_quota_percent=400,  # 4 cores
        requires_approval=True
    )
    
    manager.create_workflow(workflow)
    manager.run()


================================================================================
FILE: claudeResearch2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Ultimate - Synthesized from all best practices
Combines: Android's reaping, transient units, async patterns, minimal overhead
"""
import os
import sys
import json
import sqlite3
import signal
import asyncio
import logging
from pathlib import Path
from enum import Enum
from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict

# Setup logging (deepseek1 pattern)
try:
    from systemd import journal, daemon
    handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios')
    logging.root.addHandler(handler)
except ImportError:
    pass  # Fallback to standard logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('aios')

class State(Enum):
    PENDING = "pending"
    APPROVED = "approved" 
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class Workflow:
    id: str
    name: str
    command: str
    state: State = State.PENDING
    priority: int = 0
    memory_mb: int = 512
    cpu_percent: int = 100
    realtime: bool = False
    auto_approve: bool = False

class AIOS:
    """Ultimate AIOS combining best patterns from all implementations"""
    
    def __init__(self, db_path: str = "~/.aios/state.db"):
        self.db_path = Path(db_path).expanduser()
        self.db_path.parent.mkdir(exist_ok=True, parents=True)
        self.running = True
        self.tasks = {}  # Active async tasks
        
        # Initialize database (gemini WAL pattern)
        self.db = sqlite3.connect(str(self.db_path), isolation_level=None)
        self.db.row_factory = sqlite3.Row
        self.db.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            CREATE TABLE IF NOT EXISTS workflows (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                memory_mb INTEGER DEFAULT 512,
                cpu_percent INTEGER DEFAULT 100,
                realtime BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                unit_name TEXT,
                exit_code INTEGER
            );
            CREATE INDEX IF NOT EXISTS idx_state ON workflows(state, priority DESC);
        """)
        
        # Signal handling (copilot1 + deepseek1 pattern)
        signal.signal(signal.SIGTERM, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGINT, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGCHLD, signal.SIG_IGN)  # Auto-reap zombies
    
    def submit(self, workflow: Workflow) -> str:
        """Submit workflow for review/execution"""
        if workflow.auto_approve:
            workflow.state = State.APPROVED
        
        self.db.execute(
            """INSERT INTO workflows (id, name, command, state, priority, 
               memory_mb, cpu_percent, realtime) VALUES (?,?,?,?,?,?,?,?)""",
            (workflow.id, workflow.name, workflow.command, workflow.state.value,
             workflow.priority, workflow.memory_mb, workflow.cpu_percent, workflow.realtime)
        )
        logger.info(f"Submitted workflow {workflow.id}")
        
        if workflow.auto_approve:
            asyncio.create_task(self._execute(workflow))
        
        return workflow.id
    
    def approve(self, workflow_id: str) -> bool:
        """Approve pending workflow"""
        cursor = self.db.execute(
            "UPDATE workflows SET state=? WHERE id=? AND state=? RETURNING *",
            (State.APPROVED.value, workflow_id, State.PENDING.value)
        )
        row = cursor.fetchone()
        if row:
            workflow = self._row_to_workflow(row)
            asyncio.create_task(self._execute(workflow))
            return True
        return False
    
    async def _execute(self, workflow: Workflow):
        """Execute using systemd transient units (kimi1 + chatgptResearch1 pattern)"""
        import subprocess
        
        # Update state
        self.db.execute(
            "UPDATE workflows SET state=? WHERE id=?",
            (State.RUNNING.value, workflow.id)
        )
        
        # Build systemd-run command (transient units are fastest)
        unit_name = f"aios-{workflow.id}"
        cmd = [
            "systemd-run", "--user", "--collect", "--unit", unit_name,
            f"--property=MemoryMax={workflow.memory_mb}M",
            f"--property=CPUQuota={workflow.cpu_percent}%",
            "--property=StandardOutput=journal",
            "--property=StandardError=journal"
        ]
        
        # Real-time scheduling (deepseek1 pattern)
        if workflow.realtime:
            cmd.extend([
                "--property=CPUSchedulingPolicy=fifo",
                "--property=CPUSchedulingPriority=90"
            ])
        
        # Execute command
        cmd.extend(["--", "/bin/sh", "-c", workflow.command])
        
        try:
            # Start the unit
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # Store unit name for monitoring
                self.db.execute(
                    "UPDATE workflows SET unit_name=? WHERE id=?",
                    (unit_name, workflow.id)
                )
                
                # Monitor completion (async polling)
                await self._monitor_unit(workflow.id, unit_name)
            else:
                raise Exception(f"Failed to start: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Execution failed for {workflow.id}: {e}")
            self.db.execute(
                "UPDATE workflows SET state=?, exit_code=-1 WHERE id=?",
                (State.FAILED.value, workflow.id)
            )
    
    async def _monitor_unit(self, workflow_id: str, unit_name: str):
        """Monitor systemd unit status"""
        import subprocess
        
        while self.running:
            # Check unit status
            result = subprocess.run(
                ["systemctl", "--user", "show", unit_name, 
                 "--property=ActiveState,ExecMainStatus"],
                capture_output=True, text=True
            )
            
            props = {}
            for line in result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    props[k] = v
            
            state = props.get('ActiveState', '')
            
            if state in ('inactive', 'failed'):
                # Unit finished
                exit_code = int(props.get('ExecMainStatus', '-1'))
                final_state = State.COMPLETED if exit_code == 0 else State.FAILED
                
                self.db.execute(
                    "UPDATE workflows SET state=?, exit_code=? WHERE id=?",
                    (final_state.value, exit_code, workflow_id)
                )
                
                logger.info(f"Workflow {workflow_id} {final_state.value} (exit={exit_code})")
                break
            
            await asyncio.sleep(1)
    
    def _row_to_workflow(self, row) -> Workflow:
        """Convert DB row to Workflow"""
        return Workflow(
            id=row['id'], name=row['name'], command=row['command'],
            state=State(row['state']), priority=row['priority'],
            memory_mb=row['memory_mb'], cpu_percent=row['cpu_percent'],
            realtime=bool(row['realtime'])
        )
    
    async def scheduler(self):
        """Main scheduler loop (claude1 pattern)"""
        try:
            if hasattr(daemon, 'notify'):
                daemon.notify('READY=1')
        except:
            pass
        
        while self.running:
            # Get approved workflows
            rows = self.db.execute(
                """SELECT * FROM workflows WHERE state=? 
                   ORDER BY priority DESC, created_at LIMIT 5""",
                (State.APPROVED.value,)
            ).fetchall()
            
            # Execute in parallel
            if rows:
                tasks = [self._execute(self._row_to_workflow(row)) for row in rows]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # Status update
            stats = self.db.execute(
                "SELECT state, COUNT(*) as cnt FROM workflows GROUP BY state"
            ).fetchall()
            
            status = {row['state']: row['cnt'] for row in stats}
            logger.info(f"Status: {status}")
            
            if hasattr(daemon, 'notify'):
                active = status.get('running', 0)
                daemon.notify(f'STATUS=Running {active} workflows')
            
            await asyncio.sleep(2)
    
    def status(self, workflow_id: Optional[str] = None) -> Dict[str, Any]:
        """Get workflow status"""
        if workflow_id:
            row = self.db.execute(
                "SELECT * FROM workflows WHERE id=?", (workflow_id,)
            ).fetchone()
            return dict(row) if row else None
        else:
            return {
                row['id']: row['state'] for row in 
                self.db.execute("SELECT id, state FROM workflows")
            }

# CLI Interface (qwen1 + geminiWeb1 pattern)
async def main():
    """Main entry point with CLI"""
    aios = AIOS()
    
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        
        if cmd == "submit":
            # Example: ./aios.py submit "test" "echo hello" --auto
            wf = Workflow(
                id=f"wf-{os.urandom(4).hex()}",
                name=sys.argv[2] if len(sys.argv) > 2 else "test",
                command=sys.argv[3] if len(sys.argv) > 3 else "echo test",
                auto_approve="--auto" in sys.argv,
                realtime="--rt" in sys.argv
            )
            print(f"Submitted: {aios.submit(wf)}")
            
        elif cmd == "approve" and len(sys.argv) > 2:
            if aios.approve(sys.argv[2]):
                print(f"Approved: {sys.argv[2]}")
            else:
                print("Not found or already approved")
                
        elif cmd == "status":
            wf_id = sys.argv[2] if len(sys.argv) > 2 else None
            print(json.dumps(aios.status(wf_id), indent=2))
            
        else:
            print(f"Usage: {sys.argv[0]} [submit|approve|status|run]")
            sys.exit(1)
    else:
        # Run scheduler
        await aios.scheduler()

if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: copilot1.py
================================================================================

#!/usr/bin/env python3

import os
import sqlite3
import threading
import queue
import sched
import signal
import time
import logging

try:
    from systemd import daemon
except ImportError:
    daemon = None

class ZombieReaper(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self._q = queue.Queue()
        self.start()

    def run(self):
        while True:
            pid = self._q.get()
            if pid is None:
                break
            os.waitpid(pid, 0)

    def reap(self, pid):
        self._q.put(pid)

    def close(self):
        self._q.put(None)
        self.join()

class WorkflowManager:
    def __init__(self, db_path='/var/lib/aios/aios.db'):
        self.db = sqlite3.connect(db_path, check_same_thread=False)
        self._init_db()
        self.reaper = ZombieReaper()
        self.scheduler = sched.scheduler(time.time, time.sleep)
        signal.signal(signal.SIGCHLD, self._on_sigchld)

    def _init_db(self):
        cur = self.db.cursor()
        cur.execute('''
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY,
                command TEXT NOT NULL,
                delay INTEGER NOT NULL,
                realtime INTEGER NOT NULL DEFAULT 0
            )
        ''')
        self.db.commit()

    def _on_sigchld(self, *_):
        try:
            while True:
                pid, _ = os.waitpid(-1, os.WNOHANG)
                if pid == 0:
                    break
                self.reaper.reap(pid)
        except ChildProcessError:
            pass

    def add_workflow(self, cmd, delay, realtime=False):
        cur = self.db.cursor()
        cur.execute(
            'INSERT INTO workflows (command, delay, realtime) VALUES (?, ?, ?)',
            (cmd, delay, int(bool(realtime)))
        )
        self.db.commit()

    def load_and_schedule(self):
        now = time.time()
        cur = self.db.cursor()
        cur.execute('SELECT id, command, delay, realtime FROM workflows')
        for wf_id, cmd, delay, rt in cur:
            self.scheduler.enterabs(
                now + delay,
                1,
                self._execute,
                (wf_id, cmd, rt)
            )

    def _execute(self, wf_id, cmd, realtime):
        pid = os.fork()
        if pid == 0:
            if realtime:
                try:
                    os.sched_setscheduler(0, os.SCHED_FIFO, os.sched_param(1))
                except PermissionError:
                    pass
            os.execvp(cmd.split()[0], cmd.split())
        else:
            self.reaper.reap(pid)
            cur = self.db.cursor()
            cur.execute('DELETE FROM workflows WHERE id = ?', (wf_id,))
            self.db.commit()

    def serve(self):
        if daemon:
            daemon.notify('READY=1')
        self.load_and_schedule()
        try:
            self.scheduler.run()
        except KeyboardInterrupt:
            pass
        finally:
            self.reaper.close()

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    mgr = WorkflowManager()
    # Example: mgr.add_workflow('python3 /usr/bin/cleanup.py', delay=60, realtime=True)
    mgr.serve()


================================================================================
FILE: deepseek1.py
================================================================================

#!/usr/bin/env python3
import os
import time
import signal
import subprocess
import sqlite3
from systemd import daemon
from systemd import journal
from datetime import datetime
import select
import ctypes
import ctypes.util

# Real-time priority setup
libc = ctypes.CDLL(ctypes.util.find_library('c'), use_errno=True)

class sched_param(ctypes.Structure):
    _fields_ = [('sched_priority', ctypes.c_int)]

def set_realtime_priority(priority=80):
    param = sched_param()
    param.sched_priority = priority
    if libc.sched_setscheduler(0, 1, param) < 0:  # 1 = SCHED_FIFO
        errno = ctypes.get_errno()
        journal.send(f"Failed to set realtime priority: {os.strerror(errno)}")

class AIOSManager:
    def __init__(self, db_path='/etc/aios/aios.db'):
        self.db_path = db_path
        self.running = True
        self.watchdog_interval = 5
        self.processes = {}
        
        # Systemd integration
        signal.signal(signal.SIGTERM, self.handle_signal)
        signal.signal(signal.SIGINT, self.handle_signal)
        
    def handle_signal(self, signum, frame):
        journal.send("Shutdown signal received")
        self.running = False
        
    def init_db(self):
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript('''
                CREATE TABLE IF NOT EXISTS workflows (
                    id INTEGER PRIMARY KEY,
                    name TEXT NOT NULL,
                    command TEXT NOT NULL,
                    schedule TEXT,
                    priority INTEGER DEFAULT 0,
                    enabled INTEGER DEFAULT 1,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS executions (
                    id INTEGER PRIMARY KEY,
                    workflow_id INTEGER,
                    pid INTEGER,
                    start_time DATETIME,
                    end_time DATETIME,
                    exit_code INTEGER,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (id)
                );
            ''')
            
    def get_pending_workflows(self):
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, name, command, priority FROM workflows 
                WHERE enabled = 1 AND schedule <= datetime('now')
            ''')
            return cursor.fetchall()
            
    def execute_workflow(self, workflow_id, command, priority):
        try:
            # Set real-time priority for child process
            env = os.environ.copy()
            env['AIOS_PRIORITY'] = str(priority)
            
            proc = subprocess.Popen(
                command,
                shell=True,
                start_new_session=True,  # Prevent zombie processes
                env=env,
                preexec_fn=lambda: set_realtime_priority(priority)
            )
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    'INSERT INTO executions (workflow_id, pid, start_time) VALUES (?, ?, ?)',
                    (workflow_id, proc.pid, datetime.now())
                )
                
            self.processes[proc.pid] = workflow_id
            journal.send(f"Started workflow {workflow_id} with PID {proc.pid}")
            
        except Exception as e:
            journal.send(f"Error executing workflow {workflow_id}: {str(e)}", PRIORITY=journal.LOG_ERR)
            
    def monitor_processes(self):
        for pid, workflow_id in list(self.processes.items()):
            try:
                _, status = os.waitpid(pid, os.WNOHANG)
                if os.WIFEXITED(status):
                    exit_code = os.WEXITSTATUS(status)
                    with sqlite3.connect(self.db_path) as conn:
                        conn.execute(
                            'UPDATE executions SET end_time=?, exit_code=? WHERE pid=?',
                            (datetime.now(), exit_code, pid)
                        )
                    del self.processes[pid]
                    journal.send(f"Workflow {workflow_id} completed with exit code {exit_code}")
            except ChildProcessError:
                del self.processes[pid]
                
    def run(self):
        # Initialize systemd integration
        daemon.notify('READY=1')
        journal.send("AIOS Manager started successfully")
        
        set_realtime_priority(90)  # Higher priority for manager
        
        # Main loop
        while self.running:
            try:
                # Update watchdog
                daemon.notify('WATCHDOG=1')
                
                # Check for new workflows
                workflows = self.get_pending_workflows()
                for wf_id, name, cmd, priority in workflows:
                    self.execute_workflow(wf_id, cmd, priority)
                
                # Monitor running processes
                self.monitor_processes()
                
                # Sleep with watchdog timing
                time.sleep(self.watchdog_interval)
                
            except Exception as e:
                journal.send(f"Main loop error: {str(e)}", PRIORITY=journal.LOG_ERR)
                time.sleep(self.watchdog_interval)
                
        # Clean shutdown
        for pid in list(self.processes.keys()):
            try:
                os.kill(pid, signal.SIGTERM)
            except ProcessLookupError:
                pass
        daemon.notify('STOPPING=1')

if __name__ == '__main__':
    manager = AIOSManager()
    manager.init_db()
    manager.run()


================================================================================
FILE: deepseek2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Systemd Manager - Synthesized Production Version
Combines best practices from Android, Kubernetes, and large-scale deployments
Manages AI workflows with systemd integration, real-time scheduling, and zombie prevention
"""

import os
import sys
import sqlite3
import subprocess
import signal
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional

# Systemd integration
try:
    from systemd import daemon, journal
    logging.root.addHandler(journal.JournalHandler(SYSLOG_IDENTIFIER='aios'))
except ImportError:
    pass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIOSSystemdManager:
    def __init__(self, db_path: str = "/var/lib/aios/aios.db"):
        self.db_path = db_path
        self.running = True
        self.processes: Dict[int, int] = {}  # pid -> workflow_id
        
        # Setup signal handlers
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGCHLD, self._reap_children)
        
        # Initialize database
        self._init_db()
        
    def _init_db(self):
        """Initialize SQLite database with required tables"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS workflows (
                    id INTEGER PRIMARY KEY,
                    name TEXT NOT NULL,
                    command TEXT NOT NULL,
                    schedule TEXT,
                    priority INTEGER DEFAULT 0,
                    state TEXT DEFAULT 'pending',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS executions (
                    id INTEGER PRIMARY KEY,
                    workflow_id INTEGER,
                    pid INTEGER,
                    start_time DATETIME,
                    end_time DATETIME,
                    exit_code INTEGER,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (id)
                );
            """)
    
    def _handle_signal(self, signum, frame):
        """Handle shutdown signals gracefully"""
        logger.info(f"Received signal {signum}, initiating shutdown")
        self.running = False
        daemon.notify('STOPPING=1') if 'daemon' in globals() else None
    
    def _reap_children(self, signum, frame):
        """Reap completed child processes to prevent zombies (Android pattern)"""
        try:
            while True:
                pid, status = os.waitpid(-1, os.WNOHANG)
                if pid == 0:
                    break
                if pid in self.processes:
                    workflow_id = self.processes[pid]
                    exit_code = os.WEXITSTATUS(status) if os.WIFEXITED(status) else -1
                    self._update_execution(workflow_id, pid, exit_code)
                    del self.processes[pid]
                    logger.info(f"Workflow {workflow_id} completed with exit code {exit_code}")
        except ChildProcessError:
            pass
    
    def _update_execution(self, workflow_id: int, pid: int, exit_code: int):
        """Update execution record in database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "UPDATE executions SET end_time=CURRENT_TIMESTAMP, exit_code=? WHERE pid=?",
                (exit_code, pid)
            )
            conn.execute(
                "UPDATE workflows SET state='completed' WHERE id=?",
                (workflow_id,)
            )
    
    def submit_workflow(self, name: str, command: str, schedule: Optional[str] = None, 
                       priority: int = 0) -> int:
        """Submit a new workflow for review"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "INSERT INTO workflows (name, command, schedule, priority) VALUES (?, ?, ?, ?) RETURNING id",
                (name, command, schedule, priority)
            )
            workflow_id = cursor.fetchone()[0]
            logger.info(f"Submitted workflow {workflow_id}: {name}")
            return workflow_id
    
    def review_workflow(self, workflow_id: int, approve: bool):
        """Review and approve/reject a workflow"""
        state = "approved" if approve else "rejected"
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "UPDATE workflows SET state=? WHERE id=?",
                (state, workflow_id)
            )
        logger.info(f"Workflow {workflow_id} {state}")
    
    def _set_realtime_priority(self, pid: int, priority: int):
        """Set real-time scheduling priority for process (Android/Kubernetes pattern)"""
        try:
            subprocess.run([
                'systemctl', 'set-property', 
                f'{pid}', 
                f'CPUSchedulingPolicy=rr',
                f'CPUSchedulingPriority={priority}'
            ], check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.warning(f"Could not set real-time priority for PID {pid}")
    
    def execute_workflow(self, workflow_id: int):
        """Execute a workflow using systemd-run for proper supervision"""
        with sqlite3.connect(self.db_path) as conn:
            workflow = conn.execute(
                "SELECT name, command, priority FROM workflows WHERE id=?", 
                (workflow_id,)
            ).fetchone()
        
        if not workflow:
            logger.error(f"Workflow {workflow_id} not found")
            return
        
        name, command, priority = workflow
        
        try:
            # Use systemd-run for proper process supervision and resource control
            proc = subprocess.Popen([
                'systemd-run', '--user', '--scope', '--same-dir',
                '--property=CPUWeight=100',
                '--property=MemoryMax=500M',
                '/bin/sh', '-c', command
            ], start_new_session=True)
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    "INSERT INTO executions (workflow_id, pid, start_time) VALUES (?, ?, CURRENT_TIMESTAMP)",
                    (workflow_id, proc.pid)
                )
                conn.execute(
                    "UPDATE workflows SET state='running' WHERE id=?",
                    (workflow_id,)
                )
            
            self.processes[proc.pid] = workflow_id
            
            # Set real-time priority if specified
            if priority > 0:
                self._set_realtime_priority(proc.pid, priority)
            
            logger.info(f"Started workflow {workflow_id} with PID {proc.pid}")
            
        except Exception as e:
            logger.error(f"Error executing workflow {workflow_id}: {str(e)}")
    
    def run_scheduler(self):
        """Main scheduler loop that processes approved workflows"""
        if 'daemon' in globals():
            daemon.notify('READY=1')
        
        logger.info("AIOS Systemd Manager started")
        
        while self.running:
            try:
                # Get approved workflows
                with sqlite3.connect(self.db_path) as conn:
                    workflows = conn.execute(
                        "SELECT id, schedule FROM workflows WHERE state='approved'"
                    ).fetchall()
                
                # Execute workflows based on schedule
                for workflow_id, schedule in workflows:
                    if self._should_execute(schedule):
                        self.execute_workflow(workflow_id)
                
                # Update systemd watchdog
                if 'daemon' in globals():
                    daemon.notify('WATCHDOG=1')
                
                time.sleep(5)
                
            except Exception as e:
                logger.error(f"Scheduler error: {str(e)}")
                time.sleep(30)
        
        # Clean shutdown
        for pid in list(self.processes.keys()):
            try:
                os.kill(pid, signal.SIGTERM)
            except ProcessLookupError:
                pass
        
        logger.info("AIOS Systemd Manager stopped")
    
    def _should_execute(self, schedule: Optional[str]) -> bool:
        """Determine if a workflow should execute based on its schedule"""
        if not schedule:
            return True  # Immediate execution
        
        # Implement simple scheduling logic
        # In production, this would parse systemd-style calendar events
        return True

def main():
    """Command-line interface for AIOS management"""
    manager = AIOSSystemdManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "submit" and len(sys.argv) >= 4:
            name = sys.argv[2]
            cmd = sys.argv[3]
            schedule = sys.argv[4] if len(sys.argv) > 4 else None
            priority = int(sys.argv[5]) if len(sys.argv) > 5 else 0
            manager.submit_workflow(name, cmd, schedule, priority)
        
        elif command == "review" and len(sys.argv) >= 4:
            workflow_id = int(sys.argv[2])
            action = sys.argv[3].lower()
            manager.review_workflow(workflow_id, action == "approve")
        
        elif command == "start":
            manager.run_scheduler()
        
        else:
            print("Usage: aios [submit|review|start]")
    else:
        manager.run_scheduler()

if __name__ == "__main__":
    main()


================================================================================
FILE: gemini2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Orchestrator: Synthesized Production Model
- Uses a high-performance SQLite queue for task state.
- Delegates execution and reaping to systemd via transient units.
"""
import sqlite3
import subprocess
import json
import time
import sys
import os
import signal
import shlex
from pathlib import Path

# --- Configuration ---
DB_PATH = Path(__file__).parent / "aios.db"
UNIT_PREFIX = "aios-task-"

# --- High-Performance SQLite Task Queue ---
class TaskQueue:
    """Manages task state in SQLite using production-grade settings."""
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(str(db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA busy_timeout=5000;
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL UNIQUE,
                cmd TEXT NOT NULL,
                rt_priority INTEGER DEFAULT 0, -- 0 for non-RT, 1-99 for RT
                status TEXT DEFAULT 'pending', -- pending, running, completed, failed
                created_at INT DEFAULT (strftime('%s', 'now')),
                completed_at INT
            );
            CREATE INDEX IF NOT EXISTS idx_status_prio ON tasks(status, rt_priority DESC, created_at);
        """)

    def add(self, name, cmd, rt_priority=0):
        """Adds a task, returning its ID or None if the name is not unique."""
        try:
            return self.conn.execute(
                "INSERT INTO tasks(name, cmd, rt_priority) VALUES(?,?,?)",
                (name, cmd, rt_priority)
            ).lastrowid
        except sqlite3.IntegrityError:
            print(f"Error: Task name '{name}' already exists.")
            return None

    def get_next(self):
        """Atomically fetches and marks the next task as 'running'."""
        try:
            cursor = self.conn.execute("""
                UPDATE tasks SET status='running' WHERE id = (
                    SELECT id FROM tasks WHERE status='pending'
                    ORDER BY rt_priority DESC, created_at ASC LIMIT 1
                ) RETURNING id, name, cmd, rt_priority
            """)
            row = cursor.fetchone()
            return dict(row) if row else None
        except sqlite3.OperationalError: # Fallback for older SQLite
            with self.conn: # Implicit BEGIN/COMMIT
                row = self.conn.execute(
                    "SELECT id, name, cmd, rt_priority FROM tasks WHERE status='pending' "
                    "ORDER BY rt_priority DESC, created_at ASC LIMIT 1"
                ).fetchone()
                if not row: return None
                self.conn.execute("UPDATE tasks SET status='running' WHERE id=?", (row['id'],))
                return dict(row)

    def complete(self, task_id, success):
        """Marks a task as completed or failed."""
        status = 'completed' if success else 'failed'
        self.conn.execute(
            "UPDATE tasks SET status=?, completed_at=strftime('%s', 'now') WHERE id=?",
            (status, task_id)
        )

    def stats(self):
        """Returns a count of tasks by status."""
        return {r['status']: r['count'] for r in
                self.conn.execute("SELECT status, COUNT(*) as count FROM tasks GROUP BY status")}

# --- Systemd Executor for Perfect Reaping ---
class SystemdExecutor:
    """Delegates command execution to transient systemd units."""
    def _run_cmd(self, args):
        return subprocess.run(["systemctl", "--user"] + args, capture_output=True, text=True)

    def run_task(self, name, cmd, rt_priority):
        """Runs a command in a transient .service unit, which guarantees no zombies."""
        unit_name = f"{UNIT_PREFIX}{name.replace(' ', '_')}.service"
        print(f"Executing '{name}' via transient unit '{unit_name}'")

        run_args = ["systemd-run", "--user", "--collect", "--unit", unit_name,
                    "--property=StandardOutput=journal", "--property=StandardError=journal"]
        if rt_priority > 0:
            run_args.extend([f"--property=CPUSchedulingPolicy=rr",
                             f"--property=CPUSchedulingPriority={rt_priority}"])

        proc = subprocess.run(run_args + shlex.split(cmd), capture_output=True, text=True)
        if proc.returncode != 0:
            print(f"Error starting systemd-run for '{name}': {proc.stderr}")
            return False, None
        return True, unit_name

    def is_active(self, unit_name):
        return self._run_cmd(["is-active", unit_name]).returncode == 0

    def get_result(self, unit_name):
        res = self._run_cmd(["show", unit_name, "--property=Result"]).stdout.strip()
        return res.split("=")[1] == "success" if "=" in res else False

# --- Worker & CLI ---
def worker_loop(q, executor):
    """Connects the queue to the executor, managing running tasks."""
    print(f"Worker started (PID: {os.getpid()}). Press Ctrl+C to exit.")
    running_tasks = {}  # {task_id: unit_name}
    shutdown = threading.Event()
    signal.signal(signal.SIGINT, lambda s, f: shutdown.set())
    signal.signal(signal.SIGTERM, lambda s, f: shutdown.set())

    while not shutdown.is_set():
        for task_id, unit_name in list(running_tasks.items()):
            if not executor.is_active(unit_name):
                success = executor.get_result(unit_name)
                print(f"Task {task_id} ('{unit_name}') finished with status: {'SUCCESS' if success else 'FAILURE'}")
                q.complete(task_id, success)
                del running_tasks[task_id]

        if not shutdown.is_set() and (task := q.get_next()):
            success, unit_name = executor.run_task(task['name'], task['cmd'], task['rt_priority'])
            if success:
                running_tasks[task['id']] = unit_name
            else:
                q.complete(task['id'], success=False)

        time.sleep(1) # Poll interval
    print("\nShutdown complete.")

def main():
    if len(sys.argv) < 2 or sys.argv[1] not in ["worker", "add", "stats"]:
        print(f"Usage: {sys.argv[0]} <worker|add|stats>", file=sys.stderr)
        sys.exit(1)

    q = TaskQueue()
    cmd = sys.argv[1]

    if cmd == "worker":
        worker_loop(q, SystemdExecutor())
    elif cmd == "add":
        if len(sys.argv) < 4:
            print("Usage: add <name> <command> [--rt priority]", file=sys.stderr)
            sys.exit(1)
        name, command = sys.argv[2], sys.argv[3]
        rt = int(sys.argv[5]) if len(sys.argv) > 4 and sys.argv[4] == '--rt' else 0
        if task_id := q.add(name, command, rt):
            print(f"Added task '{name}' with ID {task_id}.")
    elif cmd == "stats":
        print(json.dumps(q.stats(), indent=2))

if __name__ == "__main__":
    main()


================================================================================
FILE: geminiAISTUDIO1.py
================================================================================

#!/usr/bin/env python3
import sqlite3
import subprocess
import os

DB_PATH = "/var/lib/aios/workflows.db"
SYSTEMD_USER_PATH = os.path.expanduser("~/.config/systemd/user/")

def setup_database():
    """Initializes the SQLite database and table if they don't exist."""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS workflows (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL UNIQUE,
            command TEXT NOT NULL,
            schedule TEXT, -- e.g., '*-*-* 02:00:00' for daily at 2 AM
            realtime BOOLEAN DEFAULT 0,
            priority INTEGER DEFAULT 50, -- Real-time priority (1-99)
            cpu_weight INTEGER DEFAULT 1024, -- Non-real-time CPU shares
            memory_max_mb INTEGER -- Max memory in MB
        )
    ''')
    conn.commit()
    conn.close()

def create_systemd_service(workflow):
    """Generates and writes a systemd .service file for a workflow."""
    service_content = f"""[Unit]
Description=AIOS Workflow: {workflow['name']}

[Service]
Type=simple
ExecStart={workflow['command']}
Restart=on-failure
"""
    if workflow['realtime']:
        service_content += f"CPUSchedulingPolicy=fifo\n"
        service_content += f"CPUSchedulingPriority={workflow['priority']}\n"
    else:
        service_content += f"CPUWeight={workflow['cpu_weight']}\n"

    if workflow['memory_max_mb']:
        service_content += f"MemoryMax={workflow['memory_max_mb']}M\n"

    service_content += "\n[Install]\nWantedBy=default.target\n"
    
    service_file_path = os.path.join(SYSTEMD_USER_PATH, f"{workflow['name']}.service")
    os.makedirs(SYSTEMD_USER_PATH, exist_ok=True)
    with open(service_file_path, "w") as f:
        f.write(service_content)
    return service_file_path

def create_systemd_timer(workflow):
    """Generates and writes a systemd .timer file for a scheduled workflow."""
    if not workflow.get('schedule'):
        return None
        
    timer_content = f"""[Unit]
Description=Timer for AIOS Workflow: {workflow['name']}

[Timer]
OnCalendar={workflow['schedule']}
Persistent=true

[Install]
WantedBy=timers.target
"""
    timer_file_path = os.path.join(SYSTEMD_USER_PATH, f"{workflow['name']}.timer")
    with open(timer_file_path, "w") as f:
        f.write(timer_content)
    return timer_file_path

def run_systemctl(command, unit_name):
    """Helper function to run systemctl commands for the user session."""
    try:
        subprocess.run(["systemctl", "--user"] + command + [unit_name], check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error executing systemctl command: {e}")

def add_and_enable_workflow(workflow_details):
    """Adds a workflow to the DB and creates/enables the systemd units."""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO workflows (name, command, schedule, realtime, priority, cpu_weight, memory_max_mb) VALUES (?, ?, ?, ?, ?, ?, ?)",
        (
            workflow_details['name'], workflow_details['command'],
            workflow_details.get('schedule'), workflow_details.get('realtime', 0),
            workflow_details.get('priority', 50), workflow_details.get('cpu_weight', 1024),
            workflow_details.get('memory_max_mb')
        )
    )
    conn.commit()
    conn.close()

    create_systemd_service(workflow_details)
    run_systemctl(["daemon-reload"], "")

    if workflow_details.get('schedule'):
        create_systemd_timer(workflow_details)
        run_systemctl(["enable", "--now"], f"{workflow_details['name']}.timer")
        print(f"Enabled and started timer for scheduled workflow: {workflow_details['name']}")
    else:
        run_systemctl(["enable", "--now"], f"{workflow_details['name']}.service")
        print(f"Enabled and started on-demand workflow: {workflow_details['name']}")

if __name__ == "__main__":
    setup_database()

    # Example: Add a standard, on-demand AI workflow for data processing
    data_processing_workflow = {
        "name": "aios-data-processor",
        "command": "/usr/bin/python3 -c 'import time; print(\"Processing data...\"); time.sleep(10)'",
        "memory_max_mb": 512
    }
    #add_and_enable_workflow(data_processing_workflow)

    # Example: Add a scheduled, daily reporting workflow
    reporting_workflow = {
        "name": "aios-daily-report",
        "command": "/usr/bin/python3 -c 'print(\"Generating daily report...\")'",
        "schedule": "*-*-* 03:00:00",
        "cpu_weight": 512
    }
    #add_and_enable_workflow(reporting_workflow)
    
    # Example: Add a real-time inference workflow
    inference_workflow = {
        "name": "aios-realtime-inference",
        "command": "/usr/bin/python3 /opt/aios/inference_server.py",
        "realtime": True,
        "priority": 90
    }
    #add_and_enable_workflow(inference_workflow)

    print("\nAIOS Manager setup complete. Check systemctl --user status for workflow status.")
    # To see the status of a service: systemctl --user status aios-data-processor.service
    # To see the status of a timer: systemctl --user list-timers


================================================================================
FILE: geminiAISTUDIO2.py
================================================================================

#!/usr/bin/env python3
import sqlite3
import subprocess
import os

DB_PATH = "/var/lib/aios/workflows.db"
SYSTEMD_USER_PATH = os.path.expanduser("~/.config/systemd/user/")

def setup_database():
    """Initializes the SQLite database and table if they don't exist."""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS workflows (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL UNIQUE,
            command TEXT NOT NULL,
            schedule TEXT, -- e.g., '*-*-* 02:00:00' for daily at 2 AM
            realtime BOOLEAN DEFAULT 0,
            priority INTEGER DEFAULT 50, -- Real-time priority (1-99)
            cpu_weight INTEGER DEFAULT 1024, -- Non-real-time CPU shares
            memory_max_mb INTEGER -- Max memory in MB
        )
    ''')
    conn.commit()
    conn.close()

def create_systemd_service(workflow):
    """Generates and writes a systemd .service file for a workflow."""
    service_content = f"""[Unit]
Description=AIOS Workflow: {workflow['name']}

[Service]
Type=simple
ExecStart={workflow['command']}
Restart=on-failure
"""
    if workflow['realtime']:
        service_content += f"CPUSchedulingPolicy=fifo\n"
        service_content += f"CPUSchedulingPriority={workflow['priority']}\n"
    else:
        service_content += f"CPUWeight={workflow['cpu_weight']}\n"

    if workflow['memory_max_mb']:
        service_content += f"MemoryMax={workflow['memory_max_mb']}M\n"

    service_content += "\n[Install]\nWantedBy=default.target\n"
    
    service_file_path = os.path.join(SYSTEMD_USER_PATH, f"{workflow['name']}.service")
    os.makedirs(SYSTEMD_USER_PATH, exist_ok=True)
    with open(service_file_path, "w") as f:
        f.write(service_content)
    return service_file_path

def create_systemd_timer(workflow):
    """Generates and writes a systemd .timer file for a scheduled workflow."""
    if not workflow.get('schedule'):
        return None
        
    timer_content = f"""[Unit]
Description=Timer for AIOS Workflow: {workflow['name']}

[Timer]
OnCalendar={workflow['schedule']}
Persistent=true

[Install]
WantedBy=timers.target
"""
    timer_file_path = os.path.join(SYSTEMD_USER_PATH, f"{workflow['name']}.timer")
    with open(timer_file_path, "w") as f:
        f.write(timer_content)
    return timer_file_path

def run_systemctl(command, unit_name):
    """Helper function to run systemctl commands for the user session."""
    try:
        subprocess.run(["systemctl", "--user"] + command + [unit_name], check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error executing systemctl command: {e}")

def add_and_enable_workflow(workflow_details):
    """Adds a workflow to the DB and creates/enables the systemd units."""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO workflows (name, command, schedule, realtime, priority, cpu_weight, memory_max_mb) VALUES (?, ?, ?, ?, ?, ?, ?)",
        (
            workflow_details['name'], workflow_details['command'],
            workflow_details.get('schedule'), workflow_details.get('realtime', 0),
            workflow_details.get('priority', 50), workflow_details.get('cpu_weight', 1024),
            workflow_details.get('memory_max_mb')
        )
    )
    conn.commit()
    conn.close()

    create_systemd_service(workflow_details)
    run_systemctl(["daemon-reload"], "")

    if workflow_details.get('schedule'):
        create_systemd_timer(workflow_details)
        run_systemctl(["enable", "--now"], f"{workflow_details['name']}.timer")
        print(f"Enabled and started timer for scheduled workflow: {workflow_details['name']}")
    else:
        run_systemctl(["enable", "--now"], f"{workflow_details['name']}.service")
        print(f"Enabled and started on-demand workflow: {workflow_details['name']}")

if __name__ == "__main__":
    setup_database()

    # Example: Add a standard, on-demand AI workflow for data processing
    data_processing_workflow = {
        "name": "aios-data-processor",
        "command": "/usr/bin/python3 -c 'import time; print(\"Processing data...\"); time.sleep(10)'",
        "memory_max_mb": 512
    }
    #add_and_enable_workflow(data_processing_workflow)

    # Example: Add a scheduled, daily reporting workflow
    reporting_workflow = {
        "name": "aios-daily-report",
        "command": "/usr/bin/python3 -c 'print(\"Generating daily report...\")'",
        "schedule": "*-*-* 03:00:00",
        "cpu_weight": 512
    }
    #add_and_enable_workflow(reporting_workflow)
    
    # Example: Add a real-time inference workflow
    inference_workflow = {
        "name": "aios-realtime-inference",
        "command": "/usr/bin/python3 /opt/aios/inference_server.py",
        "realtime": True,
        "priority": 90
    }
    #add_and_enable_workflow(inference_workflow)

    print("\nAIOS Manager setup complete. Check systemctl --user status for workflow status.")
    # To see the status of a service: systemctl --user status aios-data-processor.service
    # To see the status of a timer: systemctl --user list-timers


================================================================================
FILE: geminiDeep1.py
================================================================================

#!/usr/bin/env python3
import sqlite3
import subprocess
import os
import logging
import textwrap

# Configuration
DB_PATH = os.path.expanduser("~/aios_workflows.db")
# Using systemd user services for management without root
SYSTEMD_USER_DIR = os.path.expanduser("~/.config/systemd/user/")
os.makedirs(SYSTEMD_USER_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO)
UNIT_PREFIX = "aios-"

def init_db():
    """Initializes the SQLite database schema."""
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS workloads (
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE NOT NULL,
            command TEXT NOT NULL,
            schedule TEXT,
            realtime_prio INTEGER,
            enabled INTEGER DEFAULT 1
        )
    """)
    conn.commit()
    conn.close()

def run_systemctl(args):
    """Executes systemctl --user commands."""
    cmd = ["systemctl", "--user"] + args
    try:
        subprocess.run(cmd, check=True, capture_output=True)
    except subprocess.CalledProcessError as e:
        logging.error(f"systemctl failed: {' '.join(cmd)}. Error: {e.stderr.decode().strip()}")

def generate_units(wf):
    """Generates systemd service and timer content."""
    # Service Unit: Handles execution, reaping, and RT scheduling
    service_content = textwrap.dedent(f"""\
    [Unit]
    Description=AIOS Workload: {wf['name']}
    [Service]
    Type=simple
    ExecStart={wf['command']}
    Restart=on-failure
    KillMode=control-group
    Environment="PYTHONUNBUFFERED=1"
    """)
    
    # Real-time scheduling (Note: often requires configuration of user limits)
    if wf['realtime_prio'] and wf['realtime_prio'] > 0:
        service_content += f"CPUSchedulingPolicy=rr\nCPUSchedulingPriority={wf['realtime_prio']}\nCPUSchedulingResetOnFork=yes\n"

    # Timer Unit: Handles complex scheduling
    timer_content = None
    if wf['schedule']:
        timer_content = textwrap.dedent(f"""\
        [Unit]
        Description=AIOS Scheduler for: {wf['name']}
        [Timer]
        OnCalendar={wf['schedule']}
        Persistent=true
        [Install]
        WantedBy=timers.target
        """)
    return service_content, timer_content

def sync_workloads():
    """Synchronizes the database state with systemd units."""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    workflows = {w['name']: w for w in conn.execute("SELECT * FROM workloads").fetchall()}
    conn.close()

    # 1. Identify existing AIOS units on the filesystem
    existing_units = set()
    for filename in os.listdir(SYSTEMD_USER_DIR):
        if filename.startswith(UNIT_PREFIX) and (filename.endswith(".service") or filename.endswith(".timer")):
            base_name = filename.replace(UNIT_PREFIX, "").replace(".service", "").replace(".timer", "")
            existing_units.add(base_name)

    # 2. Create/Update units from DB
    for name, wf in workflows.items():
        s_content, t_content = generate_units(wf)
        with open(os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.service"), "w") as f:
            f.write(s_content)
        
        timer_path = os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.timer")
        if t_content:
            with open(timer_path, "w") as f:
                f.write(t_content)
        elif os.path.exists(timer_path):
            # Schedule removed; clean up the timer file
            run_systemctl(["stop", f"{UNIT_PREFIX}{name}.timer"])
            os.remove(timer_path)

    # 3. Remove orphaned units (Deleted from DB)
    for name in existing_units - workflows.keys():
        logging.info(f"Removing orphaned workload: {name}")
        run_systemctl(["stop", f"{UNIT_PREFIX}{name}.timer"])
        run_systemctl(["stop", f"{UNIT_PREFIX}{name}.service"])
        run_systemctl(["disable", f"{UNIT_PREFIX}{name}.timer"])
        run_systemctl(["disable", f"{UNIT_PREFIX}{name}.service"])
        if os.path.exists(os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.timer")):
             os.remove(os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.timer"))
        if os.path.exists(os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.service")):
             os.remove(os.path.join(SYSTEMD_USER_DIR, f"{UNIT_PREFIX}{name}.service"))

    # 4. Reload systemd configuration
    run_systemctl(["daemon-reload"])

    # 5. Apply desired state (Start/Stop/Enable/Disable)
    for name, wf in workflows.items():
        # Determine the primary unit to manage
        target = f"{UNIT_PREFIX}{name}.timer" if wf['schedule'] else f"{UNIT_PREFIX}{name}.service"
        
        if wf['enabled']:
            run_systemctl(["enable", "--now", target])
        else:
            run_systemctl(["disable", "--now", target])

if __name__ == '__main__':
    init_db()
    # This sync function ensures the systemd state matches the database configuration.
    sync_workloads()
    logging.info("AIOS workload synchronization complete.")
    # Note for systemd --user: Ensure 'linger' is enabled for the user if workloads
    # must run without an active login session (e.g., `loginctl enable-linger $USER`).


================================================================================
FILE: geminiDeep2.py
================================================================================

#!/usr/bin/env python3
import sqlite3
import subprocess
import logging
import textwrap
import json
from pathlib import Path
import os

# Configuration
DB_PATH = Path.home() / ".aios_workflows.db"
# Use systemd user services for management without root privileges
SYSTEMD_USER_DIR = Path.home() / ".config/systemd/user/"
os.makedirs(SYSTEMD_USER_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO)
UNIT_PREFIX = "aios-"

# AIOS Workflow States
STATE_APPROVED = "approved"

def init_db():
    """Initializes the SQLite database schema with AIOS states and configuration."""
    conn = sqlite3.connect(DB_PATH)
    # Use Write-Ahead Logging (WAL) mode for better concurrency and performance
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS workloads (
            id INTEGER PRIMARY KEY,
            name TEXT UNIQUE NOT NULL,
            command TEXT NOT NULL,
            schedule TEXT,           -- systemd OnCalendar format (e.g., 'daily')
            -- Status: proposed (for review), approved (active), rejected, disabled
            status TEXT DEFAULT 'proposed',
            config TEXT              -- JSON for advanced settings (RT, resources)
        )
    """)
    conn.commit()
    conn.close()

def run_systemctl(args):
    """Executes systemctl --user commands robustly."""
    cmd = ["systemctl", "--user"] + args
    try:
        # Execute command, capturing output silently. check=True raises on failure.
        subprocess.run(cmd, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        # Suppress common "unit not found" or "not loaded" errors during stop/disable, log others
        if "not found" not in e.stderr and "not loaded" not in e.stderr:
            logging.warning(f"systemctl command failed: {' '.join(cmd)}. STDERR: {e.stderr.strip()}")
    except FileNotFoundError:
        logging.error("systemctl command not found. Ensure systemd is installed and running.")

def generate_units(wf, config):
    """Generates systemd service and timer content."""
    # Service Unit: Handles execution and process management
    # KillMode=control-group ensures systemd reaps all child processes (no zombies)
    service_content = textwrap.dedent(f"""\
    [Unit]
    Description=AIOS Workload: {wf['name']}
    [Service]
    Type=simple
    ExecStart={wf['command']}
    Restart=on-failure
    KillMode=control-group
    Environment="PYTHONUNBUFFERED=1"
    """)
    
    # Apply Real-Time scheduling and Resource constraints from config
    # Real-time scheduling (e.g., {"rt_prio": 50, "rt_policy": "rr"})
    if config.get("rt_prio"):
        policy = config.get("rt_policy", "rr")
        service_content += f"CPUSchedulingPolicy={policy}\nCPUSchedulingPriority={config['rt_prio']}\n"
        service_content += "CPUSchedulingResetOnFork=yes\n" # Safety measure

    if config.get("mem_max_mb"):
        service_content += f"MemoryMax={config['mem_max_mb']}M\n"

    # Timer Unit: Handles complex scheduling
    timer_content = None
    if wf['schedule']:
        timer_content = textwrap.dedent(f"""\
        [Unit]
        Description=AIOS Scheduler for: {wf['name']}
        [Timer]
        OnCalendar={wf['schedule']}
        Persistent=true
        [Install]
        WantedBy=timers.target
        """)
    return service_content, timer_content

def sync_workloads():
    """The core reconciliation loop: syncs DB state (desired) with systemd (actual)."""
    logging.info("Starting AIOS workload synchronization...")
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    # We only manage workloads that are 'approved' or 'disabled' in systemd
    managed_workflows = {w['name']: w for w in conn.execute("SELECT * FROM workloads WHERE status IN (?, 'disabled')", (STATE_APPROVED,)).fetchall()}
    conn.close()

    # 1. Identify existing AIOS units on the filesystem
    existing_units = set()
    for filename in os.listdir(SYSTEMD_USER_DIR):
        if filename.startswith(UNIT_PREFIX) and (filename.endswith((".service", ".timer"))):
            base_name = filename.replace(UNIT_PREFIX, "").replace(".service", "").replace(".timer", "")
            existing_units.add(base_name)

    # 2. Create/Update units from DB
    for name, wf in managed_workflows.items():
        try:
            config = json.loads(wf['config'] or "{}")
        except json.JSONDecodeError:
            logging.error(f"Invalid JSON config for {name}. Skipping unit file generation.")
            continue

        s_content, t_content = generate_units(wf, config)
        (SYSTEMD_USER_DIR / f"{UNIT_PREFIX}{name}.service").write_text(s_content)
        
        timer_path = SYSTEMD_USER_DIR / f"{UNIT_PREFIX}{name}.timer"
        if t_content:
            timer_path.write_text(t_content)
        elif timer_path.exists():
            # Schedule was removed; stop and delete the timer file
            run_systemctl(["stop", f"{UNIT_PREFIX}{name}.timer"])
            timer_path.unlink()

    # 3. Remove orphaned units (Deleted, rejected, or returned to proposed)
    for name in existing_units - managed_workflows.keys():
        logging.info(f"Removing orphaned/unmanaged workload: {name}")
        run_systemctl(["stop", f"{UNIT_PREFIX}{name}.timer"])
        run_systemctl(["stop", f"{UNIT_PREFIX}{name}.service"])
        (SYSTEMD_USER_DIR / f"{UNIT_PREFIX}{name}.timer").unlink(missing_ok=True)
        (SYSTEMD_USER_DIR / f"{UNIT_PREFIX}{name}.service").unlink(missing_ok=True)

    # 4. Reload systemd configuration to apply file changes
    run_systemctl(["daemon-reload"])

    # 5. Apply desired running state (Start/Stop/Enable/Disable)
    for name, wf in managed_workflows.items():
        # If scheduled, manage the timer; otherwise, manage the service directly.
        target = f"{UNIT_PREFIX}{name}.timer" if wf['schedule'] else f"{UNIT_PREFIX}{name}.service"
        
        if wf['status'] == STATE_APPROVED:
            run_systemctl(["enable", "--now", target])
        elif wf['status'] == 'disabled':
            # Stop the unit now (--now) and disable auto-start on boot
            run_systemctl(["disable", "--now", target])

    logging.info("Synchronization complete.")

if __name__ == '__main__':
    init_db()
    # Run the synchronization whenever the database might have changed.
    sync_workloads()


================================================================================
FILE: geminiResearch1.py
================================================================================

#!/usr/bin/env python3
import sqlite3
import signal
import subprocess
import os
import time
import sys
import sdnotify

# --- Constants ---
DB_PATH = "aios.db"
SHUTDOWN_TIMEOUT = 10  # Seconds to wait for children to terminate

class AIOSManager:
    """
    Manages AI-generated workflows and programs as child processes,
    interfacing with systemd for robust service lifecycle management.
    """
    def __init__(self):
        self.running = True
        self.children =
        self.db_conn = None
        self.notifier = sdnotify.SystemdNotifier()

        # Set up graceful shutdown handlers
        signal.signal(signal.SIGINT, self.shutdown_handler)
        signal.signal(signal.SIGTERM, self.shutdown_handler)

    def _log(self, message):
        """Simple logger that prints to stderr for systemd journal capture."""
        print(f"AIOSManager: {message}", file=sys.stderr)

    def setup(self):
        """Initializes database and notifies systemd of progress."""
        self._log("Initializing...")
        self.notifier.notify("STATUS=Initializing database...")

        try:
            # Connect to SQLite DB. WAL mode is crucial for concurrency.
            self.db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)
            self.db_conn.execute("PRAGMA journal_mode=WAL;")
            self._create_schema()
            self._log("Database connection established.")
        except sqlite3.Error as e:
            self._log(f"Database initialization failed: {e}")
            self.notifier.notify(f"STATUS=DB Error: {e}")
            sys.exit(1)

    def _create_schema(self):
        """Ensures the necessary database tables exist."""
        cursor = self.db_conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS projects (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                status TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER,
                script_path TEXT NOT NULL,
                pid INTEGER,
                status TEXT NOT NULL,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        """)
        self.db_conn.commit()

    def run(self):
        """Main service loop."""
        self.setup()

        # Signal to systemd that we are fully initialized and ready.
        self._log("Startup complete. Notifying systemd.")
        self.notifier.notify("READY=1")

        # Example: Spawn a dummy workflow for demonstration
        self.spawn_workflow(1, "/usr/bin/sleep", ["3600"])

        while self.running:
            self.notifier.notify("STATUS=Monitoring workflows...")
            self._monitor_children()

            # In a real application, this loop would also check for new
            # projects to run from the database or an IPC queue.
            time.sleep(5)

        self._log("Main loop exited.")

    def spawn_workflow(self, workflow_id, command, args):
        """Launches and tracks a new workflow as a child process."""
        try:
            self._log(f"Spawning workflow {workflow_id}: {command} {' '.join(args)}")
            # Using Popen to run the child process non-blockingly
            process = subprocess.Popen([command] + args)
            self.children.append(process)

            # Update database with PID
            cursor = self.db_conn.cursor()
            cursor.execute(
                "UPDATE workflows SET pid =?, status = 'running' WHERE id =?",
                (process.pid, workflow_id)
            )
            self.db_conn.commit()
            self._log(f"Workflow {workflow_id} started with PID {process.pid}.")

        except (OSError, FileNotFoundError) as e:
            self._log(f"Failed to spawn workflow {workflow_id}: {e}")
            self.notifier.notify(f"STATUS=Spawn Error: {e}")

    def _monitor_children(self):
        """
        Checks for terminated child processes and reaps them.
        This prevents zombie processes at the application level.
        """
        reaped_pids =
        for i in reversed(range(len(self.children))):
            child = self.children[i]
            if child.poll() is not None:  # poll() returns exit code or None if running
                self._log(f"Child process {child.pid} terminated with code {child.returncode}.")
                reaped_pids.append(child.pid)
                del self.children[i]

        if reaped_pids:
            # Update database for reaped processes
            cursor = self.db_conn.cursor()
            placeholders = ','.join('?' for _ in reaped_pids)
            query = f"UPDATE workflows SET status = 'terminated' WHERE pid IN ({placeholders})"
            cursor.execute(query, reaped_pids)
            self.db_conn.commit()

    def shutdown_handler(self, signum, frame):
        """Handles SIGTERM/SIGINT for graceful shutdown."""
        self._log(f"Received signal {signum}. Initiating graceful shutdown.")
        self.notifier.notify("STOPPING=1\nSTATUS=Graceful shutdown initiated...")
        self.running = False

    def cleanup(self):
        """Performs final cleanup before exiting."""
        self._log("Terminating all child processes...")
        for child in self.children:
            if child.poll() is None:
                child.terminate() # Send SIGTERM

        # Wait for children to exit
        start_time = time.time()
        while any(child.poll() is None for child in self.children):
            if time.time() - start_time > SHUTDOWN_TIMEOUT:
                self._log("Timeout reached. Forcing termination of remaining children.")
                for child in self.children:
                    if child.poll() is None:
                        child.kill() # Send SIGKILL
                break
            time.sleep(0.1)

        self._log("All child processes terminated.")
        if self.db_conn:
            self.db_conn.close()
            self._log("Database connection closed.")
        self._log("Cleanup complete. Exiting.")

if __name__ == "__main__":
    manager = AIOSManager()
    try:
        manager.run()
    finally:
        manager.cleanup()


================================================================================
FILE: geminiResearch2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Orchestrator: Synthesized Production Model
- Uses a single SQLite DB for task state management.
- Delegates all execution and process reaping to transient systemd user units.
- Combines minimalism with production-grade patterns.
"""
import sqlite3
import subprocess
import json
import time
import sys
import shlex
from pathlib import Path

# --- Configuration ---
DB_PATH = Path(__file__).parent / "aios.db"
UNIT_PREFIX = "aios-task-"

class AiosDB:
    """Manages workflow state in a single SQLite database."""
    def __init__(self, path=DB_PATH):
        self.conn = sqlite3.connect(str(path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL UNIQUE,
                cmd TEXT NOT NULL,
                status TEXT DEFAULT 'proposed', -- proposed, approved, running, completed, failed
                rt_prio INTEGER DEFAULT 0, -- 0 for non-real-time, 1-99 for real-time
                schedule TEXT -- systemd OnCalendar string
            );
        """)

    def propose(self, name, cmd, schedule=None, rt_prio=0):
        try:
            return self.conn.execute(
                "INSERT INTO workflows(name, cmd, schedule, rt_prio) VALUES(?,?,?,?)",
                (name, cmd, schedule, rt_prio)
            ).lastrowid
        except sqlite3.IntegrityError:
            print(f"Error: Workflow name '{name}' already exists.", file=sys.stderr)
            return None

    def review(self, name, approve=True):
        status = 'approved' if approve else 'rejected'
        cur = self.conn.execute("UPDATE workflows SET status=? WHERE name=? AND status='proposed'", (status, name))
        if status == 'rejected':
            self.conn.execute("DELETE FROM workflows WHERE name=?", (name,))
        return cur.rowcount > 0

    def get_approved_workflow(self):
        # Atomically fetch and mark the next 'approved' task as 'running'
        try:
            cur = self.conn.execute("""
                UPDATE workflows SET status='running' WHERE id = (
                    SELECT id FROM workflows WHERE status='approved' LIMIT 1
                ) RETURNING id, name, cmd, schedule, rt_prio
            """)
            row = cur.fetchone()
            return dict(row) if row else None
        except sqlite3.OperationalError: # Fallback for older SQLite
            with self.conn:
                row = self.conn.execute("SELECT * FROM workflows WHERE status='approved' LIMIT 1").fetchone()
                if not row: return None
                self.conn.execute("UPDATE workflows SET status='running' WHERE id=?", (row['id'],))
                return dict(row)

    def finalize(self, name, success):
        status = 'completed' if success else 'failed'
        self.conn.execute("UPDATE workflows SET status=? WHERE name=?", (status, name))

class SystemdManager:
    """Delegates command execution to transient systemd --user units."""
    def _run(self, args, check=False):
        return subprocess.run(args, capture_output=True, text=True, check=check)

    def execute(self, wf):
        """Runs a workflow using systemd-run, returning the generated unit name."""
        unit_name = f"{UNIT_PREFIX}{wf['name']}.service"
        args = ["systemd-run", "--user", "--unit", unit_name, "--collect",
                "--property=KillMode=control-group",
                "--property=StandardOutput=journal",
                "--property=StandardError=journal"]
        if wf['schedule']:
            args.append(f"--on-calendar={wf['schedule']}")
        if wf['rt_prio'] and 1 <= wf['rt_prio'] <= 99:
            args.extend([f"--property=CPUSchedulingPolicy=rr",
                         f"--property=CPUSchedulingPriority={wf['rt_prio']}"])
        args.extend(shlex.split(wf['cmd']))

        proc = self._run(args)
        if proc.returncode != 0:
            print(f"Error launching systemd unit for '{wf['name']}': {proc.stderr}", file=sys.stderr)
            return None
        return unit_name

    def get_status(self, unit_name):
        """Checks the status of a systemd unit. Returns (state, result)."""
        proc = self._run(["systemctl", "--user", "show", unit_name, "--property=ActiveState,Result"])
        props = dict(line.split("=", 1) for line in proc.stdout.strip().splitlines() if "=" in line)
        return props.get("ActiveState"), props.get("Result")

def worker_loop(db, manager):
    """Main daemon loop to process approved workflows."""
    print(f"AIOS Worker started (PID: {os.getpid()}). Polling for approved workflows...")
    running = {} # Maps workflow name to unit name

    def shutdown(signum, frame):
        print("\nShutdown signal received. AIOS worker stopping.")
        nonlocal running
        running = None # Sentinel to break loop
    signal.signal(signal.SIGINT, shutdown)
    signal.signal(signal.SIGTERM, shutdown)

    while running is not None:
        # 1. Launch new approved workflows
        if len(running) < os.cpu_count(): # Simple concurrency limit
            wf = db.get_approved_workflow()
            if wf:
                print(f"Launching workflow '{wf['name']}'...")
                unit_name = manager.execute(wf)
                if unit_name:
                    running[wf['name']] = unit_name
                else:
                    db.finalize(wf['name'], success=False)

        # 2. Check status of running workflows
        for wf_name, unit_name in list(running.items()):
            state, result = manager.get_status(unit_name)
            if state in ('inactive', 'failed', 'deactivating'):
                success = (result == 'success')
                print(f"Workflow '{wf_name}' finished. Success: {success}")
                db.finalize(wf_name, success)
                del running[wf_name]

        time.sleep(2) # Poll interval
    print("Worker loop finished.")

def main():
    """CLI to manage and run the AIOS orchestrator."""
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <propose|review|worker>", file=sys.stderr)
        sys.exit(1)

    db = AiosDB()
    cmd = sys.argv[1]

    if cmd == 'propose':
        if len(sys.argv) < 4:
            print(f"Usage: {sys.argv[0]} propose <name> <command> [--schedule='...'] [--rt=prio]", file=sys.stderr)
            sys.exit(1)
        name, command = sys.argv[2], sys.argv[3]
        schedule = next((arg.split('=')[1] for arg in sys.argv[4:] if arg.startswith('--schedule')), None)
        rt_prio = int(next((arg.split('=')[1] for arg in sys.argv[4:] if arg.startswith('--rt')), 0))
        if db.propose(name, command, schedule, rt_prio):
            print(f"Workflow '{name}' proposed for review.")

    elif cmd == 'review':
        if len(sys.argv) != 4 or sys.argv[3] not in ['accept', 'reject']:
            print(f"Usage: {sys.argv[0]} review <name> <accept|reject>", file=sys.stderr)
            sys.exit(1)
        name, action = sys.argv[2], sys.argv[3]
        if db.review(name, approve=(action == 'accept')):
            print(f"Workflow '{name}' has been {action}ed.")
        else:
            print(f"Error: Could not find proposed workflow '{name}'.", file=sys.stderr)

    elif cmd == 'worker':
        manager = SystemdManager()
        worker_loop(db, manager)
    else:
        print(f"Unknown command: {cmd}", file=sys.stderr)

if __name__ == "__main__":
    main()


================================================================================
FILE: geminiWeb1.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Task Orchestrator: Synthesized Production Model
- Uses SQLite for task state management (high-performance WAL mode).
- Delegates all execution and process reaping to systemd for robustness.
- Combines minimalism and production-grade patterns.
"""
import sqlite3
import subprocess
import json
import time
import sys
import os
import signal
from pathlib import Path

# --- Configuration ---
DB_PATH = Path(__file__).parent / "aios.db"
UNIT_PREFIX = "aios-task-"

# --- SQLite Queue (`claudeCodeC` minimalism) ---
class TaskQueue:
    """Manages task state in SQLite using high-performance settings."""
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(str(db_path), isolation_level=None, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA busy_timeout=5000;
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL UNIQUE,
                cmd TEXT NOT NULL,
                priority INT DEFAULT 0,
                status TEXT DEFAULT 'pending', -- pending, running, completed, failed
                created_at INT DEFAULT (strftime('%s', 'now')),
                completed_at INT
            );
            CREATE INDEX IF NOT EXISTS idx_status_priority
            ON tasks(status, priority DESC, created_at);
        """)

    def add(self, name, cmd, priority=0):
        """Add a new task. Returns its ID or None if the name is not unique."""
        try:
            return self.conn.execute(
                "INSERT INTO tasks(name, cmd, priority) VALUES(?,?,?)",
                (name, cmd, priority)
            ).lastrowid
        except sqlite3.IntegrityError:
            print(f"Error: Task name '{name}' already exists.")
            return None

    def get_next(self):
        """Atomically fetch and mark the next pending task as 'running'."""
        # The 'UPDATE...RETURNING' is the most atomic and performant way to pop.
        # This avoids race conditions between workers.
        try:
            cursor = self.conn.execute("""
                UPDATE tasks SET status='running' WHERE id = (
                    SELECT id FROM tasks WHERE status='pending'
                    ORDER BY priority DESC, created_at ASC LIMIT 1
                ) RETURNING id, name, cmd
            """)
            row = cursor.fetchone()
            return dict(row) if row else None
        except sqlite3.OperationalError: # Fallback for older SQLite versions
            with self.conn:
                row = self.conn.execute(
                    "SELECT id, name, cmd FROM tasks WHERE status='pending' "
                    "ORDER BY priority DESC, created_at ASC LIMIT 1"
                ).fetchone()
                if not row: return None
                self.conn.execute("UPDATE tasks SET status='running' WHERE id=?", (row['id'],))
                return dict(row)


    def complete(self, task_id, success):
        """Mark a task as completed or failed."""
        status = 'completed' if success else 'failed'
        self.conn.execute(
            "UPDATE tasks SET status=?, completed_at=strftime('%s', 'now') WHERE id=?",
            (status, task_id)
        )

    def stats(self):
        """Return a count of tasks by status."""
        return {
            row['status']: row['count'] for row in
            self.conn.execute("SELECT status, COUNT(*) as count FROM tasks GROUP BY status")
        }

# --- Systemd Executor ---
class SystemdExecutor:
    """Delegates command execution to transient systemd units."""
    def _run_cmd(self, *args):
        return subprocess.run(["systemctl", "--user"] + list(args), capture_output=True, text=True)

    def run_task(self, name, cmd):
        """
        Runs a command in a transient .service unit.
        This is the core pattern: systemd handles logging, cgroups, and cleanup.
        The unit is automatically discarded after it stops.
        """
        unit_name = f"{UNIT_PREFIX}{name}.service"
        print(f"Executing task '{name}' via transient unit '{unit_name}'")
        # Using --no-block will start the unit and return immediately.
        # `systemd-run` is the ideal tool for this pattern.
        proc = self._run_cmd(
            "start",
            "--no-block",
            "--unit", unit_name,
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "/bin/sh", "-c", cmd
        )
        return proc.returncode == 0, unit_name

    def is_active(self, unit_name):
        """Check if the transient unit is still running."""
        proc = self._run_cmd("is-active", unit_name)
        return proc.returncode == 0 # is-active returns 0 if active

    def get_result(self, unit_name):
        """Get the result ('success' or 'failed') of a completed unit."""
        proc = self._run_cmd("show", unit_name, "--property=Result")
        if proc.stdout.strip():
            return proc.stdout.strip().split("=")[1] == "success"
        return False # Assume failure if result isn't found

# --- Worker Loop ---
def worker_loop(q, executor):
    """The main worker loop connecting the queue to the executor."""
    print(f"Worker started (PID: {os.getpid()}). Waiting for tasks...")
    running_tasks = {} # {task_id: unit_name}
    shutdown = threading.Event()

    def handle_signal(*_):
        print("\nShutdown signal received. Finishing running tasks...")
        shutdown.set()

    signal.signal(signal.SIGINT, handle_signal)
    signal.signal(signal.SIGTERM, handle_signal)

    while not shutdown.is_set():
        # Check status of ongoing tasks
        for task_id, unit_name in list(running_tasks.items()):
            if not executor.is_active(unit_name):
                success = executor.get_result(unit_name)
                status_str = "SUCCESS" if success else "FAILURE"
                print(f"Task {task_id} ('{unit_name}') finished with status: {status_str}")
                q.complete(task_id, success)
                del running_tasks[task_id]

        # Fetch new tasks if not shutting down
        if not shutdown.is_set():
            task = q.get_next()
            if task:
                success, unit_name = executor.run_task(task['name'], task['cmd'])
                if success:
                    running_tasks[task['id']] = unit_name
                else:
                    print(f"Error: Failed to start systemd unit for task {task['id']}")
                    q.complete(task['id'], success=False)

        time.sleep(1) # Poll interval

    print("Worker shutdown complete.")

# --- CLI Interface ---
def main():
    """A simple command-line interface for the orchestrator."""
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <worker|add|stats>")
        print("  worker                  - Start a worker process.")
        print("  add <name> <cmd> [pri]  - Add a new task.")
        print("  stats                   - Show queue statistics.")
        sys.exit(1)

    q = TaskQueue()
    command = sys.argv[1]

    if command == "worker":
        executor = SystemdExecutor()
        worker_loop(q, executor)
    elif command == "add":
        if len(sys.argv) < 4:
            print("Usage: add <name> <command> [priority]")
            sys.exit(1)
        name = sys.argv[2]
        cmd = sys.argv[3]
        priority = int(sys.argv[4]) if len(sys.argv) > 4 else 0
        task_id = q.add(name, cmd, priority)
        if task_id:
            print(f"Successfully added task '{name}' with ID {task_id}.")
    elif command == "stats":
        print(json.dumps(q.stats(), indent=2))
    else:
        print(f"Unknown command: {command}")

if __name__ == "__main__":
    main()


================================================================================
FILE: grok1.py
================================================================================

#!/usr/bin/env python3

import sqlite3
import os
import sys
import dbus
import argparse
import uuid
import subprocess

DB_PATH = 'aios.db'
UNIT_DIR = '/etc/systemd/system/'  # Or user dir for non-root
SYSTEMD_BUS = dbus.SystemBus()
SYSTEMD_OBJ = SYSTEMD_BUS.get_object('org.freedesktop.systemd1', '/org/freedesktop/systemd1')
MANAGER = dbus.Interface(SYSTEMD_OBJ, 'org.freedesktop.systemd1.Manager')

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS workflows
                 (id TEXT PRIMARY KEY, name TEXT, script_code TEXT, status TEXT, schedule TEXT, rt_priority INTEGER)''')
    conn.commit()
    conn.close()

def add_proposed_workflow(name, script_code, schedule=None, rt_priority=None):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    wf_id = str(uuid.uuid4())
    c.execute("INSERT INTO workflows VALUES (?, ?, ?, ?, ?, ?)",
              (wf_id, name, script_code, 'proposed', schedule, rt_priority))
    conn.commit()
    conn.close()
    print(f"Proposed workflow {name} added with ID {wf_id}")

def list_workflows(status=None):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    query = "SELECT id, name, status FROM workflows"
    if status:
        query += f" WHERE status = '{status}'"
    c.execute(query)
    rows = c.fetchall()
    conn.close()
    return rows

def review_workflow(wf_id, action):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT * FROM workflows WHERE id = ?", (wf_id,))
    wf = c.fetchone()
    if not wf:
        print("Workflow not found")
        return
    if action == 'accept':
        c.execute("UPDATE workflows SET status = 'approved' WHERE id = ?", (wf_id,))
        conn.commit()
        deploy_workflow(wf)
    elif action == 'reject':
        c.execute("DELETE FROM workflows WHERE id = ?", (wf_id,))
        conn.commit()
        print(f"Workflow {wf_id} rejected and deleted")
    conn.close()

def deploy_workflow(wf):
    wf_id, name, script_code, _, schedule, rt_priority = wf
    script_path = f"/tmp/{wf_id}.py"
    with open(script_path, 'w') as f:
        f.write(script_code)
    os.chmod(script_path, 0o755)

    service_name = f"aios-{wf_id}.service"
    service_path = os.path.join(UNIT_DIR, service_name)
    with open(service_path, 'w') as f:
        f.write(f"""[Unit]
Description=AIOS Workflow {name}

[Service]
Type=notify
ExecStart=/usr/bin/python3 {script_path}
Restart=on-failure
Environment=PYTHONUNBUFFERED=1
""")
        if rt_priority:
            f.write(f"CPUSchedulingPolicy=rr\nCPUSchedulingPriority={rt_priority}\nCPUSchedulingResetOnFork=true\n")

    if schedule:
        timer_name = f"aios-{wf_id}.timer"
        timer_path = os.path.join(UNIT_DIR, timer_name)
        with open(timer_path, 'w') as f:
            f.write(f"""[Unit]
Description=Timer for AIOS Workflow {name}

[Timer]
OnCalendar={schedule}  # e.g., '*-*-* *:0/30:00' for every 30 min

[Install]
WantedBy=timers.target
""")
        subprocess.run(['systemctl', 'daemon-reload'])
        MANAGER.EnableUnitFiles([timer_name], False, True)
        MANAGER.StartUnit(timer_name, 'replace')
    else:
        subprocess.run(['systemctl', 'daemon-reload'])
        MANAGER.EnableUnitFiles([service_name], False, True)
        MANAGER.StartUnit(service_name, 'replace')

    print(f"Workflow {wf_id} deployed")

def manage_workflow(wf_id, command):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT status FROM workflows WHERE id = ?", (wf_id,))
    status = c.fetchone()
    conn.close()
    if not status or status[0] != 'approved':
        print("Only approved workflows can be managed")
        return

    service_name = f"aios-{wf_id}.service"
    if command == 'stop':
        MANAGER.StopUnit(service_name, 'replace')
    elif command == 'restart':
        MANAGER.RestartUnit(service_name, 'replace')
    elif command == 'status':
        unit = MANAGER.GetUnit(service_name)
        prop = dbus.Interface(SYSTEMD_BUS.get_object('org.freedesktop.systemd1', unit), 'org.freedesktop.DBus.Properties')
        print(prop.Get('org.freedesktop.systemd1.Unit', 'ActiveState'))

if __name__ == '__main__':
    init_db()
    parser = argparse.ArgumentParser(description='AIOS Workflow Manager')
    subparsers = parser.add_subparsers(dest='cmd')

    add = subparsers.add_parser('add', help='Add proposed workflow')
    add.add_argument('name')
    add.add_argument('script_file', help='Path to script code file')
    add.add_argument('--schedule', help='systemd OnCalendar format')
    add.add_argument('--rt_priority', type=int, help='Real-time priority 1-99')

    list_cmd = subparsers.add_parser('list', help='List workflows')
    list_cmd.add_argument('--status', help='Filter by status')

    review = subparsers.add_parser('review', help='Review workflow')
    review.add_argument('wf_id')
    review.add_argument('action', choices=['accept', 'reject'])

    manage = subparsers.add_parser('manage', help='Manage approved workflow')
    manage.add_argument('wf_id')
    manage.add_argument('command', choices=['stop', 'restart', 'status'])

    args = parser.parse_args()

    if args.cmd == 'add':
        with open(args.script_file, 'r') as f:
            code = f.read()
        add_proposed_workflow(args.name, code, args.schedule, args.rt_priority)
    elif args.cmd == 'list':
        for row in list_workflows(args.status):
            print(row)
    elif args.cmd == 'review':
        review_workflow(args.wf_id, args.action)
    elif args.cmd == 'manage':
        manage_workflow(args.wf_id, args.command)
    else:
        parser.print_help()


================================================================================
FILE: grok2.py
================================================================================

import sqlite3
import os
import subprocess
import signal
import sys
import time
import logging
from systemd import journal  # Requires python-systemd package
import sdnotify  # Requires sdnotify package

# Setup logging to journal
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('aios_manager')
logger.addHandler(journal.JournalHandler(SYSLOG_IDENTIFIER='aios'))

DB_FILE = 'aios.db'
UNIT_DIR = '/etc/systemd/system/'  # System-wide; use ~/.config/systemd/user/ for user

def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS workflows
                 (id INTEGER PRIMARY KEY, name TEXT, code TEXT, status TEXT, scheduling TEXT, realtime INTEGER)''')
    conn.commit()
    conn.close()

def propose_workflow(name, code, scheduling='none', realtime=0):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("INSERT INTO workflows (name, code, status, scheduling, realtime) VALUES (?, ?, 'proposed', ?, ?)",
              (name, code, scheduling, realtime))
    conn.commit()
    conn.close()
    logger.info(f"Proposed workflow: {name}")

def review_workflows():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT * FROM workflows WHERE status = 'proposed'")
    return c.fetchall()

def accept_workflow(wf_id):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT name, code, scheduling, realtime FROM workflows WHERE id = ?", (wf_id,))
    wf = c.fetchone()
    if wf:
        name, code, scheduling, realtime = wf
        # Write code to file
        script_path = f"/opt/aios/{name}.py"
        os.makedirs(os.path.dirname(script_path), exist_ok=True)
        with open(script_path, 'w') as f:
            f.write(code)
        os.chmod(script_path, 0o755)
        # Create unit file
        unit_file = f"{UNIT_DIR}{name}.service"
        with open(unit_file, 'w') as f:
            f.write(f"""[Unit]
Description=AIOS Workflow: {name}
After=network.target

[Service]
ExecStart=/usr/bin/python3 {script_path}
User=root
Type=notify
Restart=on-failure
CPUSchedulingPolicy=rr
CPUSchedulingPriority={realtime if realtime else 50}

[Install]
WantedBy=multi-user.target
""")
        # If scheduling, create timer
        if scheduling != 'none':
            timer_file = f"{UNIT_DIR}{name}.timer"
            with open(timer_file, 'w') as f:
                f.write(f"""[Unit]
Description=Timer for {name}

[Timer]
OnCalendar={scheduling}
Persistent=true

[Install]
WantedBy=timers.target
""")
            subprocess.run(['systemctl', 'enable', f"{name}.timer"])
        subprocess.run(['systemctl', 'daemon-reload'])
        subprocess.run(['systemctl', 'enable', f"{name}.service"])
        subprocess.run(['systemctl', 'start', f"{name}.service"])
        c.execute("UPDATE workflows SET status = 'accepted' WHERE id = ?", (wf_id,))
        conn.commit()
        logger.info(f"Accepted and started: {name}")
    conn.close()

def reject_workflow(wf_id):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("UPDATE workflows SET status = 'rejected' WHERE id = ?", (wf_id,))
    conn.commit()
    conn.close()
    logger.info(f"Rejected workflow ID: {wf_id}")

def reap_children():
    while True:
        try:
            pid, status = os.waitpid(-1, os.WNOHANG)
            if pid == 0:
                break
            logger.info(f"Reaped child {pid}")
        except ChildProcessError:
            break

def handle_signals(signum, frame):
    logger.info("Shutting down...")
    reap_children()
    sys.exit(0)

def main():
    init_db()
    signal.signal(signal.SIGTERM, handle_signals)
    signal.signal(signal.SIGINT, handle_signals)
    n = sdnotify.SystemdNotifier()
    n.notify("READY=1")

    while True:
        print("\nAIOS Manager: 1=Propose, 2=Review/Accept/Reject, 3=Exit")
        choice = input("Choice: ")
        if choice == '1':
            name = input("Name: ")
            code = input("Code (multi-line, end with EOF): ")
            scheduling = input("Scheduling (e.g., '*-*-1 02:00:00' or 'none'): ")
            realtime = int(input("Real-time priority (1-99, 0 for none): "))
            propose_workflow(name, code, scheduling, realtime)
        elif choice == '2':
            wfs = review_workflows()
            for wf in wfs:
                print(f"ID: {wf[0]}, Name: {wf[1]}, Code: {wf[2]}")
            wf_id = int(input("ID to accept/reject: "))
            action = input("Accept (a) or Reject (r)? ")
            if action == 'a':
                accept_workflow(wf_id)
            elif action == 'r':
                reject_workflow(wf_id)
        elif choice == '3':
            break
        time.sleep(1)  # Poll gently
        reap_children()  # Reap any children

if __name__ == "__main__":
    main()


================================================================================
FILE: grok3.py
================================================================================

#!/usr/bin/env python3

import sqlite3
import os
import sys
import dbus
import argparse
import uuid
import subprocess

DB_PATH = os.path.expanduser('~/.aios.db')
UNIT_DIR = os.path.expanduser('~/.config/systemd/user/')
SYSTEMD_BUS = dbus.SystemBus()
SYSTEMD_OBJ = SYSTEMD_BUS.get_object('org.freedesktop.systemd1', '/org/freedesktop/systemd1')
MANAGER = dbus.Interface(SYSTEMD_OBJ, 'org.freedesktop.systemd1.Manager')

def init_db():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS workflows
                 (id TEXT PRIMARY KEY, name TEXT, script_code TEXT, status TEXT,
                  schedule TEXT, rt_policy TEXT, rt_priority INTEGER)''')
    conn.commit()
    conn.close()

def add_proposed_workflow(name, script_code, schedule=None, rt_policy=None, rt_priority=None):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    wf_id = str(uuid.uuid4())
    c.execute("INSERT INTO workflows VALUES (?, ?, ?, ?, ?, ?, ?)",
              (wf_id, name, script_code, 'proposed', schedule, rt_policy, rt_priority))
    conn.commit()
    conn.close()
    print(f"Proposed workflow {name} added with ID {wf_id}")

def list_workflows(status=None):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    query = "SELECT id, name, status FROM workflows"
    if status:
        query += f" WHERE status = '{status}'"
    c.execute(query)
    rows = c.fetchall()
    conn.close()
    return rows

def review_workflow(wf_id, action):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT * FROM workflows WHERE id = ?", (wf_id,))
    wf = c.fetchone()
    if not wf:
        print("Workflow not found")
        return
    if action == 'accept':
        c.execute("UPDATE workflows SET status = 'approved' WHERE id = ?", (wf_id,))
        conn.commit()
        deploy_workflow(wf)
    elif action == 'reject':
        c.execute("DELETE FROM workflows WHERE id = ?", (wf_id,))
        conn.commit()
        print(f"Workflow {wf_id} rejected and deleted")
    conn.close()

def deploy_workflow(wf):
    wf_id, name, script_code, _, schedule, rt_policy, rt_priority = wf
    script_path = os.path.expanduser(f'~/.aios_scripts/{wf_id}.py')
    os.makedirs(os.path.dirname(script_path), exist_ok=True)
    with open(script_path, 'w') as f:
        f.write(script_code)
    os.chmod(script_path, 0o755)

    service_name = f"aios-{wf_id}.service"
    service_path = os.path.join(UNIT_DIR, service_name)
    with open(service_path, 'w') as f:
        f.write(f"""[Unit]
Description=AIOS Workflow {name}
[Service]
Type=simple
ExecStart=/usr/bin/python3 {script_path}
Restart=on-failure
Environment=PYTHONUNBUFFERED=1
KillMode=control-group
""")
        if rt_policy and rt_priority:
            f.write(f"CPUSchedulingPolicy={rt_policy}\nCPUSchedulingPriority={rt_priority}\nCPUSchedulingResetOnFork=true\n")
        f.write("\n[Install]\nWantedBy=default.target\n")

    if schedule:
        timer_name = f"aios-{wf_id}.timer"
        timer_path = os.path.join(UNIT_DIR, timer_name)
        with open(timer_path, 'w') as f:
            f.write(f"""[Unit]
Description=Timer for AIOS Workflow {name}
[Timer]
OnCalendar={schedule}
Persistent=true
[Install]
WantedBy=timers.target
""")
        subprocess.run(['systemctl', '--user', 'daemon-reload'])
        MANAGER.EnableUnitFiles([timer_name], False, True)
        MANAGER.StartUnit(timer_name, 'replace')
    else:
        subprocess.run(['systemctl', '--user', 'daemon-reload'])
        MANAGER.EnableUnitFiles([service_name], False, True)
        MANAGER.StartUnit(service_name, 'replace')

    print(f"Workflow {wf_id} deployed")

def manage_workflow(wf_id, command):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT status FROM workflows WHERE id = ?", (wf_id,))
    status = c.fetchone()
    conn.close()
    if not status or status[0] != 'approved':
        print("Only approved workflows can be managed")
        return

    service_name = f"aios-{wf_id}.service"
    if command == 'stop':
        MANAGER.StopUnit(service_name, 'replace')
    elif command == 'restart':
        MANAGER.RestartUnit(service_name, 'replace')
    elif command == 'status':
        unit = MANAGER.GetUnit(service_name)
        prop = dbus.Interface(SYSTEMD_BUS.get_object('org.freedesktop.systemd1', unit), 'org.freedesktop.DBus.Properties')
        active_state = prop.Get('org.freedesktop.systemd1.Unit', 'ActiveState')
        sub_state = prop.Get('org.freedesktop.systemd1.Unit', 'SubState')
        print(f"ActiveState: {active_state}, SubState: {sub_state}")

if __name__ == '__main__':
    init_db()
    parser = argparse.ArgumentParser(description='AIOS Workflow Manager')
    subparsers = parser.add_subparsers(dest='cmd')

    add = subparsers.add_parser('add', help='Add proposed workflow')
    add.add_argument('name')
    add.add_argument('script_file', help='Path to script code file')
    add.add_argument('--schedule', help='systemd OnCalendar format, e.g., "*-*-* *:0/30:00"')
    add.add_argument('--rt_policy', default='rr', help='Real-time policy: rr or fifo')
    add.add_argument('--rt_priority', type=int, help='Real-time priority 1-99')

    list_cmd = subparsers.add_parser('list', help='List workflows')
    list_cmd.add_argument('--status', help='Filter by status')

    review = subparsers.add_parser('review', help='Review workflow')
    review.add_argument('wf_id')
    review.add_argument('action', choices=['accept', 'reject'])

    manage = subparsers.add_parser('manage', help='Manage approved workflow')
    manage.add_argument('wf_id')
    manage.add_argument('command', choices=['stop', 'restart', 'status'])

    args = parser.parse_args()

    if args.cmd == 'add':
        with open(args.script_file, 'r') as f:
            code = f.read()
        add_proposed_workflow(args.name, code, args.schedule, args.rt_policy, args.rt_priority)
    elif args.cmd == 'list':
        for row in list_workflows(args.status):
            print(row)
    elif args.cmd == 'review':
        review_workflow(args.wf_id, args.action)
    elif args.cmd == 'manage':
        manage_workflow(args.wf_id, args.command)
    else:
        parser.print_help()


================================================================================
FILE: grok4.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Workflow Manager - Synthesized from best practices
- Uses file-based units for persistence
- pystemd for direct control (pip install pystemd)
- sdnotify and journal for integration
- SQLite for state with approval flow
- Real-time scheduling, timers for complex schedules
- Graceful shutdown, no manual reaping (systemd handles)
"""
import sqlite3
import os
import sys
import time
import signal
import logging
from pathlib import Path
try:
    from pystemd.systemd1 import Unit, Manager
    from pystemd import daemon
except ImportError:
    print("Requires: pip install pystemd")
    sys.exit(1)
# Setup logging to journal
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('aios')
logger.addHandler(daemon.JournalHandler(SYSLOG_IDENTIFIER='aios'))
DB_PATH = Path('/var/lib/aios/aios.db')
UNIT_DIR = Path('/etc/systemd/system/')
class AIOSManager:
    def __init__(self):
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(DB_PATH)
        self.conn.row_factory = sqlite3.Row
        self._init_db()
        self.manager = Manager()
        self.manager.load()
        self.running = True
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        daemon.notify('READY=1')
        logger.info("AIOS Manager started")
    def _init_db(self):
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                code TEXT NOT NULL,
                status TEXT DEFAULT 'proposed',  -- proposed, accepted, rejected
                scheduling TEXT,  -- OnCalendar format or none
                realtime_priority INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        self.conn.commit()
    def _handle_signal(self, signum, frame):
        logger.info(f"Received signal {signum}, shutting down")
        self.running = False
        daemon.notify('STOPPING=1')
    def propose_workflow(self, name, code, scheduling='none', realtime=0):
        self.conn.execute(
            "INSERT INTO workflows (name, code, scheduling, realtime_priority) VALUES (?, ?, ?, ?)",
            (name, code, scheduling, realtime)
        )
        self.conn.commit()
        logger.info(f"Proposed workflow: {name}")
    def review_workflows(self):
        return self.conn.execute(
            "SELECT id, name, code FROM workflows WHERE status = 'proposed'"
        ).fetchall()
    def accept_workflow(self, wf_id):
        wf = self.conn.execute(
            "SELECT * FROM workflows WHERE id = ? AND status = 'proposed'", (wf_id,)
        ).fetchone()
        if not wf:
            return False
        name, code, scheduling, realtime = wf['name'], wf['code'], wf['scheduling'], wf['realtime_priority']
        script_path = f"/opt/aios/{name}.py"
        os.makedirs(os.path.dirname(script_path), exist_ok=True)
        with open(script_path, 'w') as f:
            f.write(code)
        os.chmod(script_path, 0o755)
        service_name = f"aios-{name}.service"
        service_path = UNIT_DIR / service_name
        service_content = f"""[Unit]
Description=AIOS Workflow: {name}
After=network.target
[Service]
Type=notify
ExecStart=/usr/bin/python3 {script_path}
Restart=on-failure
StandardOutput=journal
StandardError=journal
KillMode=control-group
"""
        if realtime > 0:
            service_content += f"CPUSchedulingPolicy=rr\nCPUSchedulingPriority={realtime}\n"
        service_content += "[Install]\nWantedBy=multi-user.target\n"
        service_path.write_text(service_content)
        if scheduling != 'none':
            timer_name = f"aios-{name}.timer"
            timer_path = UNIT_DIR / timer_name
            timer_content = f"""[Unit]
Description=Timer for {name}
[Timer]
OnCalendar={scheduling}
Persistent=true
Unit={service_name}
[Install]
WantedBy=timers.target
"""
            timer_path.write_text(timer_content)
            self.manager.Reload()
            unit = Unit(timer_name.encode())
            unit.load()
            unit.Unit.Enable(b'true')
            unit.Unit.Start(b'replace')
        else:
            self.manager.Reload()
            unit = Unit(service_name.encode())
            unit.load()
            unit.Unit.Enable(b'true')
            unit.Unit.Start(b'replace')
        self.conn.execute(
            "UPDATE workflows SET status = 'accepted' WHERE id = ?", (wf_id,)
        )
        self.conn.commit()
        logger.info(f"Accepted and deployed: {name}")
        return True
    def reject_workflow(self, wf_id):
        self.conn.execute(
            "UPDATE workflows SET status = 'rejected' WHERE id = ?", (wf_id,)
        )
        self.conn.commit()
        logger.info(f"Rejected workflow ID: {wf_id}")
    def run(self):
        while self.running:
            print("\nAIOS Manager CLI:")
            print("1. Propose workflow")
            print("2. Review/Accept/Reject proposed")
            print("3. Exit")
            choice = input("Choice: ").strip()
            if choice == '1':
                name = input("Name: ").strip()
                code = input("Code (multi-line, end with EOF):\n")
                scheduling = input("Scheduling (e.g., '*-*-1 02:00:00' or 'none'): ").strip()
                realtime = int(input("Real-time priority (1-99, 0 for none): ").strip())
                self.propose_workflow(name, code, scheduling, realtime)
            elif choice == '2':
                wfs = self.review_workflows()
                if not wfs:
                    print("No proposed workflows.")
                    continue
                for wf in wfs:
                    print(f"ID: {wf['id']}, Name: {wf['name']}, Code: {wf['code']}")
                wf_id = int(input("ID to review: ").strip())
                action = input("Accept (a) or Reject (r)? ").strip().lower()
                if action == 'a':
                    self.accept_workflow(wf_id)
                elif action == 'r':
                    self.reject_workflow(wf_id)
            elif choice == '3':
                break
            time.sleep(0.1)  # Gentle poll
        logger.info("AIOS Manager shutdown")
if __name__ == "__main__":
    manager = AIOSManager()
    manager.run()


================================================================================
FILE: kimi1.py
================================================================================

#!/usr/bin/env python3
"""
aios_systemd.py – tiny launcher for AIOS workflows
- starts any executable as a *transient* systemd user unit
- guarantees auto-reap (no zombies)
- supports real-time policy / nice / cgroup slices
- <200 lines, single file, zero deps except systemd
"""
import argparse, subprocess, sys, shlex, time, re, json, sqlite3, os

DB = "aios.sqlite"          # same DB AIOS already uses
SYSTEMD_RUN = "/usr/bin/systemd-run"

def _quote(*a): return " ".join(shlex.quote(str(x)) for x in a)

def _unit_name(wid: str) -> str:
    return "aios-" + re.sub(r"[^a-zA-Z0-9_:.-]", "_", wid)

def _status(unit: str):
    out = subprocess.run(["systemctl", "--user", "show", unit],
                         capture_output=True, text=True)
    return dict(line.split("=", 1) for line in out.stdout.splitlines() if "=" in line)

def launch(workflow_id: str, cmdline: list, *,
           realtime=False, nice=0, slice_name="aios.slice"):
    unit = _unit_name(workflow_id)
    extra = []
    if realtime:
        extra += ["--property=CPUSchedulingPolicy=rr",
                  "--property=CPUSchedulingPriority=90"]
    if nice:
        extra += [f"--property=Nice={nice}"]
    extra += [f"--slice={slice_name}"]
    subprocess.run([SYSTEMD_RUN,
                    "--user", "--collect", "--quiet",
                    "--unit", unit,
                    *extra, "--", *cmdline], check=True)
    return unit

def reap_until_done(unit: str, poll=1):
    while True:
        st = _status(unit).get("ActiveState", "")
        if st in ("inactive", "failed"): break
        time.sleep(poll)
    return st != "failed"

def db_record(wid: str, unit: str, status: str):
    with sqlite3.connect(DB) as con:
        con.execute("INSERT OR REPLACE INTO jobs(id,unit,status) VALUES(?,?,?)",
                    (wid, unit, status))

# ---------------- CLI ----------------
def cli():
    p = argparse.ArgumentParser()
    sub = p.add_subparsers(dest="cmd", required=True)

    a = sub.add_parser("launch", help="start workflow as systemd unit")
    a.add_argument("workflow_id")
    a.add_argument("command", nargs="+")
    a.add_argument("--realtime", action="store_true")
    a.add_argument("--nice", type=int, default=0)

    b = sub.add_parser("wait", help="block until workflow finishes")
    b.add_argument("workflow_id")

    c = sub.add_parser("cleanup", help="remove finished units from DB")

    args = p.parse_args()

    if args.cmd == "launch":
        unit = launch(args.workflow_id, args.command,
                      realtime=args.realtime, nice=args.nice)
        db_record(args.workflow_id, unit, "running")
        print(unit)

    elif args.cmd == "wait":
        unit = _unit_name(args.workflow_id)
        ok = reap_until_done(unit)
        db_record(args.workflow_id, unit, "done" if ok else "failed")
        sys.exit(0 if ok else 1)

    elif args.cmd == "cleanup":
        with sqlite3.connect(DB) as con:
            cur = con.execute("SELECT id,unit FROM jobs")
            for wid, unit in cur.fetchall():
                st = _status(unit).get("ActiveState", "")
                if st in ("inactive", "failed", None):
                    con.execute("DELETE FROM jobs WHERE id=?", (wid,))

if __name__ == "__main__":
    cli()


================================================================================
FILE: kimi2.py
================================================================================

#!/usr/bin/env python3
"""
aios_systemd.py – 180-line production launcher
* transient user units → systemd reaps zombies
* RT / quota / memory in one call
* SQLite ledger (same DB AIOS already owns)
* Works on any distro with systemd ≥ 238
"""
import argparse, sqlite3, subprocess, sys, shlex, json, time, re, os, signal
from pathlib import Path

DB         = Path(os.getenv("AIOS_DB") or "aios.sqlite")
SYSTEMD_RUN= "/usr/bin/systemd-run"
UNIT_PREFIX= "aios-"

# ------------------------------------------------------------------ helpers
def _run(cmd, *, check=True, quiet=True, **kw):
    return subprocess.run(cmd, check=check, capture_output=quiet, text=True, **kw)

def _unit(wid: str) -> str:
    return UNIT_PREFIX + re.sub(r"[^A-Za-z0-9:._-]", "_", wid)

def _status(unit: str) -> dict:
    out = _run(["systemctl", "--user", "show", unit], check=False).stdout
    return dict(line.split("=", 1) for line in out.splitlines() if "=" in line)

# ------------------------------------------------------------------ core API
def launch(workflow_id: str, cmdline: list, *,
           rt: bool = False, cpu: int = 100, mem: str = "1G", nice: int = 0):
    """Launch command as transient user unit.  Returns unit name."""
    unit = _unit(workflow_id)
    props = [f"MemoryMax={mem}", f"CPUQuota={cpu}%", f"Nice={nice}"]
    if rt:
        props += ["CPUSchedulingPolicy=rr", "CPUSchedulingPriority=90"]
    _run([SYSTEMD_RUN, "--user", "--collect", "--quiet",
          "--unit", unit, *map("--property={}".format, props), "--", *cmdline])
    _log(workflow_id, unit, "running")
    return unit

def reap(unit: str, timeout: int = 86400) -> bool:
    """Block until unit finishes (or timeout).  Returns success."""
    for _ in range(timeout):
        st = _status(unit).get("ActiveState", "")
        if st in ("inactive", "failed"):
            return st != "failed"
        time.sleep(1)
    return False

def kill(workflow_id: str):
    """Stop a running workflow immediately."""
    unit = _unit(workflow_id)
    _run(["systemctl", "--user", "stop", unit], check=False)

# ------------------------------------------------------------------ SQLite
def _log(wid: str, unit: str, state: str):
    DB.parent.mkdir(exist_ok=True)
    with sqlite3.connect(DB) as c:
        c.execute("INSERT OR REPLACE INTO jobs(id,unit,state) VALUES(?,?,?)", (wid, unit, state))

def _init_db():
    with sqlite3.connect(DB) as c:
        c.execute("""CREATE TABLE IF NOT EXISTS jobs(
                       id TEXT PRIMARY KEY,
                       unit TEXT,
                       state TEXT,
                       ts DATETIME DEFAULT CURRENT_TIMESTAMP)""")

# ------------------------------------------------------------------ CLI
def _cli():
    p = argparse.ArgumentParser()
    sub = p.add_subparsers(dest="cmd", required=True)

    a = sub.add_parser("launch", help="start workflow")
    a.add_argument("id")
    a.add_argument("cmd", nargs="+")
    a.add_argument("--rt", action="store_true", help="real-time SCHED_RR")
    a.add_argument("--cpu", type=int, default=100, help="CPU quota %")
    a.add_argument("--mem", default="1G", help="Memory cap (K/M/G)")
    a.add_argument("--nice", type=int, default=0)

    b = sub.add_parser("wait", help="block until done")
    b.add_argument("id")
    b.add_argument("--timeout", type=int, default=86400)

    c = sub.add_parser("kill", help="stop workflow")
    c.add_argument("id")

    d = sub.add_parser("status", help="json status")
    d.add_argument("id", nargs="?")

    return p.parse_args()

def main():
    _init_db()
    args = _cli()
    if args.cmd == "launch":
        unit = launch(args.id, args.cmd, rt=args.rt, cpu=args.cpu, mem=args.mem, nice=args.nice)
        print(unit)
    elif args.cmd == "wait":
        ok = reap(_unit(args.id), args.timeout)
        _log(args.id, _unit(args.id), "done" if ok else "failed")
        sys.exit(0 if ok else 1)
    elif args.cmd == "kill":
        kill(args.id)
    elif args.cmd == "status":
        if args.id:
            st = _status(_unit(args.id))
            print(json.dumps(st, indent=2))
        else:
            print(json.dumps({wid: _status(_unit(wid)) for wid in
                              {r[0] for r in sqlite3.connect(DB).execute("SELECT id FROM jobs")}},
                             indent=2))

if __name__ == "__main__":
    signal.signal(signal.SIGTERM, lambda *_: sys.exit(0))
    main()


================================================================================
FILE: qwen1.py
================================================================================

#!/usr/bin/env python3
import os
import sys
import sqlite3
import subprocess
import signal
from pathlib import Path

DB_PATH = Path("/var/lib/aios/workflows.db")
SYSTEMD_DIR = Path("/etc/systemd/system/")

class AIOSManager:
    def __init__(self):
        DB_PATH.parent.mkdir(exist_ok=True)
        self.db = sqlite3.connect(DB_PATH)
        self.init_db()

    def init_db(self):
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE,
                script_path TEXT,
                schedule TEXT,
                enabled BOOLEAN DEFAULT 1,
                accepted BOOLEAN DEFAULT 0
            )
        """)
        self.db.commit()

    def register_workflow(self, name, script, schedule="*-*-* *:*:00"):
        if not Path(script).exists():
            raise FileNotFoundError(f"Script {script} not found")
        self.db.execute(
            "INSERT OR REPLACE INTO workflows (name, script_path, schedule, accepted) VALUES (?, ?, ?, 1)",
            (name, script, schedule)
        )
        self.db.commit()
        self.write_systemd_unit(name, script, schedule)

    def write_systemd_unit(self, name, script, schedule):
        unit_name = f"aios-{name}.service"
        timer_name = f"aios-{name}.timer"
        service_content = f"""[Unit]
Description=AIOS Workflow - {name}
After=network.target

[Service]
Type=exec
ExecStart={sys.executable} {script}
Restart=on-failure
RestartSec=5
StandardOutput=journal
StandardError=journal
KillMode=process

[Install]
WantedBy=multi-user.target
"""
        timer_content = f"""[Unit]
Description=AIOS Timer for {name}

[Timer]
OnCalendar={schedule}
Persistent=true

[Install]
WantedBy=timers.target
"""
        (SYSTEMD_DIR / unit_name).write_text(service_content)
        (SYSTEMD_DIR / timer_name).write_text(timer_content)
        subprocess.run(["systemctl", "daemon-reload"], check=True)
        subprocess.run(["systemctl", "enable", timer_name], check=True)
        subprocess.run(["systemctl", "start", timer_name], check=True)

    def list_workflows(self):
        return self.db.execute("SELECT name, script_path, schedule, enabled FROM workflows WHERE accepted = 1").fetchall()

    def approve_workflow(self, name):
        self.db.execute("UPDATE workflows SET accepted = 1 WHERE name = ?", (name,))
        self.db.commit()

    def reject_workflow(self, name):
        self.db.execute("UPDATE workflows SET accepted = 0 WHERE name = ?", (name,))
        self.db.commit()
        self.disable_workflow(name)

    def disable_workflow(self, name):
        timer_name = f"aios-{name}.timer"
        service_name = f"aios-{name}.service"
        subprocess.run(["systemctl", "stop", timer_name], stderr=subprocess.DEVNULL)
        subprocess.run(["systemctl", "disable", timer_name], stderr=subprocess.DEVNULL)
        (SYSTEMD_DIR / timer_name).unlink(missing_ok=True)
        (SYSTEMD_DIR / service_name).unlink(missing_ok=True)
        subprocess.run(["systemctl", "daemon-reload"], check=True)

if __name__ == "__main__":
    mgr = AIOSManager()
    print("AIOS Systemd Workflow Manager (v0.1)")
    print("Workflows:", mgr.list_workflows())


================================================================================
FILE: qwen2.py
================================================================================

#!/usr/bin/env python3
import os
import sys
import sqlite3
import subprocess
import signal
from pathlib import Path

DB_PATH = Path("/var/lib/aios/workflows.db")
SYSTEMD_DIR = Path("/etc/systemd/system/")

class AIOSManager:
    def __init__(self):
        DB_PATH.parent.mkdir(exist_ok=True)
        self.db = sqlite3.connect(DB_PATH)
        self.init_db()

    def init_db(self):
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE,
                script_path TEXT,
                schedule TEXT,
                enabled BOOLEAN DEFAULT 1,
                accepted BOOLEAN DEFAULT 0
            )
        """)
        self.db.commit()

    def register_workflow(self, name, script, schedule="*-*-* *:*:00"):
        if not Path(script).exists():
            raise FileNotFoundError(f"Script {script} not found")
        self.db.execute(
            "INSERT OR REPLACE INTO workflows (name, script_path, schedule, accepted) VALUES (?, ?, ?, 1)",
            (name, script, schedule)
        )
        self.db.commit()
        self.write_systemd_unit(name, script, schedule)

    def write_systemd_unit(self, name, script, schedule):
        unit_name = f"aios-{name}.service"
        timer_name = f"aios-{name}.timer"
        service_content = f"""[Unit]
Description=AIOS Workflow - {name}
After=network.target

[Service]
Type=exec
ExecStart={sys.executable} {script}
Restart=on-failure
RestartSec=5
StandardOutput=journal
StandardError=journal
KillMode=process

[Install]
WantedBy=multi-user.target
"""
        timer_content = f"""[Unit]
Description=AIOS Timer for {name}

[Timer]
OnCalendar={schedule}
Persistent=true

[Install]
WantedBy=timers.target
"""
        (SYSTEMD_DIR / unit_name).write_text(service_content)
        (SYSTEMD_DIR / timer_name).write_text(timer_content)
        subprocess.run(["systemctl", "daemon-reload"], check=True)
        subprocess.run(["systemctl", "enable", timer_name], check=True)
        subprocess.run(["systemctl", "start", timer_name], check=True)

    def list_workflows(self):
        return self.db.execute("SELECT name, script_path, schedule, enabled FROM workflows WHERE accepted = 1").fetchall()

    def approve_workflow(self, name):
        self.db.execute("UPDATE workflows SET accepted = 1 WHERE name = ?", (name,))
        self.db.commit()

    def reject_workflow(self, name):
        self.db.execute("UPDATE workflows SET accepted = 0 WHERE name = ?", (name,))
        self.db.commit()
        self.disable_workflow(name)

    def disable_workflow(self, name):
        timer_name = f"aios-{name}.timer"
        service_name = f"aios-{name}.service"
        subprocess.run(["systemctl", "stop", timer_name], stderr=subprocess.DEVNULL)
        subprocess.run(["systemctl", "disable", timer_name], stderr=subprocess.DEVNULL)
        (SYSTEMD_DIR / timer_name).unlink(missing_ok=True)
        (SYSTEMD_DIR / service_name).unlink(missing_ok=True)
        subprocess.run(["systemctl", "daemon-reload"], check=True)

if __name__ == "__main__":
    mgr = AIOSManager()
    print("AIOS Systemd Workflow Manager (v0.1)")
    print("Workflows:", mgr.list_workflows())


================================================================================
FILE: systemd dbus.py
================================================================================

import dbus
import subprocess
import os
import sys
import time

SYSTEMD_BUSNAME = 'org.freedesktop.systemd1'
SYSTEMD_PATH = '/org/freedesktop/systemd1'
SYSTEMD_MANAGER_INTERFACE = 'org.freedesktop.systemd1.Manager'
SYSTEMD_UNIT_INTERFACE = 'org.freedesktop.systemd1.Unit'

bus = dbus.SystemBus()
proxy = bus.get_object('org.freedesktop.PolicyKit1', '/org/freedesktop/PolicyKit1/Authority')
authority = dbus.Interface(proxy, dbus_interface='org.freedesktop.PolicyKit1.Authority')
system_bus_name = bus.get_unique_name()
subject = ('system-bus-name', {'name' : system_bus_name})
action_id = 'org.freedesktop.systemd1.manage-units'
details = {}
flags = 1  # AllowUserInteraction flag
cancellation_id = ''  # No cancellation id
result = authority.CheckAuthorization(subject, action_id, details, flags, cancellation_id)

if result[1] != 0:
    sys.exit("Need administrative privilege")

systemd_object = bus.get_object(SYSTEMD_BUSNAME, SYSTEMD_PATH)
systemd_manager = dbus.Interface(systemd_object, SYSTEMD_MANAGER_INTERFACE)


================================================================================
FILE: systemdOrchestrator.py
================================================================================

#!/usr/bin/env python3
"""
Systemd-based orchestrator - Ultra-minimal, ultra-fast
Leverages systemd for process management, restart, and zombie reaping
"""
import os
import sys
import time
import subprocess
import json
from pathlib import Path

BASE_DIR = Path(__file__).parent.absolute()
UNIT_PREFIX = "aios-"

class SystemdOrchestrator:
    """Minimal systemd wrapper - let systemd handle everything"""

    def __init__(self):
        self.jobs = {}
        self._load_jobs()

    def _run(self, *args):
        """Run systemctl command"""
        return subprocess.run(["systemctl", "--user"] + list(args),
                            capture_output=True, text=True, check=False)

    def _load_jobs(self):
        """Load existing AIOS jobs from systemd"""
        result = self._run("list-units", f"{UNIT_PREFIX}*.service", "--no-legend", "--plain")
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                if parts:
                    name = parts[0].replace('.service', '').replace(UNIT_PREFIX, '')
                    self.jobs[name] = parts[0]

    def add_job(self, name: str, command: str, restart: str = "always") -> str:
        """Create systemd service unit"""
        unit_name = f"{UNIT_PREFIX}{name}.service"
        unit_path = Path(f"~/.config/systemd/user/{unit_name}").expanduser()
        unit_path.parent.mkdir(parents=True, exist_ok=True)

        # Systemd handles: zombie reaping, process groups, restart, logging
        unit_content = f"""[Unit]
Description=AIOS Job: {name}

[Service]
Type=simple
ExecStart=/bin/sh -c '{command}'
Restart={restart}
RestartSec=0
StandardOutput=journal
StandardError=journal
KillMode=control-group
TimeoutStopSec=0

[Install]
WantedBy=default.target
"""
        unit_path.write_text(unit_content)
        self.jobs[name] = unit_name
        self._run("daemon-reload")
        return unit_name

    def start_job(self, name: str) -> float:
        """Start job via systemd"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("start", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def stop_job(self, name: str) -> float:
        """Stop job immediately"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("stop", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def restart_job(self, name: str) -> float:
        """Restart job via systemd"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("restart", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def restart_all(self) -> dict:
        """Restart all jobs"""
        start = time.perf_counter()
        times = {}

        # Use systemd's batch restart for speed
        units = list(self.jobs.values())
        if units:
            result = self._run("restart", *units)
            for name in self.jobs:
                times[name] = 0.5  # systemd handles it in parallel

        total = (time.perf_counter() - start) * 1000
        print(f"=== RESTART ALL in {total:.2f}ms ===")
        return times

    def status(self) -> dict:
        """Get status of all jobs"""
        status = {}
        for name, unit in self.jobs.items():
            result = self._run("show", unit, "--property=ActiveState,MainPID,ExecMainStartTimestampMonotonic")
            props = {}
            for line in result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    props[k] = v

            status[name] = {
                'state': props.get('ActiveState', 'unknown'),
                'pid': int(props.get('MainPID', 0))
            }
        return status

    def cleanup(self):
        """Remove all AIOS systemd units"""
        for unit in self.jobs.values():
            self._run("stop", unit)
            self._run("disable", unit)
            unit_path = Path(f"~/.config/systemd/user/{unit}").expanduser()
            if unit_path.exists():
                unit_path.unlink()
        self._run("daemon-reload")

def main():
    """Main entry with example usage"""
    orch = SystemdOrchestrator()

    # Add jobs if they don't exist
    if "heartbeat" not in orch.jobs:
        orch.add_job("heartbeat", "while true; do echo Heartbeat; sleep 5; done")
        orch.start_job("heartbeat")
    if "todo_app" not in orch.jobs:
        orch.add_job("todo_app", "/usr/bin/python3 " + str(BASE_DIR / "hybridTODO.py"))
        orch.start_job("todo_app")

    # Handle commands
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "start":
            for name in orch.jobs:
                ms = orch.start_job(name)
                print(f"Started {name} in {ms:.2f}ms")
        elif cmd == "stop":
            for name in orch.jobs:
                ms = orch.stop_job(name)
                print(f"Stopped {name} in {ms:.2f}ms")
        elif cmd == "restart":
            times = orch.restart_all()
            print(f"Restart times: {times}")
        elif cmd == "status":
            print(json.dumps(orch.status(), indent=2))
        elif cmd == "cleanup":
            orch.cleanup()
            print("Cleaned up all units")
        else:
            print(f"Usage: {sys.argv[0]} [start|stop|restart|status|cleanup]")
    else:
        # Just show status
        status = orch.status()
        print(f"=== Systemd Orchestrator ===")
        print(f"Jobs: {len(status)}")
        for name, info in status.items():
            print(f"  {name}: {info['state']} (PID: {info['pid']})")

if __name__ == "__main__":
    main()


================================================================================
FILE: systemdServiceManagerFlask.py
================================================================================

#!/usr/bin/env python3
import signal
import time
import logging
from systemd import journal

# Configure journal logging
journal_handler = journal.JournalHandler(SYSLOG_IDENTIFIER='my_service')
journal_handler.setLevel(logging.INFO)
logging.root.addHandler(journal_handler)
logging.root.setLevel(logging.INFO)

logger = logging.getLogger(__name__)

class ServiceManager:
    def __init__(self):
        self.shutdown = False
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        
    def _handle_signal(self, signum, frame):
        logger.info(f"Received signal {signum}, shutting down gracefully")
        self.shutdown = True
        
    def run(self):
        logger.info("Starting the service")
        total_duration = 0
        
        while not self.shutdown:
            # Your service logic here
            time.sleep(60)
            total_duration += 60
            logger.info(f"Total duration: {total_duration}")
            
            # Simulate crash after 5 minutes for demo
            if total_duration >= 300:
                raise Exception("Service crash simulation")

if __name__ == "__main__":
    service = ServiceManager()
    try:
        service.run()
    except Exception as e:
        logger.error(f"Service crashed: {e}")
        exit(1)


