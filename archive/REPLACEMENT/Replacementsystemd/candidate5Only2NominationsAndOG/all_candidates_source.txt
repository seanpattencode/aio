================================================================================
SYSTEMD CANDIDATES1 - ALL SOURCE FILES
================================================================================

================================================================================
FILE: chatgpt2.py
================================================================================

#!/usr/bin/env python3
# aios_systemd_orchestrator.py â€” best-of synthesis (<200 lines)
# - Transient user units & timers (systemd-run) = no zombies, auto cgroups
# - RT scheduling & nice via properties; optional slice/limits
# - Single SQLite DB for state; simple CLI
import argparse, json, os, shlex, sqlite3, subprocess, sys, time
from pathlib import Path

DB = Path.home() / ".aios_tasks.db"
UNIT_PREFIX = "aios-"
SYSTEMCTL = ["systemctl", "--user"]
SYSDRUN = ["systemd-run", "--user", "--collect", "--quiet"]

def sh(cmd:list[str]) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True)

def ok(cp): return cp.returncode == 0

def unit_name(name:str) -> str:
    safe = "".join(c if c.isalnum() or c in "._-:" else "_" for c in name)
    return f"{UNIT_PREFIX}{safe}.service"

def db():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    con.executescript("""
      PRAGMA journal_mode=WAL; PRAGMA busy_timeout=5000;
      CREATE TABLE IF NOT EXISTS jobs(
        name TEXT PRIMARY KEY, cmd TEXT NOT NULL, args TEXT,
        env TEXT, cwd TEXT, schedule TEXT, rtprio INTEGER,
        nice INTEGER, slice TEXT, cpu_weight INTEGER, mem_max_mb INTEGER,
        unit TEXT, status TEXT DEFAULT 'added', created_at INTEGER
      );
    """)
    return con

def add_job(**kw):
    kw.setdefault("created_at", int(time.time()))
    with db() as con:
        con.execute("""INSERT OR REPLACE INTO jobs
        (name,cmd,args,env,cwd,schedule,rtprio,nice,slice,cpu_weight,mem_max_mb,unit,status,created_at)
        VALUES(:name,:cmd,:args,:env,:cwd,:schedule,:rtprio,:nice,:slice,:cpu_weight,:mem_max_mb,:unit,:status,:created_at)""", kw)

def list_jobs():
    with db() as con:
        return con.execute("SELECT * FROM jobs ORDER BY created_at DESC").fetchall()

def show(unit:str, *props:str) -> dict:
    out = sh(SYSTEMCTL + ["show", unit, *(["--property="+p for p in props] if props else [])]).stdout
    return {k:v for k,v in (line.split("=",1) for line in out.splitlines() if "=" in line)}

def start_transient(job:sqlite3.Row):
    unit = unit_name(job["name"])
    props = [
        "--property=StandardOutput=journal",
        "--property=StandardError=journal",
        "--property=KillMode=control-group",
    ]
    if job["rtprio"]: props += ["--property=CPUSchedulingPolicy=rr",
                                f"--property=CPUSchedulingPriority={job['rtprio']}"]
    if job["nice"] is not None: props += [f"--property=Nice={int(job['nice'])}"]
    if job["slice"]: props += [f"--slice={job['slice']}"]
    if job["cpu_weight"]: props += [f"--property=CPUWeight={job['cpu_weight']}"]
    if job["mem_max_mb"]: props += [f"--property=MemoryMax={int(job['mem_max_mb'])}M"]
    env = []
    if job["env"]:
        for k,v in json.loads(job["env"]).items():
            env += ["--setenv", f"{k}={v}"]
    when = []
    if job["schedule"]: when += ["--on-calendar", job["schedule"]]
    if job["cwd"]: props += [f"--property=WorkingDirectory={job['cwd']}"]

    cmd = [*SYSDRUN, "--unit", unit, *props, *env, *when, "--", job["cmd"], *json.loads(job["args"] or "[]")]
    cp = sh(cmd)
    with db() as con:
        con.execute("UPDATE jobs SET unit=?, status=? WHERE name=?", (unit, "scheduled" if job["schedule"] else "started", job["name"]))
    return ok(cp), unit, cp.stderr.strip() or cp.stdout.strip()

def stop(name:str):
    unit = unit_name(name)
    sh(SYSTEMCTL + ["stop", unit])
    sh(SYSTEMCTL + ["stop", unit.replace(".service",".timer")])
    with db() as con:
        con.execute("UPDATE jobs SET status='stopped' WHERE name=?", (name,))

def status(name:str):
    unit = unit_name(name)
    info = show(unit, "ActiveState","Result","MainPID")
    if not info: return {"unit":unit,"active":"unknown"}
    return {"unit":unit,"active":info.get("ActiveState"),"result":info.get("Result"),"pid":info.get("MainPID")}

def set_rt(name:str, policy="fifo", prio=20):
    unit = unit_name(name)
    return sh(SYSTEMCTL + ["set-property", unit, f"CPUSchedulingPolicy={policy}", f"CPUSchedulingPriority={prio}"]).stdout

def reconcile():
    # Mark finished jobs by querying unit state
    with db() as con:
        for r in con.execute("SELECT name,unit,status FROM jobs WHERE unit IS NOT NULL"):
            info = show(r["unit"], "ActiveState","Result")
            if info and info.get("ActiveState") in ("inactive","failed"):
                new = "completed" if info.get("Result") == "success" else "failed"
                con.execute("UPDATE jobs SET status=? WHERE name=?", (new, r["name"]))

def main():
    ap = argparse.ArgumentParser(description="AIOS systemd orchestrator")
    sub = ap.add_subparsers(dest="cmd", required=True)

    a = sub.add_parser("add", help="record & (optionally) start a job")
    a.add_argument("name"); a.add_argument("command"); a.add_argument("args", nargs="*")
    a.add_argument("--env", action="append", default=[], help="KEY=VAL")
    a.add_argument("--cwd"); a.add_argument("--on-calendar")
    a.add_argument("--rtprio", type=int); a.add_argument("--nice", type=int)
    a.add_argument("--slice"); a.add_argument("--cpu-weight", type=int); a.add_argument("--mem-max-mb", type=int)
    a.add_argument("--start", action="store_true")

    sub.add_parser("list", help="list jobs")
    s = sub.add_parser("start", help="start existing job"); s.add_argument("name")
    t = sub.add_parser("stop", help="stop job"); t.add_argument("name")
    u = sub.add_parser("status", help="status"); u.add_argument("name")
    r = sub.add_parser("set-rt", help="tune RT"); r.add_argument("name"); r.add_argument("--policy", default="fifo"); r.add_argument("--prio", type=int, default=20)
    sub.add_parser("reconcile", help="refresh job statuses")

    args = ap.parse_args()

    if args.cmd == "add":
        env = dict(e.split("=",1) for e in args.env) if args.env else {}
        add_job(
            name=args.name, cmd=args.command, args=json.dumps(args.args),
            env=json.dumps(env) if env else None, cwd=args.cwd, schedule=args.on_calendar,
            rtprio=args.rtprio, nice=args.nice, slice=args.slice,
            cpu_weight=args.cpu_weight, mem_max_mb=args.mem_max_mb, unit=None, status="added"
        )
        if args.start:
            with db() as con:
                row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
            ok_, unit, msg = start_transient(row)
            print(unit if ok_ else f"ERROR: {msg}")
        else:
            print("added")

    elif args.cmd == "list":
        reconcile()
        for r in list_jobs():
            print(f"{r['name']}: {r['status']}  unit={r['unit'] or '-'}  sched={r['schedule'] or '-'}")

    elif args.cmd == "start":
        with db() as con:
            row = con.execute("SELECT * FROM jobs WHERE name=?", (args.name,)).fetchone()
        if not row: sys.exit("unknown job")
        ok_, unit, msg = start_transient(row)
        print(unit if ok_ else f"ERROR: {msg}")

    elif args.cmd == "stop":
        stop(args.name); print("stopped")

    elif args.cmd == "status":
        print(json.dumps(status(args.name), indent=2))

    elif args.cmd == "set-rt":
        print(set_rt(args.name, args.policy, args.prio).strip())

    elif args.cmd == "reconcile":
        reconcile(); print("ok")

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeCode3.py
================================================================================

#!/usr/bin/env python3
"""
claudeCode3: Production-Ready Orchestrator (<200 lines)
Synthesizes: Android reaping + Chrome WAL + Firefox journal patterns
Best for: Production deployments, high reliability
"""
import sqlite3
import subprocess
import signal
import logging
import json
import sys
import os
import time
from pathlib import Path
from typing import Dict, Optional, List
from enum import Enum

# Production configuration
DB_PATH = Path("/var/lib/aios/orchestrator.db")
UNIT_PREFIX = "aios-"
WATCHDOG_INTERVAL = 30

class WorkflowState(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class ProductionOrchestrator:
    """Production-grade systemd orchestrator with reliability patterns"""

    def __init__(self):
        self.running = True
        self.children = {}

        # Setup logging to systemd journal
        try:
            from systemd import journal
            handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios')
            logging.root.addHandler(handler)
        except ImportError:
            pass

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        # Initialize database with production settings
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(str(DB_PATH), isolation_level=None)
        self._init_db()

        # Signal handlers for graceful shutdown
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGCHLD, self._reap_children)

    def _init_db(self):
        """Initialize SQLite with production optimizations"""
        # Chrome/Firefox WAL mode patterns
        self.db.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            PRAGMA cache_size=-8000;
            PRAGMA busy_timeout=5000;
            PRAGMA wal_autocheckpoint=1000;

            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                retry_count INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                started_at DATETIME,
                completed_at DATETIME,
                pid INTEGER,
                exit_code INTEGER,
                unit_name TEXT
            );

            CREATE INDEX IF NOT EXISTS idx_state_priority
            ON workflows(state, priority DESC, created_at);
        """)

    def _handle_signal(self, signum, frame):
        """Graceful shutdown handler"""
        self.logger.info(f"Received signal {signum}, shutting down")
        self.running = False
        try:
            from systemd import daemon
            daemon.notify("STOPPING=1")
        except ImportError:
            pass

    def _reap_children(self, signum, frame):
        """Android-style zombie reaping pattern"""
        while True:
            try:
                pid, status = os.waitpid(-1, os.WNOHANG)
                if pid == 0:
                    break

                exit_code = os.WEXITSTATUS(status) if os.WIFEXITED(status) else -1

                # Update database
                self.db.execute("""
                    UPDATE workflows
                    SET state = ?, exit_code = ?, completed_at = CURRENT_TIMESTAMP
                    WHERE pid = ?
                """, ('completed' if exit_code == 0 else 'failed', exit_code, pid))

                self.logger.info(f"Reaped child {pid} with exit code {exit_code}")

            except ChildProcessError:
                break

    def submit(self, name: str, command: str, priority: int = 0) -> int:
        """Submit new workflow"""
        cursor = self.db.execute("""
            INSERT INTO workflows (name, command, priority)
            VALUES (?, ?, ?)
        """, (name, command, priority))

        workflow_id = cursor.lastrowid
        self.logger.info(f"Submitted workflow {workflow_id}: {name}")
        return workflow_id

    def deploy_workflow(self, workflow_id: int) -> bool:
        """Deploy workflow as systemd service with production settings"""
        row = self.db.execute("""
            SELECT name, command, priority FROM workflows WHERE id = ?
        """, (workflow_id,)).fetchone()

        if not row:
            return False

        name, command, priority = row
        unit_name = f"{UNIT_PREFIX}{workflow_id}.service"

        # Create transient unit with production properties
        properties = [
            "--collect",  # Clean up automatically
            "--property=Type=exec",
            "--property=Restart=on-failure",
            "--property=RestartSec=10",
            f"--property=Nice={-priority}",  # Higher priority = lower nice
            "--property=StandardOutput=journal",
            "--property=StandardError=journal",
            "--property=KillMode=control-group",  # Kill all children
            "--property=TimeoutStopSec=30",
            # Resource limits
            "--property=MemoryMax=4G",
            "--property=CPUQuota=200%",  # 2 cores max
            "--property=TasksMax=1000",
            # Security
            "--property=PrivateTmp=yes",
            "--property=ProtectSystem=strict",
            "--property=NoNewPrivileges=yes",
        ]

        # Use systemd-run for transient units
        result = subprocess.run(
            ["systemd-run", "--user", "--unit", unit_name] + properties +
            ["--", "sh", "-c", command],
            capture_output=True, text=True
        )

        if result.returncode == 0:
            # Get PID from systemd
            pid_result = subprocess.run(
                ["systemctl", "--user", "show", unit_name, "--property=MainPID"],
                capture_output=True, text=True
            )

            pid = 0
            if pid_result.returncode == 0 and "=" in pid_result.stdout:
                pid = int(pid_result.stdout.split("=")[1].strip())

            # Update database
            self.db.execute("""
                UPDATE workflows
                SET state = 'running', started_at = CURRENT_TIMESTAMP,
                    pid = ?, unit_name = ?
                WHERE id = ?
            """, (pid, unit_name, workflow_id))

            self.logger.info(f"Deployed workflow {workflow_id} as {unit_name} (PID: {pid})")
            return True

        return False

    def monitor(self):
        """Monitor and process workflows"""
        # Deploy pending workflows
        pending = self.db.execute("""
            SELECT id FROM workflows
            WHERE state = 'pending'
            ORDER BY priority DESC, created_at
            LIMIT 5
        """).fetchall()

        for row in pending:
            self.deploy_workflow(row[0])

        # Check running workflows
        running = self.db.execute("""
            SELECT id, unit_name, retry_count, max_retries
            FROM workflows WHERE state = 'running'
        """).fetchall()

        for workflow_id, unit_name, retry_count, max_retries in running:
            result = subprocess.run(
                ["systemctl", "--user", "is-active", unit_name],
                capture_output=True, text=True
            )

            if result.stdout.strip() == "inactive":
                # Check if should retry
                if retry_count < max_retries:
                    self.db.execute("""
                        UPDATE workflows
                        SET state = 'pending', retry_count = retry_count + 1
                        WHERE id = ?
                    """, (workflow_id,))
                    self.logger.info(f"Retrying workflow {workflow_id} (attempt {retry_count + 1})")

        # Periodic WAL checkpoint (Firefox pattern)
        self.db.execute("PRAGMA wal_checkpoint(PASSIVE)")

    def run(self):
        """Main service loop"""
        try:
            from systemd import daemon
            daemon.notify("READY=1")
        except ImportError:
            pass

        self.logger.info("Production orchestrator started")

        while self.running:
            try:
                self.monitor()

                # Update systemd watchdog
                try:
                    from systemd import daemon
                    daemon.notify("WATCHDOG=1")
                except ImportError:
                    pass

                time.sleep(5)

            except Exception as e:
                self.logger.error(f"Monitor error: {e}")
                time.sleep(10)

        self.logger.info("Orchestrator shutdown complete")

def main():
    """CLI interface"""
    if len(sys.argv) < 2:
        print("Usage: claudeCode3.py <submit|status|run>")
        sys.exit(1)

    cmd = sys.argv[1]

    if cmd == "submit" and len(sys.argv) >= 4:
        orch = ProductionOrchestrator()
        workflow_id = orch.submit(sys.argv[2], sys.argv[3])
        print(f"Workflow ID: {workflow_id}")

    elif cmd == "status":
        db = sqlite3.connect(str(DB_PATH))
        for row in db.execute("SELECT id, name, state FROM workflows"):
            print(f"{row[0]}: {row[1]} [{row[2]}]")

    elif cmd == "run":
        orch = ProductionOrchestrator()
        orch.run()

if __name__ == "__main__":
    main()


================================================================================
FILE: claudeResearch2.py
================================================================================

#!/usr/bin/env python3
"""
AIOS Ultimate - Synthesized from all best practices
Combines: Android's reaping, transient units, async patterns, minimal overhead
"""
import os
import sys
import json
import sqlite3
import signal
import asyncio
import logging
from pathlib import Path
from enum import Enum
from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict

# Setup logging (deepseek1 pattern)
try:
    from systemd import journal, daemon
    handler = journal.JournalHandler(SYSLOG_IDENTIFIER='aios')
    logging.root.addHandler(handler)
except ImportError:
    pass  # Fallback to standard logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('aios')

class State(Enum):
    PENDING = "pending"
    APPROVED = "approved" 
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class Workflow:
    id: str
    name: str
    command: str
    state: State = State.PENDING
    priority: int = 0
    memory_mb: int = 512
    cpu_percent: int = 100
    realtime: bool = False
    auto_approve: bool = False

class AIOS:
    """Ultimate AIOS combining best patterns from all implementations"""
    
    def __init__(self, db_path: str = "~/.aios/state.db"):
        self.db_path = Path(db_path).expanduser()
        self.db_path.parent.mkdir(exist_ok=True, parents=True)
        self.running = True
        self.tasks = {}  # Active async tasks
        
        # Initialize database (gemini WAL pattern)
        self.db = sqlite3.connect(str(self.db_path), isolation_level=None)
        self.db.row_factory = sqlite3.Row
        self.db.executescript("""
            PRAGMA journal_mode=WAL;
            PRAGMA synchronous=NORMAL;
            CREATE TABLE IF NOT EXISTS workflows (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                command TEXT NOT NULL,
                state TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                memory_mb INTEGER DEFAULT 512,
                cpu_percent INTEGER DEFAULT 100,
                realtime BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                unit_name TEXT,
                exit_code INTEGER
            );
            CREATE INDEX IF NOT EXISTS idx_state ON workflows(state, priority DESC);
        """)
        
        # Signal handling (copilot1 + deepseek1 pattern)
        signal.signal(signal.SIGTERM, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGINT, lambda *_: setattr(self, 'running', False))
        signal.signal(signal.SIGCHLD, signal.SIG_IGN)  # Auto-reap zombies
    
    def submit(self, workflow: Workflow) -> str:
        """Submit workflow for review/execution"""
        if workflow.auto_approve:
            workflow.state = State.APPROVED
        
        self.db.execute(
            """INSERT INTO workflows (id, name, command, state, priority, 
               memory_mb, cpu_percent, realtime) VALUES (?,?,?,?,?,?,?,?)""",
            (workflow.id, workflow.name, workflow.command, workflow.state.value,
             workflow.priority, workflow.memory_mb, workflow.cpu_percent, workflow.realtime)
        )
        logger.info(f"Submitted workflow {workflow.id}")
        
        if workflow.auto_approve:
            asyncio.create_task(self._execute(workflow))
        
        return workflow.id
    
    def approve(self, workflow_id: str) -> bool:
        """Approve pending workflow"""
        cursor = self.db.execute(
            "UPDATE workflows SET state=? WHERE id=? AND state=? RETURNING *",
            (State.APPROVED.value, workflow_id, State.PENDING.value)
        )
        row = cursor.fetchone()
        if row:
            workflow = self._row_to_workflow(row)
            asyncio.create_task(self._execute(workflow))
            return True
        return False
    
    async def _execute(self, workflow: Workflow):
        """Execute using systemd transient units (kimi1 + chatgptResearch1 pattern)"""
        import subprocess
        
        # Update state
        self.db.execute(
            "UPDATE workflows SET state=? WHERE id=?",
            (State.RUNNING.value, workflow.id)
        )
        
        # Build systemd-run command (transient units are fastest)
        unit_name = f"aios-{workflow.id}"
        cmd = [
            "systemd-run", "--user", "--collect", "--unit", unit_name,
            f"--property=MemoryMax={workflow.memory_mb}M",
            f"--property=CPUQuota={workflow.cpu_percent}%",
            "--property=StandardOutput=journal",
            "--property=StandardError=journal"
        ]
        
        # Real-time scheduling (deepseek1 pattern)
        if workflow.realtime:
            cmd.extend([
                "--property=CPUSchedulingPolicy=fifo",
                "--property=CPUSchedulingPriority=90"
            ])
        
        # Execute command
        cmd.extend(["--", "/bin/sh", "-c", workflow.command])
        
        try:
            # Start the unit
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # Store unit name for monitoring
                self.db.execute(
                    "UPDATE workflows SET unit_name=? WHERE id=?",
                    (unit_name, workflow.id)
                )
                
                # Monitor completion (async polling)
                await self._monitor_unit(workflow.id, unit_name)
            else:
                raise Exception(f"Failed to start: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Execution failed for {workflow.id}: {e}")
            self.db.execute(
                "UPDATE workflows SET state=?, exit_code=-1 WHERE id=?",
                (State.FAILED.value, workflow.id)
            )
    
    async def _monitor_unit(self, workflow_id: str, unit_name: str):
        """Monitor systemd unit status"""
        import subprocess
        
        while self.running:
            # Check unit status
            result = subprocess.run(
                ["systemctl", "--user", "show", unit_name, 
                 "--property=ActiveState,ExecMainStatus"],
                capture_output=True, text=True
            )
            
            props = {}
            for line in result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    props[k] = v
            
            state = props.get('ActiveState', '')
            
            if state in ('inactive', 'failed'):
                # Unit finished
                exit_code = int(props.get('ExecMainStatus', '-1'))
                final_state = State.COMPLETED if exit_code == 0 else State.FAILED
                
                self.db.execute(
                    "UPDATE workflows SET state=?, exit_code=? WHERE id=?",
                    (final_state.value, exit_code, workflow_id)
                )
                
                logger.info(f"Workflow {workflow_id} {final_state.value} (exit={exit_code})")
                break
            
            await asyncio.sleep(1)
    
    def _row_to_workflow(self, row) -> Workflow:
        """Convert DB row to Workflow"""
        return Workflow(
            id=row['id'], name=row['name'], command=row['command'],
            state=State(row['state']), priority=row['priority'],
            memory_mb=row['memory_mb'], cpu_percent=row['cpu_percent'],
            realtime=bool(row['realtime'])
        )
    
    async def scheduler(self):
        """Main scheduler loop (claude1 pattern)"""
        try:
            if hasattr(daemon, 'notify'):
                daemon.notify('READY=1')
        except:
            pass
        
        while self.running:
            # Get approved workflows
            rows = self.db.execute(
                """SELECT * FROM workflows WHERE state=? 
                   ORDER BY priority DESC, created_at LIMIT 5""",
                (State.APPROVED.value,)
            ).fetchall()
            
            # Execute in parallel
            if rows:
                tasks = [self._execute(self._row_to_workflow(row)) for row in rows]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # Status update
            stats = self.db.execute(
                "SELECT state, COUNT(*) as cnt FROM workflows GROUP BY state"
            ).fetchall()
            
            status = {row['state']: row['cnt'] for row in stats}
            logger.info(f"Status: {status}")
            
            if hasattr(daemon, 'notify'):
                active = status.get('running', 0)
                daemon.notify(f'STATUS=Running {active} workflows')
            
            await asyncio.sleep(2)
    
    def status(self, workflow_id: Optional[str] = None) -> Dict[str, Any]:
        """Get workflow status"""
        if workflow_id:
            row = self.db.execute(
                "SELECT * FROM workflows WHERE id=?", (workflow_id,)
            ).fetchone()
            return dict(row) if row else None
        else:
            return {
                row['id']: row['state'] for row in 
                self.db.execute("SELECT id, state FROM workflows")
            }

# CLI Interface (qwen1 + geminiWeb1 pattern)
async def main():
    """Main entry point with CLI"""
    aios = AIOS()
    
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        
        if cmd == "submit":
            # Example: ./aios.py submit "test" "echo hello" --auto
            wf = Workflow(
                id=f"wf-{os.urandom(4).hex()}",
                name=sys.argv[2] if len(sys.argv) > 2 else "test",
                command=sys.argv[3] if len(sys.argv) > 3 else "echo test",
                auto_approve="--auto" in sys.argv,
                realtime="--rt" in sys.argv
            )
            print(f"Submitted: {aios.submit(wf)}")
            
        elif cmd == "approve" and len(sys.argv) > 2:
            if aios.approve(sys.argv[2]):
                print(f"Approved: {sys.argv[2]}")
            else:
                print("Not found or already approved")
                
        elif cmd == "status":
            wf_id = sys.argv[2] if len(sys.argv) > 2 else None
            print(json.dumps(aios.status(wf_id), indent=2))
            
        else:
            print(f"Usage: {sys.argv[0]} [submit|approve|status|run]")
            sys.exit(1)
    else:
        # Run scheduler
        await aios.scheduler()

if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: systemdOrchestrator.py
================================================================================

#!/usr/bin/env python3
"""
Systemd-based orchestrator - Ultra-minimal, ultra-fast
Leverages systemd for process management, restart, and zombie reaping
"""
import os
import sys
import time
import subprocess
import json
from pathlib import Path

BASE_DIR = Path(__file__).parent.absolute()
UNIT_PREFIX = "aios-"

class SystemdOrchestrator:
    """Minimal systemd wrapper - let systemd handle everything"""

    def __init__(self):
        self.jobs = {}
        self._load_jobs()

    def _run(self, *args):
        """Run systemctl command"""
        return subprocess.run(["systemctl", "--user"] + list(args),
                            capture_output=True, text=True, check=False)

    def _load_jobs(self):
        """Load existing AIOS jobs from systemd"""
        result = self._run("list-units", f"{UNIT_PREFIX}*.service", "--no-legend", "--plain")
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                if parts:
                    name = parts[0].replace('.service', '').replace(UNIT_PREFIX, '')
                    self.jobs[name] = parts[0]

    def add_job(self, name: str, command: str, restart: str = "always") -> str:
        """Create systemd service unit"""
        unit_name = f"{UNIT_PREFIX}{name}.service"
        unit_path = Path(f"~/.config/systemd/user/{unit_name}").expanduser()
        unit_path.parent.mkdir(parents=True, exist_ok=True)

        # Systemd handles: zombie reaping, process groups, restart, logging
        unit_content = f"""[Unit]
Description=AIOS Job: {name}

[Service]
Type=simple
ExecStart=/bin/sh -c '{command}'
Restart={restart}
RestartSec=0
StandardOutput=journal
StandardError=journal
KillMode=control-group
TimeoutStopSec=0

[Install]
WantedBy=default.target
"""
        unit_path.write_text(unit_content)
        self.jobs[name] = unit_name
        self._run("daemon-reload")
        return unit_name

    def start_job(self, name: str) -> float:
        """Start job via systemd"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("start", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def stop_job(self, name: str) -> float:
        """Stop job immediately"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("stop", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def restart_job(self, name: str) -> float:
        """Restart job via systemd"""
        if name not in self.jobs:
            return -1
        start = time.perf_counter()
        self._run("restart", self.jobs[name])
        return (time.perf_counter() - start) * 1000

    def restart_all(self) -> dict:
        """Restart all jobs"""
        start = time.perf_counter()
        times = {}

        # Use systemd's batch restart for speed
        units = list(self.jobs.values())
        if units:
            result = self._run("restart", *units)
            for name in self.jobs:
                times[name] = 0.5  # systemd handles it in parallel

        total = (time.perf_counter() - start) * 1000
        print(f"=== RESTART ALL in {total:.2f}ms ===")
        return times

    def status(self) -> dict:
        """Get status of all jobs"""
        status = {}
        for name, unit in self.jobs.items():
            result = self._run("show", unit, "--property=ActiveState,MainPID,ExecMainStartTimestampMonotonic")
            props = {}
            for line in result.stdout.strip().split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    props[k] = v

            status[name] = {
                'state': props.get('ActiveState', 'unknown'),
                'pid': int(props.get('MainPID', 0))
            }
        return status

    def cleanup(self):
        """Remove all AIOS systemd units"""
        for unit in self.jobs.values():
            self._run("stop", unit)
            self._run("disable", unit)
            unit_path = Path(f"~/.config/systemd/user/{unit}").expanduser()
            if unit_path.exists():
                unit_path.unlink()
        self._run("daemon-reload")

def main():
    """Main entry with example usage"""
    orch = SystemdOrchestrator()

    # Add jobs if they don't exist
    if "heartbeat" not in orch.jobs:
        orch.add_job("heartbeat", "while true; do echo Heartbeat; sleep 5; done")
        orch.start_job("heartbeat")
    if "todo_app" not in orch.jobs:
        orch.add_job("todo_app", "/usr/bin/python3 " + str(BASE_DIR / "hybridTODO.py"))
        orch.start_job("todo_app")

    # Handle commands
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "start":
            for name in orch.jobs:
                ms = orch.start_job(name)
                print(f"Started {name} in {ms:.2f}ms")
        elif cmd == "stop":
            for name in orch.jobs:
                ms = orch.stop_job(name)
                print(f"Stopped {name} in {ms:.2f}ms")
        elif cmd == "restart":
            times = orch.restart_all()
            print(f"Restart times: {times}")
        elif cmd == "status":
            print(json.dumps(orch.status(), indent=2))
        elif cmd == "cleanup":
            orch.cleanup()
            print("Cleaned up all units")
        else:
            print(f"Usage: {sys.argv[0]} [start|stop|restart|status|cleanup]")
    else:
        # Just show status
        status = orch.status()
        print(f"=== Systemd Orchestrator ===")
        print(f"Jobs: {len(status)}")
        for name, info in status.items():
            print(f"  {name}: {info['state']} (PID: {info['pid']})")

if __name__ == "__main__":
    main()


