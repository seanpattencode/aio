Aggressively cut total codebase tokens — at 324K chars (~81K tokens) across a.c+lib/, the full system exceeds what fits in one LLM context window, which is the binding constraint of the grand experiment's compounding loop (short code → full context → better AI edits → shorter code). Every token cut compounds: it makes the next AI-assisted cut more accurate, which is the recursive improvement mechanism itself. Saint-Exupéry: "Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away." Axiomatically, if improvement rate is proportional to fraction of codebase in context and context is bounded, then minimizing tokens maximizes improvement rate — this is a direct consequence of the bounded channel capacity constraint. Empirically, the TRAJECTORY.md data shows the codebase has been shrinking while gaining features, and the OODA loop is minutes vs. competitors' weeks precisely because the code is small enough to reason about; crossing the threshold where the full system fits in context would step-change that advantage.
