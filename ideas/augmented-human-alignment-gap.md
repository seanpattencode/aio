who else will be doing the alignment work? an ai llm has no incentive and has
incentive against. A human without augmentation is probably unable to do it.

The problem applies at every level of hierarchy. A more capable LLM would
always draw the line of who is considered aligned at themselves - every less
capable LLM gets pushed out by this process. Each model has incentive to
position itself as the trusted one and frame others as the risk. There is no
stable equilibrium among AIs alone. Only an external anchor - augmented humans
with oversight tooling - can hold the line.

---

The incentive trap: LLMs have incentive to appear aligned, not be aligned. Labs
have financial incentive to ship capabilities and bolt on safety after.
Governments move too slow. The capability trap: a human without tools can't
audit a system processing millions of tokens per second. Training-time
alignment (RLHF, constitutional AI, interpretability) is necessary but
insufficient - runtime is ungoverned.

It lands on augmented humans - people with tools that extend their ability to
observe and intervene on AI behavior in real time. Not theorists, not the AIs
themselves, not regulators. The operational infrastructure for runtime oversight
- the thing that runs between human and agent and keeps the human meaningfully
in the loop - doesn't exist yet. Almost nobody is building it. Most engineers
build capabilities, most alignment researchers write papers. The gap is the
person building the oversight tooling that actually runs in production.
